{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding_text_classification_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalika/NLP/blob/master/word_embedding_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnVf2R_Youjn",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcAnPZ3UAnPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This is a implementation of Word2Vec using numpy. Uncomment the print functions to see Word2Vec in action! Also remember to change the number of epochs and set training_data to training_data[0] to avoid flooding your terminal. A Google Sheet implementation of Word2Vec is also available here - https://docs.google.com/spreadsheets/d/1mgf82Ue7MmQixMm2ZqnT1oWUucj6pEcd2wDs_JgHmco/edit?usp=sharing\n",
        "\n",
        "Have fun learning!\n",
        "\n",
        "Author: Derek Chia\n",
        "Email: derek@derekchia.com\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "## Randomly initialise\n",
        "\"\"\"getW1 = [[0.236, -0.962, 0.686, 0.785, -0.454, -0.833, -0.744, 0.677, -0.427, -0.066],\n",
        "\t\t[-0.907, 0.894, 0.225, 0.673, -0.579, -0.428, 0.685, 0.973, -0.070, -0.811],\n",
        "\t\t[-0.576, 0.658, -0.582, -0.112, 0.662, 0.051, -0.401, -0.921, -0.158, 0.529],\n",
        "\t\t[0.517, 0.436, 0.092, -0.835, -0.444, -0.905, 0.879, 0.303, 0.332, -0.275],\n",
        "\t\t[0.859, -0.890, 0.651, 0.185, -0.511, -0.456, 0.377, -0.274, 0.182, -0.237],\n",
        "\t\t[0.368, -0.867, -0.301, -0.222, 0.630, 0.808, 0.088, -0.902, -0.450, -0.408],\n",
        "\t\t[0.728, 0.277, 0.439, 0.138, -0.943, -0.409, 0.687, -0.215, -0.807, 0.612],\n",
        "\t\t[0.593, -0.699, 0.020, 0.142, -0.638, -0.633, 0.344, 0.868, 0.913, 0.429],\n",
        "\t\t[0.447, -0.810, -0.061, -0.495, 0.794, -0.064, -0.817, -0.408, -0.286, 0.149]]\n",
        "\n",
        "getW2 = [[-0.868, -0.406, -0.288, -0.016, -0.560, 0.179, 0.099, 0.438, -0.551],\n",
        "\t\t[-0.395, 0.890, 0.685, -0.329, 0.218, -0.852, -0.919, 0.665, 0.968],\n",
        "\t\t[-0.128, 0.685, -0.828, 0.709, -0.420, 0.057, -0.212, 0.728, -0.690],\n",
        "\t\t[0.881, 0.238, 0.018, 0.622, 0.936, -0.442, 0.936, 0.586, -0.020],\n",
        "\t\t[-0.478, 0.240, 0.820, -0.731, 0.260, -0.989, -0.626, 0.796, -0.599],\n",
        "\t\t[0.679, 0.721, -0.111, 0.083, -0.738, 0.227, 0.560, 0.929, 0.017],\n",
        "\t\t[-0.690, 0.907, 0.464, -0.022, -0.005, -0.004, -0.425, 0.299, 0.757],\n",
        "\t\t[-0.054, 0.397, -0.017, -0.563, -0.551, 0.465, -0.596, -0.413, -0.395],\n",
        "\t\t[-0.838, 0.053, -0.160, -0.164, -0.671, 0.140, -0.149, 0.708, 0.425],\n",
        "\t\t[0.096, -0.995, -0.313, 0.881, -0.402, -0.631, -0.660, 0.184, 0.487]]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "## Randomly initialise\n",
        "getW1 = [[0.236, -0.962, 0.686, 0.785, -0.454, -0.833, -0.744],\n",
        "\t\t[-0.907, 0.894, 0.225, 0.673, -0.579, -0.428, 0.685],\n",
        "\t\t[-0.576, 0.658, -0.582, -0.112, 0.662, 0.051, -0.401],\n",
        "\t\t[0.517, 0.436, 0.092, -0.835, -0.444, -0.905, 0.879],\n",
        "\t\t[0.859, -0.890, 0.651, 0.185, -0.511, -0.456, 0.377],\n",
        "\t\t[0.368, -0.867, -0.301, -0.222, 0.630, 0.808, 0.088],\n",
        "\t\t[0.728, 0.277, 0.439, 0.138, -0.943, -0.409, 0.687],\n",
        "\t\t[0.593, -0.699, 0.020, 0.142, -0.638, -0.633, 0.344],\n",
        "\t\t[0.447, -0.810, -0.061, -0.495, 0.794, -0.064, -0.817]]\n",
        "\n",
        "getW2 = [[-0.868, -0.406, -0.288, -0.016, -0.560, 0.179, 0.099, 0.438, -0.551],\n",
        "\t\t[-0.395, 0.890, 0.685, -0.329, 0.218, -0.852, -0.919, 0.665, 0.968],\n",
        "\t\t[-0.128, 0.685, -0.828, 0.709, -0.420, 0.057, -0.212, 0.728, -0.690],\n",
        "        [0.881, 0.238, 0.018, 0.622, 0.936, -0.442, 0.936, 0.586, -0.020],\n",
        "\t\t[-0.478, 0.240, 0.820, -0.731, 0.260, -0.989, -0.626, 0.796, -0.599],\n",
        "\t\t[0.679, 0.721, -0.111, 0.083, -0.738, 0.227, 0.560, 0.929, 0.017],\n",
        "\t\t[-0.690, 0.907, 0.464, -0.022, -0.005, -0.004, -0.425, 0.299, 0.757]]\n",
        "\n",
        "\n",
        "class word2vec():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.n = settings['n']\n",
        "\t\tself.lr = settings['learning_rate']\n",
        "\t\tself.epochs = settings['epochs']\n",
        "\t\tself.window = settings['window_size']\n",
        "\n",
        "\tdef generate_training_data(self, settings, corpus):\n",
        "\t\t# Find unique word counts using dictonary\n",
        "\t\tword_counts = defaultdict(int)\n",
        "\t\tfor row in corpus:\n",
        "\t\t\tfor word in row:\n",
        "\t\t\t\tword_counts[word] += 1\n",
        "\t\t#########################################################################################################################################################\n",
        "\t\t# print(word_counts)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t# # defaultdict(<class 'int'>, {'natural': 1, 'language': 1, 'processing': 1, 'and': 2, 'machine': 1, 'learning': 1, 'is': 1, 'fun': 1, 'exciting': 1})\t#\n",
        "\t\t#########################################################################################################################################################\n",
        "\n",
        "\t\t## How many unique words in vocab? 9\n",
        "\t\tself.v_count = len(word_counts.keys())\n",
        "\t\t#########################\n",
        "\t\t# print(self.v_count)\t#\n",
        "\t\t# 9\t\t\t\t\t\t#\n",
        "\t\t#########################\n",
        "\n",
        "\t\t# Generate Lookup Dictionaries (vocab)\n",
        "\t\tself.words_list = list(word_counts.keys())\n",
        "\t\t#################################################################################################\n",
        "\t\t# print(self.words_list)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t# ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting']\t#\n",
        "\t\t#################################################################################################\n",
        "\t\t\n",
        "\t\t# Generate word:index\n",
        "\t\tself.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
        "\t\t#############################################################################################################################\n",
        "\t\t# print(self.word_index)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t# # {'natural': 0, 'language': 1, 'processing': 2, 'and': 3, 'machine': 4, 'learning': 5, 'is': 6, 'fun': 7, 'exciting': 8}\t#\n",
        "\t\t#############################################################################################################################\n",
        "\n",
        "\t\t# Generate index:word\n",
        "\t\tself.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
        "\t\t#############################################################################################################################\n",
        "\t\t# print(self.index_word)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t# {0: 'natural', 1: 'language', 2: 'processing', 3: 'and', 4: 'machine', 5: 'learning', 6: 'is', 7: 'fun', 8: 'exciting'}\t#\n",
        "\t\t#############################################################################################################################\n",
        "\n",
        "\t\ttraining_data = []\n",
        "\n",
        "\t\t# Cycle through each sentence in corpus\n",
        "\t\tfor sentence in corpus:\n",
        "\t\t\tsent_len = len(sentence)\n",
        "\n",
        "\t\t\t# Cycle through each word in sentence\n",
        "\t\t\tfor i, word in enumerate(sentence):\n",
        "\t\t\t\t# Convert target word to one-hot\n",
        "\t\t\t\tw_target = self.word2onehot(sentence[i])\n",
        "\n",
        "\t\t\t\t# Cycle through context window\n",
        "\t\t\t\tw_context = []\n",
        "\n",
        "\t\t\t\t# Note: window_size 2 will have range of 5 values\n",
        "\t\t\t\tfor j in range(i - self.window, i + self.window+1):\n",
        "\t\t\t\t\t# Criteria for context word \n",
        "\t\t\t\t\t# 1. Target word cannot be context word (j != i)\n",
        "\t\t\t\t\t# 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
        "\t\t\t\t\t# 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
        "\t\t\t\t\tif j != i and j <= sent_len-1 and j >= 0:\n",
        "\t\t\t\t\t\t# Append the one-hot representation of word to w_context\n",
        "\t\t\t\t\t\tw_context.append(self.word2onehot(sentence[j]))\n",
        "\t\t\t\t\t\t# print(sentence[i], sentence[j]) \n",
        "\t\t\t\t\t\t#########################\n",
        "\t\t\t\t\t\t# Example:\t\t\t\t#\n",
        "\t\t\t\t\t\t# natural language\t\t#\n",
        "\t\t\t\t\t\t# natural processing\t#\n",
        "\t\t\t\t\t\t# language natural\t\t#\n",
        "\t\t\t\t\t\t# language processing\t#\n",
        "\t\t\t\t\t\t# language append \t\t#\n",
        "\t\t\t\t\t\t#########################\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t# training_data contains a one-hot representation of the target word and context words\n",
        "\t\t\t\t#################################################################################################\n",
        "\t\t\t\t# Example:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t\t\t# [Target] natural, [Context] language, [Context] processing\t\t\t\t\t\t\t\t\t#\n",
        "\t\t\t\t# print(training_data)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t\t\t# [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]]]\t#\n",
        "\t\t\t\t#################################################################################################\n",
        "\t\t\t\ttraining_data.append([w_target, w_context])\n",
        "\n",
        "\t\treturn np.array(training_data)\n",
        "\n",
        "\tdef word2onehot(self, word):\n",
        "\t\t# word_vec - initialise a blank vector\n",
        "\t\tword_vec = [0 for i in range(0, self.v_count)] # Alternative - np.zeros(self.v_count)\n",
        "\t\t#############################\n",
        "\t\t# print(word_vec)\t\t\t#\n",
        "\t\t# [0, 0, 0, 0, 0, 0, 0, 0]\t#\n",
        "\t\t#############################\n",
        "\n",
        "\t\t# Get ID of word from word_index\n",
        "\t\tword_index = self.word_index[word]\n",
        "\n",
        "\t\t# Change value from 0 to 1 according to ID of the word\n",
        "\t\tword_vec[word_index] = 1\n",
        "\n",
        "\t\treturn word_vec\n",
        "\n",
        "\tdef train(self, training_data):\n",
        "\t\t# Initialising weight matrices\n",
        "\t\t# np.random.uniform(HIGH, LOW, OUTPUT_SHAPE)\n",
        "\t\t# https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.uniform.html\n",
        "\t\tself.w1 = np.array(getW1)\n",
        "\t\tself.w2 = np.array(getW2)\n",
        "\t\t# self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
        "\t\t# self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
        "\t\t\n",
        "\t\t# Cycle through each epoch\n",
        "\t\tfor i in range(self.epochs):\n",
        "\t\t\t# Intialise loss to 0\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\t# Cycle through each training sample\n",
        "\t\t\t# w_t = vector for target word, w_c = vectors for context words\n",
        "\t\t\tfor w_t, w_c in training_data:\n",
        "\t\t\t\t# Forward pass\n",
        "\t\t\t\t# 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)\n",
        "\t\t\t\ty_pred, h, u = self.forward_pass(w_t)\n",
        "\t\t\t\t#########################################\n",
        "\t\t\t\t# print(\"Vector for target word:\", w_t)\t#\n",
        "\t\t\t\t# print(\"W1-before backprop\", self.w1)\t#\n",
        "\t\t\t\t# print(\"W2-before backprop\", self.w2)\t#\n",
        "\t\t\t\t#########################################\n",
        "\n",
        "\t\t\t\t# Calculate error\n",
        "\t\t\t\t# 1. For a target word, calculate difference between y_pred and each of the context words\n",
        "\t\t\t\t# 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
        "\t\t\t\tEI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
        "\t\t\t\t#########################\n",
        "\t\t\t\t# print(\"Error\", EI)\t#\n",
        "\t\t\t\t#########################\n",
        "\n",
        "\t\t\t\t# Backpropagation\n",
        "\t\t\t\t# We use SGD to backpropagate errors - calculate loss on the output layer \n",
        "\t\t\t\tself.backprop(EI, h, w_t)\n",
        "\t\t\t\t#########################################\n",
        "\t\t\t\t#print(\"W1-after backprop\", self.w1)\t#\n",
        "\t\t\t\t#print(\"W2-after backprop\", self.w2)\t#\n",
        "\t\t\t\t#########################################\n",
        "\n",
        "\t\t\t\t# Calculate loss\n",
        "\t\t\t\t# There are 2 parts to the loss function\n",
        "\t\t\t\t# Part 1: -ve sum of all the output +\n",
        "\t\t\t\t# Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
        "\t\t\t\t# Note: word.index(1) returns the index in the context word vector with value 1\n",
        "\t\t\t\t# Note: u[word.index(1)] returns the value of the output layer before softmax\n",
        "\t\t\t\tself.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
        "\t\t\t\t\n",
        "\t\t\t\t#############################################################\n",
        "\t\t\t\t# Break if you want to see weights after first target word \t#\n",
        "\t\t\t\t# break \t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
        "\t\t\t\t#############################################################\n",
        "\t\t\tprint('Epoch:', i, \"Loss:\", self.loss)\n",
        "\n",
        "\tdef forward_pass(self, x):\n",
        "\t\t# x is one-hot vector for target word, shape - 9x1\n",
        "\t\t# Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
        "\t\th = np.dot(x, self.w1)\n",
        "\t\t# Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
        "\t\tu = np.dot(h, self.w2)\n",
        "\t\t# Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
        "\t\ty_c = self.softmax(u)\n",
        "\t\treturn y_c, h, u\n",
        "\n",
        "\tdef softmax(self, x):\n",
        "\t\te_x = np.exp(x - np.max(x))\n",
        "\t\treturn e_x / e_x.sum(axis=0)\n",
        "\n",
        "\tdef backprop(self, e, h, x):\n",
        "\t\t# https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
        "\t\t# Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
        "\t\t# Going backwards, we need to take derivative of E with respect of w2\n",
        "\t\t# h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
        "\t\t# x - shape 9x1, w2 - 10x9, e.T - 9x1\n",
        "\t\tdl_dw2 = np.outer(h, e)\n",
        "\t\tdl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
        "\t\t########################################\n",
        "\t\t# print('Delta for w2', dl_dw2)\t\t\t#\n",
        "\t\t# print('Hidden layer', h)\t\t\t\t#\n",
        "\t\t# print('np.dot', np.dot(self.w2, e.T))\t#\n",
        "\t\t# print('Delta for w1', dl_dw1)\t\t\t#\n",
        "\t\t#########################################\n",
        "\n",
        "\t\t# Update weights\n",
        "\t\tself.w1 = self.w1 - (self.lr * dl_dw1)\n",
        "\t\tself.w2 = self.w2 - (self.lr * dl_dw2)\n",
        "\n",
        "\t# Get vector from word\n",
        "\tdef word_vec(self, word):\n",
        "\t\tw_index = self.word_index[word]\n",
        "\t\tv_w = self.w1[w_index]\n",
        "\t\treturn v_w\n",
        "\n",
        "\t# Input vector, returns nearest word(s)\n",
        "\tdef vec_sim(self, word, top_n):\n",
        "\t\tv_w1 = self.word_vec(word)\n",
        "\t\tword_sim = {}\n",
        "\n",
        "\t\tfor i in range(self.v_count):\n",
        "\t\t\t# Find the similary score for each word in vocab\n",
        "\t\t\tv_w2 = self.w1[i]\n",
        "\t\t\ttheta_sum = np.dot(v_w1, v_w2)\n",
        "\t\t\ttheta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
        "\t\t\ttheta = theta_sum / theta_den\n",
        "\n",
        "\t\t\tword = self.index_word[i]\n",
        "\t\t\tword_sim[word] = theta\n",
        "\n",
        "\t\twords_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "\t\tfor word, sim in words_sorted[:top_n]:\n",
        "\t\t\tprint(word, sim)\n",
        "\n",
        "#####################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBZSQzaCFakd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing modules \n",
        "import requests\n",
        "\n",
        "from lxml import etree \n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
        "article = scraped_data.read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-G27H0FG1yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing Square Brackets and Extra Spaces\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztNCwQAkG46E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pDsyMZZG7hy",
        "colab_type": "code",
        "outputId": "3136d62e-f443-4a79-9e5e-d85f18fbcae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Converting Text To Sentences\n",
        "sentence_list = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwDBjomAATiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"natural language processing and machine learning is fun and exciting\"\n",
        "\n",
        "#text = article_text\n",
        "\n",
        "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
        "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
        "corpus = [[word.lower() for word in text.split()]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBM2chXaIPdJ",
        "colab_type": "code",
        "outputId": "3f73986f-c187-440e-cfc2-aa8329dcd4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-YAIq9rIWSU",
        "colab_type": "code",
        "outputId": "248be0af-e3f0-49d6-dfc8-2349b7064117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'and',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'is',\n",
              "  'fun',\n",
              "  'and',\n",
              "  'exciting']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6g6W2apAWMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings = {\n",
        "\t'window_size': 2,\t# context window +- center word\n",
        "\t'n': 7,\t\t# dimensions of word embeddings, also refer to size of hidden layer\n",
        "\t'epochs': 50,\t\t# number of training epochs\n",
        "\t'learning_rate': 0.01\t# learning rate\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gTkp1YAf6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise object\n",
        "w2v = word2vec()\n",
        "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
        "# target_word is the input \n",
        "# context_word is the output\n",
        "training_data = w2v.generate_training_data(settings, corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ItYd_MVCWJg",
        "colab_type": "code",
        "outputId": "c7b57390-caf5-4385-8a1d-3af81e8373ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.asanyarray(training_data[0][0]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4053,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS27kprXCip3",
        "colab_type": "code",
        "outputId": "c84dd551-761e-427a-a4ce-af3526932a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "training_data[0][1:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEmeQ4E1Jd5W",
        "colab_type": "code",
        "outputId": "3447f343-dd2a-46b5-ab24-c75e235e0892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.asanyarray(training_data[0][1:]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9FXQpxhDAgD",
        "colab_type": "code",
        "outputId": "b00d10ee-aa26-468c-e7d0-8ed7737c5cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Training\n",
        "w2v.train(training_data)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 81.26139545860659\n",
            "Epoch: 1 Loss: 80.16189979840644\n",
            "Epoch: 2 Loss: 79.1560195789214\n",
            "Epoch: 3 Loss: 78.23076176494702\n",
            "Epoch: 4 Loss: 77.37568808932373\n",
            "Epoch: 5 Loss: 76.58227832114267\n",
            "Epoch: 6 Loss: 75.84347845900334\n",
            "Epoch: 7 Loss: 75.15337394201013\n",
            "Epoch: 8 Loss: 74.5069490142869\n",
            "Epoch: 9 Loss: 73.89990657372726\n",
            "Epoch: 10 Loss: 73.32853124268237\n",
            "Epoch: 11 Loss: 72.78958384483869\n",
            "Epoch: 12 Loss: 72.2802190609857\n",
            "Epoch: 13 Loss: 71.79792044020037\n",
            "Epoch: 14 Loss: 71.34044857955139\n",
            "Epoch: 15 Loss: 70.90579941726183\n",
            "Epoch: 16 Loss: 70.4921703789483\n",
            "Epoch: 17 Loss: 70.09793268266111\n",
            "Epoch: 18 Loss: 69.72160851735995\n",
            "Epoch: 19 Loss: 69.36185210875492\n",
            "Epoch: 20 Loss: 69.01743390828315\n",
            "Epoch: 21 Loss: 68.68722730739334\n",
            "Epoch: 22 Loss: 68.37019740554226\n",
            "Epoch: 23 Loss: 68.0653914570859\n",
            "Epoch: 24 Loss: 67.77193069718068\n",
            "Epoch: 25 Loss: 67.48900330536094\n",
            "Epoch: 26 Loss: 67.21585831159366\n",
            "Epoch: 27 Loss: 66.95180028623874\n",
            "Epoch: 28 Loss: 66.69618468461677\n",
            "Epoch: 29 Loss: 66.44841374042284\n",
            "Epoch: 30 Loss: 66.20793282124123\n",
            "Epoch: 31 Loss: 65.97422717484517\n",
            "Epoch: 32 Loss: 65.74681900752816\n",
            "Epoch: 33 Loss: 65.52526484596618\n",
            "Epoch: 34 Loss: 65.30915314249643\n",
            "Epoch: 35 Loss: 65.09810209056663\n",
            "Epoch: 36 Loss: 64.89175762274061\n",
            "Epoch: 37 Loss: 64.68979156826576\n",
            "Epoch: 38 Loss: 64.49189995100078\n",
            "Epoch: 39 Loss: 64.29780141161861\n",
            "Epoch: 40 Loss: 64.1072357405601\n",
            "Epoch: 41 Loss: 63.91996251032323\n",
            "Epoch: 42 Loss: 63.735759797411234\n",
            "Epoch: 43 Loss: 63.5544229857\n",
            "Epoch: 44 Loss: 63.37576364417762\n",
            "Epoch: 45 Loss: 63.19960847299986\n",
            "Epoch: 46 Loss: 63.025798312635644\n",
            "Epoch: 47 Loss: 62.8541872115727\n",
            "Epoch: 48 Loss: 62.684641548641345\n",
            "Epoch: 49 Loss: 62.51703920651282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn5287WWDHoI",
        "colab_type": "code",
        "outputId": "1ee43f5a-3ba6-4a83-a7b1-07ddd58508ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Get vector for word\n",
        "word = \"machine\"\n",
        "vec = w2v.word_vec(word)\n",
        "print(word, vec)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine [ 0.81745182 -0.97754696  0.45881187  0.20378167 -0.47598899 -0.73740533\n",
            "  0.4403721 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_upa_UPDKn2",
        "colab_type": "code",
        "outputId": "912061ad-f9d4-4cda-929e-5764477a7059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Find similar words\n",
        "w2v.vec_sim(\"machine\", 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine 1.0\n",
            "fun 0.864375673602599\n",
            "is 0.7280950417829436\n",
            "natural 0.5093532632621883\n",
            "and 0.49518851046012796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFD4NJ1C0omQ",
        "colab_type": "text"
      },
      "source": [
        "Credits to https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Preparing the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XctQ-Qj0dAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'http://mattmahoney.net/dc/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YtNsXBp0zvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "\n",
        "def maybe_download(filename, url, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified', filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jsf5Q4U1KQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the data into a list of strings.\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split() \n",
        "        # Note: \n",
        "        # 1. TensorFlow function as_str which ensures that the text is created as a string data-type.\n",
        "        # 2. split() - to create a list with all the words in the text file, separated by white-space characters. \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsFxCuEp02_2",
        "colab_type": "code",
        "outputId": "12fba40f-079a-4695-b9d1-cceb60bfcfff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filename = maybe_download('text8.zip', url, 31344016)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfsHTLWS1Ukc",
        "colab_type": "code",
        "outputId": "879b2050-4bee-4914-b2fa-6a6c4630942b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary = read_data(filename)\n",
        "print(vocabulary[:7])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM9iOoeMfi4i",
        "colab_type": "text"
      },
      "source": [
        "As you can observe, the returned vocabulary data contains a list of plain English words, ordered as they are in the sentences of the original extracted text file.  Now that we have all the words extracted in a list, we have to do some further processing to enable us to create our skip-gram batch data.  These further steps are:\n",
        "\n",
        "1.    Extract the top 10,000 most common words to include in our embedding vector\n",
        "2.  Gather together all the unique words and index them with a unique integer value – this is what is required to create an equivalent one-hot type input for the word.  We’ll use a dictionary to do this\n",
        "3. Loop through every word in the dataset (vocabulary variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7rVbcLfbqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    \"\"\"setting up a “counter” list, which will store the number of times a word \n",
        "    is found within the data-set.  Because we are restricting our vocabulary to \n",
        "    only 10,000 words, any words not within the top 10,000 most common words \n",
        "    will be marked with an “UNK” designation, standing for “unknown”. \"\"\"\n",
        "    dictionary = dict() # creates a dictionary\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    \"\"\" value assigned to each unique word key is simply an increasing integer \n",
        "    count of the size of the dictionary.  So, for instance, the most common word \n",
        "    will receive the value 1, the second most common the value 2, the third most \n",
        "    common word the value 3, and so on (the integer 0 is assigned to the ‘UNK’ \n",
        "    words).   This step creates a unique integer value for each word within the\n",
        "    vocabulary \"\"\"    \n",
        "        \n",
        "    data = list() # list called data \n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    \"\"\" A list called data is created, which will be the same length as words \n",
        "    but instead of being a list of individual words, it will instead be a list \n",
        "    of integers – with each word now being represented by the unique integer \n",
        "    that was assigned to this word in dictionary.  So, for the first sentence of\n",
        "    our data-set [‘anarchism’, ‘originated’, ‘as’, ‘a’, ‘term’, ‘of’, ‘abuse’], \n",
        "    now looks like this in the data variable: [5242, 3083, 12, 6, 195, 2, 3136]\"\"\"\n",
        "        \n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    \"\"\"creates a dictionary called reverse_dictionary that allows us to look up \n",
        "    a word based on its unique integer identifier, rather than looking up the \n",
        "    identifier based on the word i.e. the original dictionary.  \"\"\"\n",
        "    \n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rBHfc3pihvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "# generate batch data\n",
        "def generate_batch(data, batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    for _ in range(span):\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "    for i in range(batch_size // num_skips):\n",
        "        target = skip_window  # input word at the center of the buffer\n",
        "        targets_to_avoid = [skip_window]\n",
        "        for j in range(num_skips):\n",
        "            while target in targets_to_avoid:\n",
        "                target = random.randint(0, span - 1)\n",
        "            targets_to_avoid.append(target)\n",
        "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
        "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcPCARQ7lpd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "buffer = collections.deque(maxlen=span)\n",
        "for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlJdNvflpDZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(batch_size // num_skips):\n",
        "    target = skip_window  # input word at the center of the buffer\n",
        "    targets_to_avoid = [skip_window]\n",
        "    for j in range(num_skips):\n",
        "        while target in targets_to_avoid:\n",
        "            target = random.randint(0, span - 1)\n",
        "        targets_to_avoid.append(target)\n",
        "        batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
        "        context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWmC5EgipKM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent.\n",
        "valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e51y-g0rpjix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size=7\n",
        "batch_size = 128\n",
        "embedding_size = 128  # Dimension of the embedding vector.\n",
        "skip_window = 1       # How many words to consider left and right.\n",
        "num_skips = 2         # How many times to reuse an input to generate a context."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JMcofxqpnW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58sovXHGpq1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look up embeddings for inputs.\n",
        "embeddings = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5IiV4_ZqD_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "# Construct the variables for the softmax\n",
        "weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANHvZcHEqNxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert train_context to a one-hot format\n",
        "train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
        "    labels=train_one_hot))\n",
        "# Construct the SGD optimizer using a learning rate of 1.0.\n",
        "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwWbx00XieBb",
        "colab_type": "text"
      },
      "source": [
        "Setting up our data is now to create a data set comprising of our input words and associated grams, which can be used to train our Word2Vec embedding system. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grQptA5Y0SJF",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset\n",
        "https://github.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/blob/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2DRR6qEwrky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-w6ebHlzn8t",
        "colab_type": "text"
      },
      "source": [
        "Data from URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD-vskyWzrkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL_Tr = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'\n",
        "#URL_Te = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/test.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8QhBLn_zySg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "train = pd.read_csv(URL_Tr, sep='\\t')\n",
        "#test = pd.read_csv(URL_Te, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9vfzOy73Sd5",
        "colab_type": "text"
      },
      "source": [
        "Step 2: Analysing data: Dataframe analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lZ_AziaysZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWD7Og_E3lJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FCDumyG3Yun",
        "colab_type": "code",
        "outputId": "fdc7cde5-3193-4869-8fcf-56d260404132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Checking on the dimension\n",
        "print(train.shape, \"\\n\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bsIm6y3sUZ",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**: Notice the dimension of the train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtH7Ajqw4DE5",
        "colab_type": "code",
        "outputId": "270a0ef8-2a01-4e49-be55-a35bae1bdeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Null entry checking\n",
        "print (\"\\t\",train.isnull().values.any(), \"\\n\\t\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t False \n",
            "\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShT1_KZv4V7q",
        "colab_type": "code",
        "outputId": "9837863e-bd90-439f-f976-0e0a98bc6a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Number of unique sentences in the training / testing dataset\n",
        "print (len(train.groupby('SentenceId').nunique()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHYhNm3H5Gna",
        "colab_type": "text"
      },
      "source": [
        "Let's create a dataset with only full sentences. Exploring data this way will gives us cleaner graphs that aren't biased toward longer sentences. We can also add a label for the sentiment value to increased readability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhU_m6Ao5cGQ",
        "colab_type": "code",
        "outputId": "506a9802-341a-4061-da26-e9098a8916b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#Create df of full sentences\n",
        "fullSent = train.loc[train.groupby('SentenceId')['PhraseId'].idxmin()]\n",
        "\n",
        "#Change sentiment to increase readability\n",
        "fullSent['sentiment_label'] = ''\n",
        "Sentiment_Label = ['Negative', 'Somewhat Negative', \n",
        "                  'Neutral', 'Somewhat Positive', 'Positive']\n",
        "for sent, label in enumerate(Sentiment_Label):\n",
        "    fullSent.loc[train.Sentiment == sent, 'sentiment_label'] = label\n",
        "    \n",
        "fullSent.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>sentiment_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Somewhat Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining in...</td>\n",
              "      <td>4</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
              "      <td>1</td>\n",
              "      <td>Somewhat Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>117</td>\n",
              "      <td>4</td>\n",
              "      <td>A positively thrilling combination of ethnogra...</td>\n",
              "      <td>3</td>\n",
              "      <td>Somewhat Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>157</td>\n",
              "      <td>5</td>\n",
              "      <td>Aggressive self-glorification and a manipulati...</td>\n",
              "      <td>1</td>\n",
              "      <td>Somewhat Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     PhraseId  SentenceId  ... Sentiment    sentiment_label\n",
              "0           1           1  ...         1  Somewhat Negative\n",
              "63         64           2  ...         4           Positive\n",
              "81         82           3  ...         1  Somewhat Negative\n",
              "116       117           4  ...         3  Somewhat Positive\n",
              "156       157           5  ...         1  Somewhat Negative\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35pNk8pa5l0b",
        "colab_type": "text"
      },
      "source": [
        "**Step 2**: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN0VGpJF5FzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "treebank_tokenizer = TreebankWordTokenizer() # We use the treebank tokenizer.\n",
        "\n",
        "lem = WordNetLemmatizer() # lemmatization.\n",
        "\n",
        "stemmer = PorterStemmer() # Stemming.\n",
        "\n",
        "def tokenize(x):\n",
        "    return ' '.join(treebank_tokenizer.tokenize(x))\n",
        "\n",
        "def lemmatize(x):\n",
        "    return ' '.join([lem.lemmatize(s) for s in x.split(' ')])\n",
        "\n",
        "def stem(x):\n",
        "    return ' '.join([stemmer.stem(s) for s in x.split(' ')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YIUBx9g9O1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## All labled data\n",
        "#sentences = fullSent['Phrase'] # our text data\n",
        "#sentiment = fullSent['Sentiment']\n",
        "\n",
        "\n",
        "## Extended labeld data\n",
        "sentences = np.array(train['Phrase'])\n",
        "sentiment = np.array(train['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP8qDOqx0IdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer (object that will split a sentence to a set of tokens )\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Creating a tokenizer\n",
        "tokenizer = Tokenizer(lower=True)\n",
        "\n",
        "# Building word indices\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hc5n4Xa1jlG",
        "colab_type": "code",
        "outputId": "6fd5aea8-a9e5-492e-9115-02eed6e6f284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F1mBEMjmdRS",
        "colab_type": "code",
        "outputId": "7d877679-173a-4640-d0e9-ed49906fe1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A series'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJHE_hdz0oAo",
        "colab_type": "code",
        "outputId": "c28a1b39-e222-4c2e-b787-3d54b90504a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Tokenizing sentences\n",
        "X = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(len(X[0]), len(X[100]), len(X[200]), len(X[1000]), len(X[8529-1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35 9 7 2 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-0iy2ZY6p0r",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Problem**: Variable token lenths\n",
        "\n",
        "---\n",
        "**Solution:** Zero padding\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95SvSB-y2Ddi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a reverse dictionary\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Function takes a tokenized sentence and returns the words\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "# Creating texts \n",
        "tokens = list(map(sequence_to_text, X))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHL1Y9K23rYq",
        "colab_type": "code",
        "outputId": "e24c12bc-3049-42ea-a171-b6ebd70db6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "tokens[0][0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'series',\n",
              " 'of',\n",
              " 'escapades',\n",
              " 'demonstrating',\n",
              " 'the',\n",
              " 'adage',\n",
              " 'that',\n",
              " 'what',\n",
              " 'is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8tkMruWmqL3",
        "colab_type": "code",
        "outputId": "af5caa91-bca6-4bfa-bd2a-c0f0935c4142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'series']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9OvYdlD1OFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequences padding.\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator , pad_sequences\n",
        "\n",
        "X = pad_sequences(X, padding='post') #Returns x: Numpy array with shape (len(sequences), maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB6IU46j09mj",
        "colab_type": "code",
        "outputId": "9f0ae490-4392-4d50-8b85-59612ed28851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156060, 49)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCBIXNcJ1EI6",
        "colab_type": "code",
        "outputId": "c2939653-eb84-4fc1-c0b9-c4489427d06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    2,   323,     3, 14150,  6028,     1,  6586,     9,    52,\n",
              "           8,    46,    13,     1,  2976,     8,   177,    46,    13,\n",
              "           1, 10913,    65,     3,    78,   668, 10117,    19,   576,\n",
              "           3,    78,  2123,     5,    57,     3,     2,    42,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8HRIJU23_Q9",
        "colab_type": "code",
        "outputId": "9b462bf4-0a6a-4e91-9c5f-688d487d9344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Vocabulary Size.\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "print(\"Vocab size : \", vocab_size)\n",
        "print(\"X's shape : \" , X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size :  15289\n",
            "X's shape :  (156060, 49)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mE9e-Fo1_Wk",
        "colab_type": "code",
        "outputId": "bc39474b-649c-47dd-9cb1-40f10287e9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Construct the target.\n",
        "Y = sentiment\n",
        "print(\"Target size : \",Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target size :  (156060,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnTie9gA3M_V",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Train/Test Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lKRavCoIjrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build train and test datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbS3uwh_KtDc",
        "colab_type": "code",
        "outputId": "439dc9a6-bd02-4fb6-8f55-d954fa076ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((124848, 49), (124848,), (31212, 49), (31212,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnwOJsUxnXXP",
        "colab_type": "code",
        "outputId": "96976744-ef24-4f1f-c290-627e796242ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# one hot encode\n",
        "from keras.utils import to_categorical\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "print(\"Y_train's shape : \", Y_train.shape)\n",
        "print(\"Y_test's shape : \" , Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train's shape :  (124848, 5)\n",
            "Y_test's shape :  (31212, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixbDIjeAKEYW",
        "colab_type": "text"
      },
      "source": [
        "### **Step 4**: Model Building and Traning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q60zotVK-wB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten \n",
        "from keras.layers import Activation, Conv1D, GlobalMaxPooling1D\n",
        "from keras import optimizers\n",
        "\n",
        "# Define model\n",
        "def baseline_cnn_model(X , Y , embed_dim , vocab_size, epochs = 10, \n",
        "                       batch_size = 32, compiler='SGD'):\n",
        "    \n",
        "    # training and testing set :\n",
        "    length  = X.shape[0] \n",
        "    target_shape = Y.shape[1]\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    #Embeding layer\n",
        "    model.add(Embedding(vocab_size, embed_dim,input_length = X.shape[1]))    \n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(target_shape, activation='softmax'))\n",
        "    model.summary()  \n",
        "    \n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer=compiler, loss='categorical_crossentropy', \n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NdZaLj5uCjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper Functions\n",
        "from keras import backend as K\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H43HQSmvfo_S",
        "colab_type": "code",
        "outputId": "fd65290b-69e7-424a-9928-711b13a240a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Training Hyper Parameters\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "num_epochs = 1000\n",
        "decay=1e-4\n",
        "embed_dim = 32\n",
        "\n",
        "\n",
        "adm = optimizers.Adam(lr = lr , decay = decay)\n",
        "sgd = optimizers.SGD(lr = lr , nesterov=True, momentum=0.7, decay=decay) # schedule_decay=0.0004)\n",
        "Nadam = optimizers.Nadam(lr = lr , beta_1=0.9, beta_2=0.999, epsilon=1e-08)#, schedule_decay=0.0004)\n",
        "    \n",
        "\n",
        "model = baseline_cnn_model(X_train, Y_train, embed_dim = embed_dim, \n",
        "                           vocab_size = vocab_size, \n",
        "                           epochs = num_epochs , batch_size = batch_size, \n",
        "                           compiler = Nadam)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 49, 32)            486848    \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 47, 64)            6208      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling (None, 23, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 21, 128)           24704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                81984     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 600,069\n",
            "Trainable params: 600,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aadmdM3uzamK",
        "colab_type": "code",
        "outputId": "40ee0a7a-9bbf-4736-fe98-3bd43c785911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit model\n",
        "model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "          epochs=num_epochs,verbose=1,validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 99878 samples, validate on 24970 samples\n",
            "Epoch 1/1000\n",
            "99878/99878 [==============================] - 10s 102us/step - loss: 1.3351 - acc: 0.5092 - f1_m: 0.4020 - precision_m: 0.6314 - recall_m: 0.2970 - val_loss: 1.1688 - val_acc: 0.5224 - val_f1_m: 0.4936 - val_precision_m: 0.6950 - val_recall_m: 0.3838\n",
            "Epoch 2/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 1.1101 - acc: 0.5525 - f1_m: 0.4893 - precision_m: 0.7438 - recall_m: 0.3658 - val_loss: 1.0620 - val_acc: 0.5731 - val_f1_m: 0.5175 - val_precision_m: 0.7389 - val_recall_m: 0.3992\n",
            "Epoch 3/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.9364 - acc: 0.6227 - f1_m: 0.5799 - precision_m: 0.7507 - recall_m: 0.4743 - val_loss: 0.9287 - val_acc: 0.6315 - val_f1_m: 0.5862 - val_precision_m: 0.6995 - val_recall_m: 0.5050\n",
            "Epoch 4/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.7761 - acc: 0.6871 - f1_m: 0.6622 - precision_m: 0.7473 - recall_m: 0.5952 - val_loss: 0.9071 - val_acc: 0.6473 - val_f1_m: 0.6264 - val_precision_m: 0.6823 - val_recall_m: 0.5793\n",
            "Epoch 5/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.6817 - acc: 0.7222 - f1_m: 0.7072 - precision_m: 0.7594 - recall_m: 0.6621 - val_loss: 0.9191 - val_acc: 0.6501 - val_f1_m: 0.6338 - val_precision_m: 0.6791 - val_recall_m: 0.5944\n",
            "Epoch 6/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.6113 - acc: 0.7466 - f1_m: 0.7372 - precision_m: 0.7752 - recall_m: 0.7031 - val_loss: 0.9724 - val_acc: 0.6455 - val_f1_m: 0.6349 - val_precision_m: 0.6660 - val_recall_m: 0.6068\n",
            "Epoch 7/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.5560 - acc: 0.7685 - f1_m: 0.7622 - precision_m: 0.7913 - recall_m: 0.7354 - val_loss: 1.0317 - val_acc: 0.6446 - val_f1_m: 0.6353 - val_precision_m: 0.6617 - val_recall_m: 0.6111\n",
            "Epoch 8/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.5114 - acc: 0.7877 - f1_m: 0.7837 - precision_m: 0.8078 - recall_m: 0.7612 - val_loss: 1.0896 - val_acc: 0.6308 - val_f1_m: 0.6239 - val_precision_m: 0.6462 - val_recall_m: 0.6033\n",
            "Epoch 9/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.4723 - acc: 0.8031 - f1_m: 0.8003 - precision_m: 0.8204 - recall_m: 0.7814 - val_loss: 1.1825 - val_acc: 0.6431 - val_f1_m: 0.6380 - val_precision_m: 0.6570 - val_recall_m: 0.6203\n",
            "Epoch 10/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.4369 - acc: 0.8180 - f1_m: 0.8165 - precision_m: 0.8344 - recall_m: 0.7995 - val_loss: 1.2448 - val_acc: 0.6348 - val_f1_m: 0.6313 - val_precision_m: 0.6481 - val_recall_m: 0.6153\n",
            "Epoch 11/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.4052 - acc: 0.8315 - f1_m: 0.8304 - precision_m: 0.8457 - recall_m: 0.8158 - val_loss: 1.3409 - val_acc: 0.6284 - val_f1_m: 0.6226 - val_precision_m: 0.6396 - val_recall_m: 0.6066\n",
            "Epoch 12/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.3821 - acc: 0.8412 - f1_m: 0.8403 - precision_m: 0.8547 - recall_m: 0.8264 - val_loss: 1.4214 - val_acc: 0.6282 - val_f1_m: 0.6250 - val_precision_m: 0.6368 - val_recall_m: 0.6136\n",
            "Epoch 13/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.3603 - acc: 0.8490 - f1_m: 0.8485 - precision_m: 0.8617 - recall_m: 0.8358 - val_loss: 1.4725 - val_acc: 0.6158 - val_f1_m: 0.6126 - val_precision_m: 0.6267 - val_recall_m: 0.5992\n",
            "Epoch 14/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3388 - acc: 0.8593 - f1_m: 0.8593 - precision_m: 0.8711 - recall_m: 0.8479 - val_loss: 1.5685 - val_acc: 0.6265 - val_f1_m: 0.6234 - val_precision_m: 0.6342 - val_recall_m: 0.6130\n",
            "Epoch 15/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3220 - acc: 0.8664 - f1_m: 0.8662 - precision_m: 0.8772 - recall_m: 0.8556 - val_loss: 1.6200 - val_acc: 0.6225 - val_f1_m: 0.6182 - val_precision_m: 0.6315 - val_recall_m: 0.6054\n",
            "Epoch 16/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3058 - acc: 0.8748 - f1_m: 0.8744 - precision_m: 0.8846 - recall_m: 0.8644 - val_loss: 1.7653 - val_acc: 0.6226 - val_f1_m: 0.6200 - val_precision_m: 0.6298 - val_recall_m: 0.6107\n",
            "Epoch 17/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2951 - acc: 0.8769 - f1_m: 0.8773 - precision_m: 0.8872 - recall_m: 0.8677 - val_loss: 1.7885 - val_acc: 0.6203 - val_f1_m: 0.6176 - val_precision_m: 0.6275 - val_recall_m: 0.6080\n",
            "Epoch 18/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2806 - acc: 0.8835 - f1_m: 0.8838 - precision_m: 0.8934 - recall_m: 0.8745 - val_loss: 1.8358 - val_acc: 0.6209 - val_f1_m: 0.6180 - val_precision_m: 0.6279 - val_recall_m: 0.6085\n",
            "Epoch 19/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2673 - acc: 0.8886 - f1_m: 0.8887 - precision_m: 0.8978 - recall_m: 0.8798 - val_loss: 1.8926 - val_acc: 0.6125 - val_f1_m: 0.6100 - val_precision_m: 0.6203 - val_recall_m: 0.6001\n",
            "Epoch 20/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2588 - acc: 0.8927 - f1_m: 0.8932 - precision_m: 0.9025 - recall_m: 0.8842 - val_loss: 1.9230 - val_acc: 0.6078 - val_f1_m: 0.6056 - val_precision_m: 0.6157 - val_recall_m: 0.5960\n",
            "Epoch 21/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2520 - acc: 0.8962 - f1_m: 0.8964 - precision_m: 0.9052 - recall_m: 0.8879 - val_loss: 2.0977 - val_acc: 0.6206 - val_f1_m: 0.6188 - val_precision_m: 0.6261 - val_recall_m: 0.6117\n",
            "Epoch 22/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2423 - acc: 0.8992 - f1_m: 0.8995 - precision_m: 0.9080 - recall_m: 0.8912 - val_loss: 2.0472 - val_acc: 0.6141 - val_f1_m: 0.6116 - val_precision_m: 0.6201 - val_recall_m: 0.6035\n",
            "Epoch 23/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2354 - acc: 0.9029 - f1_m: 0.9033 - precision_m: 0.9110 - recall_m: 0.8957 - val_loss: 2.1297 - val_acc: 0.6123 - val_f1_m: 0.6108 - val_precision_m: 0.6184 - val_recall_m: 0.6036\n",
            "Epoch 24/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2275 - acc: 0.9053 - f1_m: 0.9053 - precision_m: 0.9130 - recall_m: 0.8977 - val_loss: 2.1566 - val_acc: 0.6115 - val_f1_m: 0.6101 - val_precision_m: 0.6184 - val_recall_m: 0.6020\n",
            "Epoch 25/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2209 - acc: 0.9075 - f1_m: 0.9079 - precision_m: 0.9154 - recall_m: 0.9007 - val_loss: 2.2879 - val_acc: 0.6180 - val_f1_m: 0.6157 - val_precision_m: 0.6226 - val_recall_m: 0.6089\n",
            "Epoch 26/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2146 - acc: 0.9100 - f1_m: 0.9102 - precision_m: 0.9172 - recall_m: 0.9033 - val_loss: 2.3201 - val_acc: 0.6140 - val_f1_m: 0.6126 - val_precision_m: 0.6182 - val_recall_m: 0.6071\n",
            "Epoch 27/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2114 - acc: 0.9133 - f1_m: 0.9137 - precision_m: 0.9209 - recall_m: 0.9066 - val_loss: 2.2859 - val_acc: 0.6095 - val_f1_m: 0.6073 - val_precision_m: 0.6148 - val_recall_m: 0.6001\n",
            "Epoch 28/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2039 - acc: 0.9154 - f1_m: 0.9159 - precision_m: 0.9224 - recall_m: 0.9094 - val_loss: 2.4091 - val_acc: 0.6141 - val_f1_m: 0.6117 - val_precision_m: 0.6184 - val_recall_m: 0.6051\n",
            "Epoch 29/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1969 - acc: 0.9178 - f1_m: 0.9183 - precision_m: 0.9249 - recall_m: 0.9118 - val_loss: 2.4402 - val_acc: 0.6158 - val_f1_m: 0.6145 - val_precision_m: 0.6201 - val_recall_m: 0.6091\n",
            "Epoch 30/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1968 - acc: 0.9175 - f1_m: 0.9176 - precision_m: 0.9246 - recall_m: 0.9109 - val_loss: 2.3824 - val_acc: 0.6097 - val_f1_m: 0.6076 - val_precision_m: 0.6139 - val_recall_m: 0.6016\n",
            "Epoch 31/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1910 - acc: 0.9207 - f1_m: 0.9211 - precision_m: 0.9271 - recall_m: 0.9153 - val_loss: 2.4762 - val_acc: 0.6087 - val_f1_m: 0.6069 - val_precision_m: 0.6131 - val_recall_m: 0.6008\n",
            "Epoch 32/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1874 - acc: 0.9215 - f1_m: 0.9218 - precision_m: 0.9280 - recall_m: 0.9156 - val_loss: 2.4928 - val_acc: 0.6073 - val_f1_m: 0.6059 - val_precision_m: 0.6112 - val_recall_m: 0.6007\n",
            "Epoch 33/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1844 - acc: 0.9226 - f1_m: 0.9231 - precision_m: 0.9293 - recall_m: 0.9169 - val_loss: 2.5179 - val_acc: 0.6075 - val_f1_m: 0.6059 - val_precision_m: 0.6116 - val_recall_m: 0.6004\n",
            "Epoch 34/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1785 - acc: 0.9234 - f1_m: 0.9241 - precision_m: 0.9300 - recall_m: 0.9183 - val_loss: 2.5145 - val_acc: 0.6103 - val_f1_m: 0.6078 - val_precision_m: 0.6142 - val_recall_m: 0.6016\n",
            "Epoch 35/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1776 - acc: 0.9254 - f1_m: 0.9257 - precision_m: 0.9316 - recall_m: 0.9200 - val_loss: 2.5429 - val_acc: 0.6038 - val_f1_m: 0.6012 - val_precision_m: 0.6077 - val_recall_m: 0.5949\n",
            "Epoch 36/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1730 - acc: 0.9265 - f1_m: 0.9271 - precision_m: 0.9327 - recall_m: 0.9215 - val_loss: 2.6147 - val_acc: 0.6032 - val_f1_m: 0.6016 - val_precision_m: 0.6077 - val_recall_m: 0.5956\n",
            "Epoch 37/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1709 - acc: 0.9265 - f1_m: 0.9271 - precision_m: 0.9329 - recall_m: 0.9214 - val_loss: 2.5910 - val_acc: 0.6021 - val_f1_m: 0.6003 - val_precision_m: 0.6062 - val_recall_m: 0.5946\n",
            "Epoch 38/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1675 - acc: 0.9279 - f1_m: 0.9284 - precision_m: 0.9337 - recall_m: 0.9232 - val_loss: 2.6420 - val_acc: 0.6036 - val_f1_m: 0.6020 - val_precision_m: 0.6079 - val_recall_m: 0.5963\n",
            "Epoch 39/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1642 - acc: 0.9290 - f1_m: 0.9294 - precision_m: 0.9347 - recall_m: 0.9242 - val_loss: 2.7376 - val_acc: 0.6109 - val_f1_m: 0.6093 - val_precision_m: 0.6139 - val_recall_m: 0.6049\n",
            "Epoch 40/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1619 - acc: 0.9296 - f1_m: 0.9300 - precision_m: 0.9350 - recall_m: 0.9251 - val_loss: 2.7796 - val_acc: 0.6106 - val_f1_m: 0.6084 - val_precision_m: 0.6136 - val_recall_m: 0.6034\n",
            "Epoch 41/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1608 - acc: 0.9319 - f1_m: 0.9320 - precision_m: 0.9368 - recall_m: 0.9272 - val_loss: 2.7042 - val_acc: 0.6062 - val_f1_m: 0.6045 - val_precision_m: 0.6101 - val_recall_m: 0.5990\n",
            "Epoch 42/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1561 - acc: 0.9332 - f1_m: 0.9335 - precision_m: 0.9385 - recall_m: 0.9286 - val_loss: 2.8483 - val_acc: 0.6056 - val_f1_m: 0.6038 - val_precision_m: 0.6083 - val_recall_m: 0.5994\n",
            "Epoch 43/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1553 - acc: 0.9329 - f1_m: 0.9332 - precision_m: 0.9379 - recall_m: 0.9286 - val_loss: 2.7864 - val_acc: 0.6067 - val_f1_m: 0.6045 - val_precision_m: 0.6090 - val_recall_m: 0.6000\n",
            "Epoch 44/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1552 - acc: 0.9335 - f1_m: 0.9336 - precision_m: 0.9382 - recall_m: 0.9292 - val_loss: 2.8288 - val_acc: 0.6044 - val_f1_m: 0.6025 - val_precision_m: 0.6073 - val_recall_m: 0.5977\n",
            "Epoch 45/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1509 - acc: 0.9342 - f1_m: 0.9344 - precision_m: 0.9390 - recall_m: 0.9299 - val_loss: 2.8659 - val_acc: 0.6022 - val_f1_m: 0.6005 - val_precision_m: 0.6054 - val_recall_m: 0.5958\n",
            "Epoch 46/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1507 - acc: 0.9347 - f1_m: 0.9351 - precision_m: 0.9395 - recall_m: 0.9307 - val_loss: 2.8793 - val_acc: 0.6121 - val_f1_m: 0.6107 - val_precision_m: 0.6152 - val_recall_m: 0.6063\n",
            "Epoch 47/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1468 - acc: 0.9359 - f1_m: 0.9362 - precision_m: 0.9406 - recall_m: 0.9319 - val_loss: 2.9323 - val_acc: 0.6081 - val_f1_m: 0.6064 - val_precision_m: 0.6112 - val_recall_m: 0.6018\n",
            "Epoch 48/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1489 - acc: 0.9356 - f1_m: 0.9361 - precision_m: 0.9408 - recall_m: 0.9315 - val_loss: 2.8257 - val_acc: 0.6014 - val_f1_m: 0.6001 - val_precision_m: 0.6060 - val_recall_m: 0.5943\n",
            "Epoch 49/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1452 - acc: 0.9374 - f1_m: 0.9376 - precision_m: 0.9421 - recall_m: 0.9332 - val_loss: 2.9071 - val_acc: 0.6047 - val_f1_m: 0.6029 - val_precision_m: 0.6081 - val_recall_m: 0.5979\n",
            "Epoch 50/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1428 - acc: 0.9379 - f1_m: 0.9379 - precision_m: 0.9425 - recall_m: 0.9333 - val_loss: 2.8705 - val_acc: 0.5982 - val_f1_m: 0.5972 - val_precision_m: 0.6013 - val_recall_m: 0.5932\n",
            "Epoch 51/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1420 - acc: 0.9377 - f1_m: 0.9382 - precision_m: 0.9425 - recall_m: 0.9340 - val_loss: 3.0249 - val_acc: 0.6019 - val_f1_m: 0.6003 - val_precision_m: 0.6044 - val_recall_m: 0.5963\n",
            "Epoch 52/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1393 - acc: 0.9386 - f1_m: 0.9388 - precision_m: 0.9431 - recall_m: 0.9346 - val_loss: 2.9871 - val_acc: 0.6070 - val_f1_m: 0.6052 - val_precision_m: 0.6096 - val_recall_m: 0.6009\n",
            "Epoch 53/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1386 - acc: 0.9387 - f1_m: 0.9393 - precision_m: 0.9435 - recall_m: 0.9351 - val_loss: 3.1262 - val_acc: 0.6035 - val_f1_m: 0.6021 - val_precision_m: 0.6058 - val_recall_m: 0.5984\n",
            "Epoch 54/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1390 - acc: 0.9396 - f1_m: 0.9397 - precision_m: 0.9438 - recall_m: 0.9357 - val_loss: 2.9884 - val_acc: 0.5995 - val_f1_m: 0.5983 - val_precision_m: 0.6022 - val_recall_m: 0.5946\n",
            "Epoch 55/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1380 - acc: 0.9405 - f1_m: 0.9409 - precision_m: 0.9451 - recall_m: 0.9367 - val_loss: 3.0538 - val_acc: 0.6052 - val_f1_m: 0.6044 - val_precision_m: 0.6084 - val_recall_m: 0.6005\n",
            "Epoch 56/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1364 - acc: 0.9395 - f1_m: 0.9399 - precision_m: 0.9441 - recall_m: 0.9358 - val_loss: 3.0277 - val_acc: 0.6050 - val_f1_m: 0.6038 - val_precision_m: 0.6080 - val_recall_m: 0.5996\n",
            "Epoch 57/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.1337 - acc: 0.9407 - f1_m: 0.9411 - precision_m: 0.9454 - recall_m: 0.9369 - val_loss: 3.1357 - val_acc: 0.6001 - val_f1_m: 0.5986 - val_precision_m: 0.6026 - val_recall_m: 0.5947\n",
            "Epoch 58/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1327 - acc: 0.9413 - f1_m: 0.9414 - precision_m: 0.9454 - recall_m: 0.9375 - val_loss: 3.0775 - val_acc: 0.6051 - val_f1_m: 0.6033 - val_precision_m: 0.6080 - val_recall_m: 0.5987\n",
            "Epoch 59/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1309 - acc: 0.9415 - f1_m: 0.9417 - precision_m: 0.9455 - recall_m: 0.9379 - val_loss: 3.1500 - val_acc: 0.6062 - val_f1_m: 0.6047 - val_precision_m: 0.6084 - val_recall_m: 0.6011\n",
            "Epoch 60/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1312 - acc: 0.9416 - f1_m: 0.9423 - precision_m: 0.9463 - recall_m: 0.9383 - val_loss: 3.1606 - val_acc: 0.6064 - val_f1_m: 0.6052 - val_precision_m: 0.6085 - val_recall_m: 0.6020\n",
            "Epoch 61/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1295 - acc: 0.9428 - f1_m: 0.9432 - precision_m: 0.9472 - recall_m: 0.9392 - val_loss: 3.1284 - val_acc: 0.6034 - val_f1_m: 0.6028 - val_precision_m: 0.6063 - val_recall_m: 0.5994\n",
            "Epoch 62/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1297 - acc: 0.9431 - f1_m: 0.9429 - precision_m: 0.9467 - recall_m: 0.9393 - val_loss: 3.0336 - val_acc: 0.6036 - val_f1_m: 0.6025 - val_precision_m: 0.6066 - val_recall_m: 0.5986\n",
            "Epoch 63/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1266 - acc: 0.9436 - f1_m: 0.9439 - precision_m: 0.9478 - recall_m: 0.9401 - val_loss: 3.1317 - val_acc: 0.6040 - val_f1_m: 0.6033 - val_precision_m: 0.6073 - val_recall_m: 0.5994\n",
            "Epoch 64/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1233 - acc: 0.9437 - f1_m: 0.9441 - precision_m: 0.9477 - recall_m: 0.9406 - val_loss: 3.2532 - val_acc: 0.6089 - val_f1_m: 0.6073 - val_precision_m: 0.6110 - val_recall_m: 0.6036\n",
            "Epoch 65/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1250 - acc: 0.9438 - f1_m: 0.9441 - precision_m: 0.9480 - recall_m: 0.9403 - val_loss: 3.1514 - val_acc: 0.5990 - val_f1_m: 0.5970 - val_precision_m: 0.6017 - val_recall_m: 0.5924\n",
            "Epoch 66/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1241 - acc: 0.9448 - f1_m: 0.9449 - precision_m: 0.9486 - recall_m: 0.9412 - val_loss: 3.1468 - val_acc: 0.5968 - val_f1_m: 0.5959 - val_precision_m: 0.5997 - val_recall_m: 0.5922\n",
            "Epoch 67/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1269 - acc: 0.9431 - f1_m: 0.9432 - precision_m: 0.9470 - recall_m: 0.9395 - val_loss: 3.1273 - val_acc: 0.6044 - val_f1_m: 0.6037 - val_precision_m: 0.6080 - val_recall_m: 0.5995\n",
            "Epoch 68/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1233 - acc: 0.9450 - f1_m: 0.9450 - precision_m: 0.9487 - recall_m: 0.9413 - val_loss: 3.2073 - val_acc: 0.6030 - val_f1_m: 0.6023 - val_precision_m: 0.6058 - val_recall_m: 0.5988\n",
            "Epoch 69/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1220 - acc: 0.9456 - f1_m: 0.9457 - precision_m: 0.9493 - recall_m: 0.9422 - val_loss: 3.3158 - val_acc: 0.5992 - val_f1_m: 0.5972 - val_precision_m: 0.6009 - val_recall_m: 0.5936\n",
            "Epoch 70/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1195 - acc: 0.9459 - f1_m: 0.9460 - precision_m: 0.9495 - recall_m: 0.9425 - val_loss: 3.2320 - val_acc: 0.6042 - val_f1_m: 0.6029 - val_precision_m: 0.6065 - val_recall_m: 0.5994\n",
            "Epoch 71/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1235 - acc: 0.9449 - f1_m: 0.9452 - precision_m: 0.9488 - recall_m: 0.9416 - val_loss: 3.2203 - val_acc: 0.6010 - val_f1_m: 0.5995 - val_precision_m: 0.6044 - val_recall_m: 0.5946\n",
            "Epoch 72/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1211 - acc: 0.9452 - f1_m: 0.9454 - precision_m: 0.9492 - recall_m: 0.9417 - val_loss: 3.2506 - val_acc: 0.6036 - val_f1_m: 0.6020 - val_precision_m: 0.6059 - val_recall_m: 0.5982\n",
            "Epoch 73/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1181 - acc: 0.9459 - f1_m: 0.9465 - precision_m: 0.9502 - recall_m: 0.9428 - val_loss: 3.3659 - val_acc: 0.6031 - val_f1_m: 0.6025 - val_precision_m: 0.6054 - val_recall_m: 0.5996\n",
            "Epoch 74/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1179 - acc: 0.9462 - f1_m: 0.9464 - precision_m: 0.9498 - recall_m: 0.9431 - val_loss: 3.3097 - val_acc: 0.6020 - val_f1_m: 0.6018 - val_precision_m: 0.6046 - val_recall_m: 0.5991\n",
            "Epoch 75/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1184 - acc: 0.9460 - f1_m: 0.9464 - precision_m: 0.9497 - recall_m: 0.9431 - val_loss: 3.3592 - val_acc: 0.5986 - val_f1_m: 0.5970 - val_precision_m: 0.6007 - val_recall_m: 0.5934\n",
            "Epoch 76/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1183 - acc: 0.9467 - f1_m: 0.9468 - precision_m: 0.9503 - recall_m: 0.9434 - val_loss: 3.2828 - val_acc: 0.6039 - val_f1_m: 0.6020 - val_precision_m: 0.6060 - val_recall_m: 0.5982\n",
            "Epoch 77/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1176 - acc: 0.9471 - f1_m: 0.9474 - precision_m: 0.9508 - recall_m: 0.9440 - val_loss: 3.3192 - val_acc: 0.5986 - val_f1_m: 0.5960 - val_precision_m: 0.6004 - val_recall_m: 0.5918\n",
            "Epoch 78/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1164 - acc: 0.9477 - f1_m: 0.9479 - precision_m: 0.9512 - recall_m: 0.9447 - val_loss: 3.2507 - val_acc: 0.5965 - val_f1_m: 0.5958 - val_precision_m: 0.5996 - val_recall_m: 0.5922\n",
            "Epoch 79/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1154 - acc: 0.9472 - f1_m: 0.9472 - precision_m: 0.9505 - recall_m: 0.9440 - val_loss: 3.3595 - val_acc: 0.5972 - val_f1_m: 0.5964 - val_precision_m: 0.5995 - val_recall_m: 0.5934\n",
            "Epoch 80/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1173 - acc: 0.9475 - f1_m: 0.9474 - precision_m: 0.9506 - recall_m: 0.9442 - val_loss: 3.2644 - val_acc: 0.6000 - val_f1_m: 0.5994 - val_precision_m: 0.6031 - val_recall_m: 0.5958\n",
            "Epoch 81/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1141 - acc: 0.9476 - f1_m: 0.9481 - precision_m: 0.9514 - recall_m: 0.9448 - val_loss: 3.2882 - val_acc: 0.5987 - val_f1_m: 0.5972 - val_precision_m: 0.6011 - val_recall_m: 0.5934\n",
            "Epoch 82/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1125 - acc: 0.9476 - f1_m: 0.9482 - precision_m: 0.9514 - recall_m: 0.9450 - val_loss: 3.2856 - val_acc: 0.5955 - val_f1_m: 0.5947 - val_precision_m: 0.5980 - val_recall_m: 0.5914\n",
            "Epoch 83/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1104 - acc: 0.9495 - f1_m: 0.9494 - precision_m: 0.9525 - recall_m: 0.9464 - val_loss: 3.5003 - val_acc: 0.5943 - val_f1_m: 0.5933 - val_precision_m: 0.5961 - val_recall_m: 0.5906\n",
            "Epoch 84/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1120 - acc: 0.9487 - f1_m: 0.9488 - precision_m: 0.9522 - recall_m: 0.9456 - val_loss: 3.5317 - val_acc: 0.6024 - val_f1_m: 0.6018 - val_precision_m: 0.6044 - val_recall_m: 0.5991\n",
            "Epoch 85/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1130 - acc: 0.9489 - f1_m: 0.9492 - precision_m: 0.9524 - recall_m: 0.9460 - val_loss: 3.4483 - val_acc: 0.6029 - val_f1_m: 0.6024 - val_precision_m: 0.6053 - val_recall_m: 0.5995\n",
            "Epoch 86/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1135 - acc: 0.9482 - f1_m: 0.9484 - precision_m: 0.9515 - recall_m: 0.9452 - val_loss: 3.4176 - val_acc: 0.6036 - val_f1_m: 0.6030 - val_precision_m: 0.6057 - val_recall_m: 0.6003\n",
            "Epoch 87/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1111 - acc: 0.9491 - f1_m: 0.9493 - precision_m: 0.9523 - recall_m: 0.9463 - val_loss: 3.4234 - val_acc: 0.6001 - val_f1_m: 0.5998 - val_precision_m: 0.6026 - val_recall_m: 0.5970\n",
            "Epoch 88/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.1101 - acc: 0.9497 - f1_m: 0.9498 - precision_m: 0.9528 - recall_m: 0.9468 - val_loss: 3.3877 - val_acc: 0.6048 - val_f1_m: 0.6045 - val_precision_m: 0.6075 - val_recall_m: 0.6014\n",
            "Epoch 89/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1098 - acc: 0.9497 - f1_m: 0.9499 - precision_m: 0.9530 - recall_m: 0.9469 - val_loss: 3.3699 - val_acc: 0.5980 - val_f1_m: 0.5972 - val_precision_m: 0.6009 - val_recall_m: 0.5936\n",
            "Epoch 90/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1106 - acc: 0.9496 - f1_m: 0.9497 - precision_m: 0.9531 - recall_m: 0.9464 - val_loss: 3.3930 - val_acc: 0.6003 - val_f1_m: 0.5988 - val_precision_m: 0.6025 - val_recall_m: 0.5952\n",
            "Epoch 91/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1091 - acc: 0.9500 - f1_m: 0.9502 - precision_m: 0.9537 - recall_m: 0.9468 - val_loss: 3.4495 - val_acc: 0.6013 - val_f1_m: 0.6006 - val_precision_m: 0.6037 - val_recall_m: 0.5975\n",
            "Epoch 92/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1073 - acc: 0.9504 - f1_m: 0.9506 - precision_m: 0.9535 - recall_m: 0.9478 - val_loss: 3.4209 - val_acc: 0.5980 - val_f1_m: 0.5971 - val_precision_m: 0.6003 - val_recall_m: 0.5940\n",
            "Epoch 93/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1075 - acc: 0.9497 - f1_m: 0.9500 - precision_m: 0.9530 - recall_m: 0.9470 - val_loss: 3.4974 - val_acc: 0.6038 - val_f1_m: 0.6032 - val_precision_m: 0.6057 - val_recall_m: 0.6007\n",
            "Epoch 94/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1088 - acc: 0.9502 - f1_m: 0.9504 - precision_m: 0.9533 - recall_m: 0.9475 - val_loss: 3.4522 - val_acc: 0.5974 - val_f1_m: 0.5969 - val_precision_m: 0.6001 - val_recall_m: 0.5938\n",
            "Epoch 95/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1084 - acc: 0.9506 - f1_m: 0.9509 - precision_m: 0.9539 - recall_m: 0.9480 - val_loss: 3.4885 - val_acc: 0.5945 - val_f1_m: 0.5928 - val_precision_m: 0.5960 - val_recall_m: 0.5897\n",
            "Epoch 96/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1086 - acc: 0.9494 - f1_m: 0.9496 - precision_m: 0.9526 - recall_m: 0.9467 - val_loss: 3.4353 - val_acc: 0.5983 - val_f1_m: 0.5978 - val_precision_m: 0.6009 - val_recall_m: 0.5948\n",
            "Epoch 97/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1072 - acc: 0.9499 - f1_m: 0.9503 - precision_m: 0.9532 - recall_m: 0.9474 - val_loss: 3.5217 - val_acc: 0.6077 - val_f1_m: 0.6068 - val_precision_m: 0.6092 - val_recall_m: 0.6044\n",
            "Epoch 98/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1032 - acc: 0.9510 - f1_m: 0.9514 - precision_m: 0.9545 - recall_m: 0.9484 - val_loss: 3.4694 - val_acc: 0.5966 - val_f1_m: 0.5958 - val_precision_m: 0.5989 - val_recall_m: 0.5928\n",
            "Epoch 99/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1031 - acc: 0.9514 - f1_m: 0.9515 - precision_m: 0.9544 - recall_m: 0.9487 - val_loss: 3.5813 - val_acc: 0.6043 - val_f1_m: 0.6036 - val_precision_m: 0.6064 - val_recall_m: 0.6008\n",
            "Epoch 100/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1053 - acc: 0.9507 - f1_m: 0.9512 - precision_m: 0.9544 - recall_m: 0.9479 - val_loss: 3.5202 - val_acc: 0.6008 - val_f1_m: 0.6004 - val_precision_m: 0.6036 - val_recall_m: 0.5974\n",
            "Epoch 101/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1030 - acc: 0.9513 - f1_m: 0.9516 - precision_m: 0.9546 - recall_m: 0.9487 - val_loss: 3.5393 - val_acc: 0.6047 - val_f1_m: 0.6038 - val_precision_m: 0.6069 - val_recall_m: 0.6008\n",
            "Epoch 102/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1051 - acc: 0.9508 - f1_m: 0.9509 - precision_m: 0.9538 - recall_m: 0.9481 - val_loss: 3.5058 - val_acc: 0.6028 - val_f1_m: 0.6019 - val_precision_m: 0.6054 - val_recall_m: 0.5986\n",
            "Epoch 103/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1036 - acc: 0.9520 - f1_m: 0.9522 - precision_m: 0.9552 - recall_m: 0.9492 - val_loss: 3.6092 - val_acc: 0.5960 - val_f1_m: 0.5950 - val_precision_m: 0.5975 - val_recall_m: 0.5926\n",
            "Epoch 104/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1059 - acc: 0.9507 - f1_m: 0.9509 - precision_m: 0.9539 - recall_m: 0.9479 - val_loss: 3.6033 - val_acc: 0.6018 - val_f1_m: 0.6015 - val_precision_m: 0.6039 - val_recall_m: 0.5991\n",
            "Epoch 105/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1032 - acc: 0.9522 - f1_m: 0.9525 - precision_m: 0.9555 - recall_m: 0.9496 - val_loss: 3.4206 - val_acc: 0.5967 - val_f1_m: 0.5956 - val_precision_m: 0.5994 - val_recall_m: 0.5919\n",
            "Epoch 106/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1023 - acc: 0.9526 - f1_m: 0.9528 - precision_m: 0.9557 - recall_m: 0.9498 - val_loss: 3.5317 - val_acc: 0.5987 - val_f1_m: 0.5976 - val_precision_m: 0.6007 - val_recall_m: 0.5946\n",
            "Epoch 107/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1021 - acc: 0.9520 - f1_m: 0.9523 - precision_m: 0.9550 - recall_m: 0.9496 - val_loss: 3.6038 - val_acc: 0.6012 - val_f1_m: 0.6003 - val_precision_m: 0.6026 - val_recall_m: 0.5980\n",
            "Epoch 108/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1021 - acc: 0.9524 - f1_m: 0.9528 - precision_m: 0.9558 - recall_m: 0.9497 - val_loss: 3.5242 - val_acc: 0.5936 - val_f1_m: 0.5923 - val_precision_m: 0.5958 - val_recall_m: 0.5889\n",
            "Epoch 109/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1002 - acc: 0.9528 - f1_m: 0.9531 - precision_m: 0.9559 - recall_m: 0.9504 - val_loss: 3.6360 - val_acc: 0.5967 - val_f1_m: 0.5959 - val_precision_m: 0.5985 - val_recall_m: 0.5934\n",
            "Epoch 110/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1017 - acc: 0.9517 - f1_m: 0.9519 - precision_m: 0.9550 - recall_m: 0.9489 - val_loss: 3.5658 - val_acc: 0.5984 - val_f1_m: 0.5972 - val_precision_m: 0.6007 - val_recall_m: 0.5938\n",
            "Epoch 111/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1012 - acc: 0.9522 - f1_m: 0.9524 - precision_m: 0.9554 - recall_m: 0.9495 - val_loss: 3.5333 - val_acc: 0.5984 - val_f1_m: 0.5984 - val_precision_m: 0.6015 - val_recall_m: 0.5954\n",
            "Epoch 112/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0983 - acc: 0.9537 - f1_m: 0.9540 - precision_m: 0.9569 - recall_m: 0.9512 - val_loss: 3.5622 - val_acc: 0.6014 - val_f1_m: 0.6011 - val_precision_m: 0.6042 - val_recall_m: 0.5980\n",
            "Epoch 113/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0990 - acc: 0.9529 - f1_m: 0.9529 - precision_m: 0.9557 - recall_m: 0.9502 - val_loss: 3.6670 - val_acc: 0.5947 - val_f1_m: 0.5935 - val_precision_m: 0.5967 - val_recall_m: 0.5904\n",
            "Epoch 114/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0994 - acc: 0.9525 - f1_m: 0.9527 - precision_m: 0.9555 - recall_m: 0.9500 - val_loss: 3.6488 - val_acc: 0.5975 - val_f1_m: 0.5969 - val_precision_m: 0.5996 - val_recall_m: 0.5943\n",
            "Epoch 115/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0987 - acc: 0.9533 - f1_m: 0.9535 - precision_m: 0.9564 - recall_m: 0.9506 - val_loss: 3.6105 - val_acc: 0.5984 - val_f1_m: 0.5981 - val_precision_m: 0.6012 - val_recall_m: 0.5951\n",
            "Epoch 116/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0991 - acc: 0.9536 - f1_m: 0.9536 - precision_m: 0.9565 - recall_m: 0.9508 - val_loss: 3.5826 - val_acc: 0.6021 - val_f1_m: 0.6014 - val_precision_m: 0.6046 - val_recall_m: 0.5983\n",
            "Epoch 117/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0992 - acc: 0.9524 - f1_m: 0.9526 - precision_m: 0.9553 - recall_m: 0.9499 - val_loss: 3.6185 - val_acc: 0.6024 - val_f1_m: 0.6013 - val_precision_m: 0.6039 - val_recall_m: 0.5988\n",
            "Epoch 118/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0983 - acc: 0.9533 - f1_m: 0.9534 - precision_m: 0.9562 - recall_m: 0.9508 - val_loss: 3.6333 - val_acc: 0.6004 - val_f1_m: 0.6000 - val_precision_m: 0.6033 - val_recall_m: 0.5967\n",
            "Epoch 119/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0981 - acc: 0.9537 - f1_m: 0.9538 - precision_m: 0.9570 - recall_m: 0.9507 - val_loss: 3.6579 - val_acc: 0.6043 - val_f1_m: 0.6042 - val_precision_m: 0.6069 - val_recall_m: 0.6015\n",
            "Epoch 120/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.0954 - acc: 0.9545 - f1_m: 0.9547 - precision_m: 0.9574 - recall_m: 0.9520 - val_loss: 3.6630 - val_acc: 0.5976 - val_f1_m: 0.5969 - val_precision_m: 0.5995 - val_recall_m: 0.5944\n",
            "Epoch 121/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0997 - acc: 0.9527 - f1_m: 0.9529 - precision_m: 0.9559 - recall_m: 0.9499 - val_loss: 3.6859 - val_acc: 0.5996 - val_f1_m: 0.5988 - val_precision_m: 0.6017 - val_recall_m: 0.5959\n",
            "Epoch 122/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0980 - acc: 0.9531 - f1_m: 0.9532 - precision_m: 0.9559 - recall_m: 0.9505 - val_loss: 3.6709 - val_acc: 0.6023 - val_f1_m: 0.6015 - val_precision_m: 0.6042 - val_recall_m: 0.5989\n",
            "Epoch 123/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0960 - acc: 0.9537 - f1_m: 0.9539 - precision_m: 0.9567 - recall_m: 0.9510 - val_loss: 3.6516 - val_acc: 0.5977 - val_f1_m: 0.5968 - val_precision_m: 0.6001 - val_recall_m: 0.5936\n",
            "Epoch 124/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0956 - acc: 0.9545 - f1_m: 0.9548 - precision_m: 0.9576 - recall_m: 0.9520 - val_loss: 3.6433 - val_acc: 0.6006 - val_f1_m: 0.5999 - val_precision_m: 0.6028 - val_recall_m: 0.5970\n",
            "Epoch 125/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0953 - acc: 0.9548 - f1_m: 0.9551 - precision_m: 0.9577 - recall_m: 0.9525 - val_loss: 3.7097 - val_acc: 0.6026 - val_f1_m: 0.6020 - val_precision_m: 0.6048 - val_recall_m: 0.5992\n",
            "Epoch 126/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0974 - acc: 0.9543 - f1_m: 0.9545 - precision_m: 0.9573 - recall_m: 0.9518 - val_loss: 3.5477 - val_acc: 0.5939 - val_f1_m: 0.5933 - val_precision_m: 0.5967 - val_recall_m: 0.5900\n",
            "Epoch 127/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0960 - acc: 0.9552 - f1_m: 0.9555 - precision_m: 0.9582 - recall_m: 0.9527 - val_loss: 3.6376 - val_acc: 0.5934 - val_f1_m: 0.5930 - val_precision_m: 0.5962 - val_recall_m: 0.5899\n",
            "Epoch 128/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0951 - acc: 0.9539 - f1_m: 0.9542 - precision_m: 0.9567 - recall_m: 0.9518 - val_loss: 3.7154 - val_acc: 0.6036 - val_f1_m: 0.6028 - val_precision_m: 0.6051 - val_recall_m: 0.6005\n",
            "Epoch 129/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0955 - acc: 0.9541 - f1_m: 0.9545 - precision_m: 0.9571 - recall_m: 0.9519 - val_loss: 3.6686 - val_acc: 0.6000 - val_f1_m: 0.5998 - val_precision_m: 0.6024 - val_recall_m: 0.5973\n",
            "Epoch 130/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0949 - acc: 0.9539 - f1_m: 0.9541 - precision_m: 0.9568 - recall_m: 0.9513 - val_loss: 3.5891 - val_acc: 0.5957 - val_f1_m: 0.5955 - val_precision_m: 0.5987 - val_recall_m: 0.5924\n",
            "Epoch 131/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0940 - acc: 0.9543 - f1_m: 0.9545 - precision_m: 0.9571 - recall_m: 0.9519 - val_loss: 3.6506 - val_acc: 0.5953 - val_f1_m: 0.5951 - val_precision_m: 0.5984 - val_recall_m: 0.5919\n",
            "Epoch 132/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0950 - acc: 0.9545 - f1_m: 0.9547 - precision_m: 0.9573 - recall_m: 0.9522 - val_loss: 3.6535 - val_acc: 0.6004 - val_f1_m: 0.5999 - val_precision_m: 0.6028 - val_recall_m: 0.5971\n",
            "Epoch 133/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0937 - acc: 0.9553 - f1_m: 0.9552 - precision_m: 0.9582 - recall_m: 0.9523 - val_loss: 3.6646 - val_acc: 0.5962 - val_f1_m: 0.5960 - val_precision_m: 0.5992 - val_recall_m: 0.5928\n",
            "Epoch 134/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0946 - acc: 0.9544 - f1_m: 0.9546 - precision_m: 0.9573 - recall_m: 0.9519 - val_loss: 3.6743 - val_acc: 0.5985 - val_f1_m: 0.5974 - val_precision_m: 0.6006 - val_recall_m: 0.5943\n",
            "Epoch 135/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0949 - acc: 0.9549 - f1_m: 0.9551 - precision_m: 0.9578 - recall_m: 0.9525 - val_loss: 3.7217 - val_acc: 0.5992 - val_f1_m: 0.5987 - val_precision_m: 0.6014 - val_recall_m: 0.5960\n",
            "Epoch 136/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0922 - acc: 0.9560 - f1_m: 0.9561 - precision_m: 0.9588 - recall_m: 0.9536 - val_loss: 3.7575 - val_acc: 0.5995 - val_f1_m: 0.5994 - val_precision_m: 0.6023 - val_recall_m: 0.5966\n",
            "Epoch 137/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0927 - acc: 0.9551 - f1_m: 0.9554 - precision_m: 0.9578 - recall_m: 0.9530 - val_loss: 3.7487 - val_acc: 0.5991 - val_f1_m: 0.5986 - val_precision_m: 0.6012 - val_recall_m: 0.5960\n",
            "Epoch 138/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0929 - acc: 0.9549 - f1_m: 0.9551 - precision_m: 0.9578 - recall_m: 0.9524 - val_loss: 3.6370 - val_acc: 0.5958 - val_f1_m: 0.5946 - val_precision_m: 0.5983 - val_recall_m: 0.5911\n",
            "Epoch 139/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0921 - acc: 0.9550 - f1_m: 0.9552 - precision_m: 0.9579 - recall_m: 0.9526 - val_loss: 3.7487 - val_acc: 0.5937 - val_f1_m: 0.5933 - val_precision_m: 0.5964 - val_recall_m: 0.5903\n",
            "Epoch 140/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0919 - acc: 0.9556 - f1_m: 0.9559 - precision_m: 0.9587 - recall_m: 0.9532 - val_loss: 3.6969 - val_acc: 0.5978 - val_f1_m: 0.5968 - val_precision_m: 0.6002 - val_recall_m: 0.5934\n",
            "Epoch 141/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0938 - acc: 0.9554 - f1_m: 0.9556 - precision_m: 0.9583 - recall_m: 0.9530 - val_loss: 3.7211 - val_acc: 0.6006 - val_f1_m: 0.5998 - val_precision_m: 0.6030 - val_recall_m: 0.5966\n",
            "Epoch 142/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0907 - acc: 0.9558 - f1_m: 0.9559 - precision_m: 0.9584 - recall_m: 0.9534 - val_loss: 3.7568 - val_acc: 0.6007 - val_f1_m: 0.5993 - val_precision_m: 0.6027 - val_recall_m: 0.5961\n",
            "Epoch 143/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0931 - acc: 0.9557 - f1_m: 0.9558 - precision_m: 0.9587 - recall_m: 0.9531 - val_loss: 3.7680 - val_acc: 0.5943 - val_f1_m: 0.5935 - val_precision_m: 0.5970 - val_recall_m: 0.5901\n",
            "Epoch 144/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0918 - acc: 0.9557 - f1_m: 0.9558 - precision_m: 0.9585 - recall_m: 0.9531 - val_loss: 3.7406 - val_acc: 0.5974 - val_f1_m: 0.5966 - val_precision_m: 0.5995 - val_recall_m: 0.5936\n",
            "Epoch 145/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0931 - acc: 0.9551 - f1_m: 0.9554 - precision_m: 0.9581 - recall_m: 0.9528 - val_loss: 3.7387 - val_acc: 0.5972 - val_f1_m: 0.5964 - val_precision_m: 0.5996 - val_recall_m: 0.5932\n",
            "Epoch 146/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0928 - acc: 0.9548 - f1_m: 0.9550 - precision_m: 0.9576 - recall_m: 0.9525 - val_loss: 3.7216 - val_acc: 0.5986 - val_f1_m: 0.5980 - val_precision_m: 0.6007 - val_recall_m: 0.5954\n",
            "Epoch 147/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0885 - acc: 0.9561 - f1_m: 0.9564 - precision_m: 0.9588 - recall_m: 0.9539 - val_loss: 3.7772 - val_acc: 0.5982 - val_f1_m: 0.5979 - val_precision_m: 0.6012 - val_recall_m: 0.5947\n",
            "Epoch 148/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.0899 - acc: 0.9561 - f1_m: 0.9565 - precision_m: 0.9590 - recall_m: 0.9539 - val_loss: 3.6137 - val_acc: 0.5939 - val_f1_m: 0.5930 - val_precision_m: 0.5965 - val_recall_m: 0.5896\n",
            "Epoch 149/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0913 - acc: 0.9561 - f1_m: 0.9562 - precision_m: 0.9588 - recall_m: 0.9536 - val_loss: 3.6901 - val_acc: 0.5998 - val_f1_m: 0.5993 - val_precision_m: 0.6026 - val_recall_m: 0.5961\n",
            "Epoch 150/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0906 - acc: 0.9564 - f1_m: 0.9566 - precision_m: 0.9591 - recall_m: 0.9542 - val_loss: 3.7856 - val_acc: 0.5987 - val_f1_m: 0.5977 - val_precision_m: 0.6008 - val_recall_m: 0.5946\n",
            "Epoch 151/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0934 - acc: 0.9558 - f1_m: 0.9559 - precision_m: 0.9586 - recall_m: 0.9533 - val_loss: 3.7311 - val_acc: 0.5973 - val_f1_m: 0.5966 - val_precision_m: 0.5996 - val_recall_m: 0.5938\n",
            "Epoch 152/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0903 - acc: 0.9560 - f1_m: 0.9563 - precision_m: 0.9589 - recall_m: 0.9537 - val_loss: 3.7297 - val_acc: 0.5929 - val_f1_m: 0.5916 - val_precision_m: 0.5950 - val_recall_m: 0.5882\n",
            "Epoch 153/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0880 - acc: 0.9567 - f1_m: 0.9568 - precision_m: 0.9594 - recall_m: 0.9543 - val_loss: 3.8710 - val_acc: 0.5958 - val_f1_m: 0.5947 - val_precision_m: 0.5974 - val_recall_m: 0.5920\n",
            "Epoch 154/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0896 - acc: 0.9566 - f1_m: 0.9568 - precision_m: 0.9593 - recall_m: 0.9543 - val_loss: 3.8225 - val_acc: 0.5978 - val_f1_m: 0.5971 - val_precision_m: 0.5998 - val_recall_m: 0.5944\n",
            "Epoch 155/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0897 - acc: 0.9565 - f1_m: 0.9568 - precision_m: 0.9594 - recall_m: 0.9542 - val_loss: 3.8503 - val_acc: 0.5970 - val_f1_m: 0.5962 - val_precision_m: 0.5991 - val_recall_m: 0.5934\n",
            "Epoch 156/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0899 - acc: 0.9568 - f1_m: 0.9568 - precision_m: 0.9593 - recall_m: 0.9544 - val_loss: 3.8142 - val_acc: 0.5903 - val_f1_m: 0.5893 - val_precision_m: 0.5926 - val_recall_m: 0.5860\n",
            "Epoch 157/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0886 - acc: 0.9564 - f1_m: 0.9566 - precision_m: 0.9591 - recall_m: 0.9541 - val_loss: 3.9051 - val_acc: 0.5933 - val_f1_m: 0.5927 - val_precision_m: 0.5953 - val_recall_m: 0.5901\n",
            "Epoch 158/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0902 - acc: 0.9557 - f1_m: 0.9560 - precision_m: 0.9584 - recall_m: 0.9536 - val_loss: 3.7431 - val_acc: 0.5946 - val_f1_m: 0.5939 - val_precision_m: 0.5970 - val_recall_m: 0.5909\n",
            "Epoch 159/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0897 - acc: 0.9562 - f1_m: 0.9565 - precision_m: 0.9593 - recall_m: 0.9538 - val_loss: 3.7357 - val_acc: 0.5965 - val_f1_m: 0.5959 - val_precision_m: 0.5994 - val_recall_m: 0.5925\n",
            "Epoch 160/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0886 - acc: 0.9560 - f1_m: 0.9565 - precision_m: 0.9590 - recall_m: 0.9539 - val_loss: 3.7718 - val_acc: 0.5923 - val_f1_m: 0.5918 - val_precision_m: 0.5950 - val_recall_m: 0.5887\n",
            "Epoch 161/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0890 - acc: 0.9561 - f1_m: 0.9563 - precision_m: 0.9588 - recall_m: 0.9538 - val_loss: 3.8184 - val_acc: 0.5923 - val_f1_m: 0.5918 - val_precision_m: 0.5947 - val_recall_m: 0.5888\n",
            "Epoch 162/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0885 - acc: 0.9565 - f1_m: 0.9567 - precision_m: 0.9593 - recall_m: 0.9542 - val_loss: 3.7863 - val_acc: 0.5915 - val_f1_m: 0.5908 - val_precision_m: 0.5940 - val_recall_m: 0.5876\n",
            "Epoch 163/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0889 - acc: 0.9562 - f1_m: 0.9563 - precision_m: 0.9590 - recall_m: 0.9537 - val_loss: 3.8217 - val_acc: 0.5940 - val_f1_m: 0.5934 - val_precision_m: 0.5963 - val_recall_m: 0.5905\n",
            "Epoch 164/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0869 - acc: 0.9565 - f1_m: 0.9566 - precision_m: 0.9593 - recall_m: 0.9540 - val_loss: 3.8654 - val_acc: 0.5978 - val_f1_m: 0.5969 - val_precision_m: 0.5998 - val_recall_m: 0.5940\n",
            "Epoch 165/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0876 - acc: 0.9564 - f1_m: 0.9566 - precision_m: 0.9589 - recall_m: 0.9544 - val_loss: 3.7887 - val_acc: 0.5975 - val_f1_m: 0.5967 - val_precision_m: 0.5999 - val_recall_m: 0.5937\n",
            "Epoch 166/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0887 - acc: 0.9567 - f1_m: 0.9571 - precision_m: 0.9596 - recall_m: 0.9546 - val_loss: 3.6868 - val_acc: 0.5967 - val_f1_m: 0.5960 - val_precision_m: 0.5998 - val_recall_m: 0.5923\n",
            "Epoch 167/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0877 - acc: 0.9568 - f1_m: 0.9570 - precision_m: 0.9596 - recall_m: 0.9545 - val_loss: 3.8467 - val_acc: 0.5982 - val_f1_m: 0.5976 - val_precision_m: 0.6006 - val_recall_m: 0.5946\n",
            "Epoch 168/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0864 - acc: 0.9572 - f1_m: 0.9573 - precision_m: 0.9600 - recall_m: 0.9548 - val_loss: 3.8490 - val_acc: 0.6004 - val_f1_m: 0.5999 - val_precision_m: 0.6025 - val_recall_m: 0.5974\n",
            "Epoch 169/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0907 - acc: 0.9567 - f1_m: 0.9568 - precision_m: 0.9593 - recall_m: 0.9543 - val_loss: 3.7837 - val_acc: 0.5996 - val_f1_m: 0.5994 - val_precision_m: 0.6024 - val_recall_m: 0.5966\n",
            "Epoch 170/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0886 - acc: 0.9561 - f1_m: 0.9561 - precision_m: 0.9587 - recall_m: 0.9536 - val_loss: 3.8047 - val_acc: 0.5979 - val_f1_m: 0.5972 - val_precision_m: 0.6004 - val_recall_m: 0.5942\n",
            "Epoch 171/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0852 - acc: 0.9573 - f1_m: 0.9574 - precision_m: 0.9601 - recall_m: 0.9547 - val_loss: 3.9156 - val_acc: 0.5971 - val_f1_m: 0.5970 - val_precision_m: 0.5996 - val_recall_m: 0.5944\n",
            "Epoch 172/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0856 - acc: 0.9573 - f1_m: 0.9574 - precision_m: 0.9599 - recall_m: 0.9549 - val_loss: 3.8895 - val_acc: 0.5974 - val_f1_m: 0.5963 - val_precision_m: 0.5995 - val_recall_m: 0.5932\n",
            "Epoch 173/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0855 - acc: 0.9574 - f1_m: 0.9576 - precision_m: 0.9601 - recall_m: 0.9551 - val_loss: 3.9089 - val_acc: 0.5926 - val_f1_m: 0.5917 - val_precision_m: 0.5947 - val_recall_m: 0.5887\n",
            "Epoch 174/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0860 - acc: 0.9574 - f1_m: 0.9574 - precision_m: 0.9598 - recall_m: 0.9551 - val_loss: 3.7409 - val_acc: 0.5928 - val_f1_m: 0.5918 - val_precision_m: 0.5957 - val_recall_m: 0.5881\n",
            "Epoch 175/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0861 - acc: 0.9576 - f1_m: 0.9580 - precision_m: 0.9605 - recall_m: 0.9556 - val_loss: 3.8656 - val_acc: 0.5985 - val_f1_m: 0.5978 - val_precision_m: 0.6008 - val_recall_m: 0.5949\n",
            "Epoch 176/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0875 - acc: 0.9577 - f1_m: 0.9580 - precision_m: 0.9607 - recall_m: 0.9554 - val_loss: 3.7093 - val_acc: 0.5976 - val_f1_m: 0.5967 - val_precision_m: 0.6006 - val_recall_m: 0.5928\n",
            "Epoch 177/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0873 - acc: 0.9569 - f1_m: 0.9570 - precision_m: 0.9597 - recall_m: 0.9544 - val_loss: 3.9141 - val_acc: 0.5963 - val_f1_m: 0.5962 - val_precision_m: 0.5991 - val_recall_m: 0.5934\n",
            "Epoch 178/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0851 - acc: 0.9580 - f1_m: 0.9581 - precision_m: 0.9609 - recall_m: 0.9554 - val_loss: 3.8686 - val_acc: 0.5971 - val_f1_m: 0.5971 - val_precision_m: 0.6000 - val_recall_m: 0.5944\n",
            "Epoch 179/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0853 - acc: 0.9579 - f1_m: 0.9583 - precision_m: 0.9605 - recall_m: 0.9560 - val_loss: 3.9697 - val_acc: 0.5943 - val_f1_m: 0.5940 - val_precision_m: 0.5965 - val_recall_m: 0.5914\n",
            "Epoch 180/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0861 - acc: 0.9572 - f1_m: 0.9575 - precision_m: 0.9601 - recall_m: 0.9550 - val_loss: 3.8652 - val_acc: 0.5972 - val_f1_m: 0.5971 - val_precision_m: 0.6001 - val_recall_m: 0.5942\n",
            "Epoch 181/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0874 - acc: 0.9572 - f1_m: 0.9574 - precision_m: 0.9601 - recall_m: 0.9548 - val_loss: 3.8727 - val_acc: 0.6010 - val_f1_m: 0.6003 - val_precision_m: 0.6033 - val_recall_m: 0.5974\n",
            "Epoch 182/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0847 - acc: 0.9577 - f1_m: 0.9578 - precision_m: 0.9605 - recall_m: 0.9550 - val_loss: 3.8465 - val_acc: 0.6000 - val_f1_m: 0.5995 - val_precision_m: 0.6023 - val_recall_m: 0.5968\n",
            "Epoch 183/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.0851 - acc: 0.9575 - f1_m: 0.9578 - precision_m: 0.9603 - recall_m: 0.9553 - val_loss: 3.8761 - val_acc: 0.5959 - val_f1_m: 0.5952 - val_precision_m: 0.5985 - val_recall_m: 0.5920\n",
            "Epoch 184/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0866 - acc: 0.9574 - f1_m: 0.9574 - precision_m: 0.9599 - recall_m: 0.9550 - val_loss: 3.9209 - val_acc: 0.5942 - val_f1_m: 0.5940 - val_precision_m: 0.5966 - val_recall_m: 0.5914\n",
            "Epoch 185/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0876 - acc: 0.9569 - f1_m: 0.9572 - precision_m: 0.9597 - recall_m: 0.9547 - val_loss: 3.8363 - val_acc: 0.5980 - val_f1_m: 0.5973 - val_precision_m: 0.6005 - val_recall_m: 0.5942\n",
            "Epoch 186/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0841 - acc: 0.9581 - f1_m: 0.9582 - precision_m: 0.9607 - recall_m: 0.9557 - val_loss: 3.9053 - val_acc: 0.5978 - val_f1_m: 0.5976 - val_precision_m: 0.6003 - val_recall_m: 0.5950\n",
            "Epoch 187/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0825 - acc: 0.9588 - f1_m: 0.9589 - precision_m: 0.9613 - recall_m: 0.9566 - val_loss: 3.8070 - val_acc: 0.5994 - val_f1_m: 0.5994 - val_precision_m: 0.6026 - val_recall_m: 0.5962\n",
            "Epoch 188/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0853 - acc: 0.9580 - f1_m: 0.9582 - precision_m: 0.9607 - recall_m: 0.9557 - val_loss: 3.8758 - val_acc: 0.5967 - val_f1_m: 0.5959 - val_precision_m: 0.5988 - val_recall_m: 0.5931\n",
            "Epoch 189/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0821 - acc: 0.9581 - f1_m: 0.9583 - precision_m: 0.9606 - recall_m: 0.9560 - val_loss: 3.8343 - val_acc: 0.5989 - val_f1_m: 0.5985 - val_precision_m: 0.6015 - val_recall_m: 0.5956\n",
            "Epoch 190/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0844 - acc: 0.9585 - f1_m: 0.9588 - precision_m: 0.9611 - recall_m: 0.9564 - val_loss: 3.9005 - val_acc: 0.5924 - val_f1_m: 0.5921 - val_precision_m: 0.5956 - val_recall_m: 0.5888\n",
            "Epoch 191/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0833 - acc: 0.9586 - f1_m: 0.9588 - precision_m: 0.9612 - recall_m: 0.9564 - val_loss: 3.9292 - val_acc: 0.5982 - val_f1_m: 0.5978 - val_precision_m: 0.6005 - val_recall_m: 0.5952\n",
            "Epoch 192/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0843 - acc: 0.9576 - f1_m: 0.9578 - precision_m: 0.9604 - recall_m: 0.9552 - val_loss: 3.8969 - val_acc: 0.5990 - val_f1_m: 0.5985 - val_precision_m: 0.6017 - val_recall_m: 0.5954\n",
            "Epoch 193/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0829 - acc: 0.9584 - f1_m: 0.9588 - precision_m: 0.9613 - recall_m: 0.9563 - val_loss: 3.9469 - val_acc: 0.5988 - val_f1_m: 0.5984 - val_precision_m: 0.6012 - val_recall_m: 0.5956\n",
            "Epoch 194/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0810 - acc: 0.9588 - f1_m: 0.9590 - precision_m: 0.9614 - recall_m: 0.9566 - val_loss: 3.9398 - val_acc: 0.6061 - val_f1_m: 0.6058 - val_precision_m: 0.6083 - val_recall_m: 0.6033\n",
            "Epoch 195/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0842 - acc: 0.9579 - f1_m: 0.9580 - precision_m: 0.9606 - recall_m: 0.9555 - val_loss: 3.8897 - val_acc: 0.6016 - val_f1_m: 0.6016 - val_precision_m: 0.6042 - val_recall_m: 0.5991\n",
            "Epoch 196/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0848 - acc: 0.9574 - f1_m: 0.9576 - precision_m: 0.9601 - recall_m: 0.9552 - val_loss: 3.9193 - val_acc: 0.5957 - val_f1_m: 0.5953 - val_precision_m: 0.5984 - val_recall_m: 0.5922\n",
            "Epoch 197/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0835 - acc: 0.9585 - f1_m: 0.9590 - precision_m: 0.9617 - recall_m: 0.9564 - val_loss: 3.9593 - val_acc: 0.5955 - val_f1_m: 0.5950 - val_precision_m: 0.5978 - val_recall_m: 0.5922\n",
            "Epoch 198/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0828 - acc: 0.9589 - f1_m: 0.9592 - precision_m: 0.9620 - recall_m: 0.9566 - val_loss: 3.9078 - val_acc: 0.5993 - val_f1_m: 0.5987 - val_precision_m: 0.6020 - val_recall_m: 0.5955\n",
            "Epoch 199/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0827 - acc: 0.9585 - f1_m: 0.9588 - precision_m: 0.9614 - recall_m: 0.9563 - val_loss: 3.8496 - val_acc: 0.5948 - val_f1_m: 0.5936 - val_precision_m: 0.5973 - val_recall_m: 0.5900\n",
            "Epoch 200/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0843 - acc: 0.9581 - f1_m: 0.9585 - precision_m: 0.9613 - recall_m: 0.9558 - val_loss: 3.9382 - val_acc: 0.5987 - val_f1_m: 0.5983 - val_precision_m: 0.6011 - val_recall_m: 0.5955\n",
            "Epoch 201/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0821 - acc: 0.9586 - f1_m: 0.9588 - precision_m: 0.9611 - recall_m: 0.9565 - val_loss: 3.9761 - val_acc: 0.5959 - val_f1_m: 0.5957 - val_precision_m: 0.5987 - val_recall_m: 0.5928\n",
            "Epoch 202/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0819 - acc: 0.9583 - f1_m: 0.9585 - precision_m: 0.9611 - recall_m: 0.9559 - val_loss: 3.8946 - val_acc: 0.5953 - val_f1_m: 0.5942 - val_precision_m: 0.5975 - val_recall_m: 0.5909\n",
            "Epoch 203/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0830 - acc: 0.9590 - f1_m: 0.9591 - precision_m: 0.9616 - recall_m: 0.9565 - val_loss: 3.8992 - val_acc: 0.5980 - val_f1_m: 0.5969 - val_precision_m: 0.6002 - val_recall_m: 0.5937\n",
            "Epoch 204/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0830 - acc: 0.9590 - f1_m: 0.9593 - precision_m: 0.9620 - recall_m: 0.9566 - val_loss: 3.9551 - val_acc: 0.5948 - val_f1_m: 0.5944 - val_precision_m: 0.5978 - val_recall_m: 0.5911\n",
            "Epoch 205/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0831 - acc: 0.9588 - f1_m: 0.9590 - precision_m: 0.9617 - recall_m: 0.9563 - val_loss: 3.8699 - val_acc: 0.5984 - val_f1_m: 0.5975 - val_precision_m: 0.6012 - val_recall_m: 0.5938\n",
            "Epoch 206/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0825 - acc: 0.9582 - f1_m: 0.9586 - precision_m: 0.9610 - recall_m: 0.9562 - val_loss: 3.9482 - val_acc: 0.5996 - val_f1_m: 0.5993 - val_precision_m: 0.6018 - val_recall_m: 0.5969\n",
            "Epoch 207/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0824 - acc: 0.9589 - f1_m: 0.9590 - precision_m: 0.9613 - recall_m: 0.9567 - val_loss: 3.8929 - val_acc: 0.5980 - val_f1_m: 0.5978 - val_precision_m: 0.6010 - val_recall_m: 0.5946\n",
            "Epoch 208/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0824 - acc: 0.9586 - f1_m: 0.9589 - precision_m: 0.9612 - recall_m: 0.9566 - val_loss: 3.8014 - val_acc: 0.5976 - val_f1_m: 0.5970 - val_precision_m: 0.6007 - val_recall_m: 0.5934\n",
            "Epoch 209/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0806 - acc: 0.9593 - f1_m: 0.9592 - precision_m: 0.9620 - recall_m: 0.9566 - val_loss: 3.8956 - val_acc: 0.5982 - val_f1_m: 0.5976 - val_precision_m: 0.6009 - val_recall_m: 0.5943\n",
            "Epoch 210/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0851 - acc: 0.9583 - f1_m: 0.9585 - precision_m: 0.9612 - recall_m: 0.9557 - val_loss: 3.8964 - val_acc: 0.5992 - val_f1_m: 0.5987 - val_precision_m: 0.6018 - val_recall_m: 0.5957\n",
            "Epoch 211/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0808 - acc: 0.9587 - f1_m: 0.9587 - precision_m: 0.9614 - recall_m: 0.9560 - val_loss: 3.8847 - val_acc: 0.5981 - val_f1_m: 0.5974 - val_precision_m: 0.6007 - val_recall_m: 0.5942\n",
            "Epoch 212/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0815 - acc: 0.9590 - f1_m: 0.9592 - precision_m: 0.9617 - recall_m: 0.9568 - val_loss: 3.9306 - val_acc: 0.5972 - val_f1_m: 0.5967 - val_precision_m: 0.5995 - val_recall_m: 0.5939\n",
            "Epoch 213/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0796 - acc: 0.9598 - f1_m: 0.9599 - precision_m: 0.9623 - recall_m: 0.9576 - val_loss: 3.8888 - val_acc: 0.5992 - val_f1_m: 0.5990 - val_precision_m: 0.6022 - val_recall_m: 0.5959\n",
            "Epoch 214/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0816 - acc: 0.9589 - f1_m: 0.9592 - precision_m: 0.9616 - recall_m: 0.9568 - val_loss: 3.8679 - val_acc: 0.5988 - val_f1_m: 0.5981 - val_precision_m: 0.6015 - val_recall_m: 0.5948\n",
            "Epoch 215/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.0828 - acc: 0.9584 - f1_m: 0.9587 - precision_m: 0.9614 - recall_m: 0.9560 - val_loss: 3.9166 - val_acc: 0.5957 - val_f1_m: 0.5953 - val_precision_m: 0.5986 - val_recall_m: 0.5921\n",
            "Epoch 216/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0801 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9620 - recall_m: 0.9572 - val_loss: 3.9445 - val_acc: 0.5938 - val_f1_m: 0.5929 - val_precision_m: 0.5965 - val_recall_m: 0.5895\n",
            "Epoch 217/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0823 - acc: 0.9586 - f1_m: 0.9589 - precision_m: 0.9615 - recall_m: 0.9564 - val_loss: 3.9574 - val_acc: 0.6009 - val_f1_m: 0.6005 - val_precision_m: 0.6034 - val_recall_m: 0.5976\n",
            "Epoch 218/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0805 - acc: 0.9595 - f1_m: 0.9597 - precision_m: 0.9621 - recall_m: 0.9573 - val_loss: 4.0811 - val_acc: 0.5951 - val_f1_m: 0.5949 - val_precision_m: 0.5979 - val_recall_m: 0.5919\n",
            "Epoch 219/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0800 - acc: 0.9583 - f1_m: 0.9587 - precision_m: 0.9612 - recall_m: 0.9563 - val_loss: 4.0085 - val_acc: 0.5979 - val_f1_m: 0.5973 - val_precision_m: 0.6005 - val_recall_m: 0.5942\n",
            "Epoch 220/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0814 - acc: 0.9591 - f1_m: 0.9593 - precision_m: 0.9620 - recall_m: 0.9567 - val_loss: 3.9298 - val_acc: 0.5958 - val_f1_m: 0.5951 - val_precision_m: 0.5987 - val_recall_m: 0.5915\n",
            "Epoch 221/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0808 - acc: 0.9591 - f1_m: 0.9593 - precision_m: 0.9619 - recall_m: 0.9567 - val_loss: 4.0394 - val_acc: 0.5968 - val_f1_m: 0.5963 - val_precision_m: 0.5992 - val_recall_m: 0.5934\n",
            "Epoch 222/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0795 - acc: 0.9591 - f1_m: 0.9594 - precision_m: 0.9619 - recall_m: 0.9569 - val_loss: 3.9107 - val_acc: 0.5918 - val_f1_m: 0.5910 - val_precision_m: 0.5949 - val_recall_m: 0.5871\n",
            "Epoch 223/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0802 - acc: 0.9594 - f1_m: 0.9595 - precision_m: 0.9621 - recall_m: 0.9569 - val_loss: 4.0895 - val_acc: 0.5927 - val_f1_m: 0.5930 - val_precision_m: 0.5959 - val_recall_m: 0.5901\n",
            "Epoch 224/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0817 - acc: 0.9596 - f1_m: 0.9599 - precision_m: 0.9625 - recall_m: 0.9573 - val_loss: 3.8310 - val_acc: 0.5936 - val_f1_m: 0.5934 - val_precision_m: 0.5974 - val_recall_m: 0.5896\n",
            "Epoch 225/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0832 - acc: 0.9585 - f1_m: 0.9589 - precision_m: 0.9614 - recall_m: 0.9563 - val_loss: 3.9633 - val_acc: 0.5898 - val_f1_m: 0.5895 - val_precision_m: 0.5932 - val_recall_m: 0.5859\n",
            "Epoch 226/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0796 - acc: 0.9597 - f1_m: 0.9599 - precision_m: 0.9623 - recall_m: 0.9575 - val_loss: 4.1193 - val_acc: 0.5954 - val_f1_m: 0.5948 - val_precision_m: 0.5974 - val_recall_m: 0.5922\n",
            "Epoch 227/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0805 - acc: 0.9598 - f1_m: 0.9599 - precision_m: 0.9623 - recall_m: 0.9575 - val_loss: 4.0162 - val_acc: 0.5926 - val_f1_m: 0.5917 - val_precision_m: 0.5953 - val_recall_m: 0.5883\n",
            "Epoch 228/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0795 - acc: 0.9593 - f1_m: 0.9596 - precision_m: 0.9621 - recall_m: 0.9571 - val_loss: 4.0549 - val_acc: 0.5983 - val_f1_m: 0.5977 - val_precision_m: 0.6009 - val_recall_m: 0.5946\n",
            "Epoch 229/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0827 - acc: 0.9592 - f1_m: 0.9594 - precision_m: 0.9618 - recall_m: 0.9570 - val_loss: 4.0271 - val_acc: 0.5962 - val_f1_m: 0.5953 - val_precision_m: 0.5982 - val_recall_m: 0.5925\n",
            "Epoch 230/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0805 - acc: 0.9596 - f1_m: 0.9597 - precision_m: 0.9624 - recall_m: 0.9571 - val_loss: 3.9553 - val_acc: 0.5949 - val_f1_m: 0.5940 - val_precision_m: 0.5972 - val_recall_m: 0.5909\n",
            "Epoch 231/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0781 - acc: 0.9603 - f1_m: 0.9604 - precision_m: 0.9628 - recall_m: 0.9580 - val_loss: 4.0068 - val_acc: 0.5979 - val_f1_m: 0.5975 - val_precision_m: 0.6003 - val_recall_m: 0.5948\n",
            "Epoch 232/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0767 - acc: 0.9599 - f1_m: 0.9601 - precision_m: 0.9626 - recall_m: 0.9576 - val_loss: 4.0977 - val_acc: 0.5922 - val_f1_m: 0.5918 - val_precision_m: 0.5946 - val_recall_m: 0.5890\n",
            "Epoch 233/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0818 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9620 - recall_m: 0.9572 - val_loss: 4.0398 - val_acc: 0.5980 - val_f1_m: 0.5978 - val_precision_m: 0.6008 - val_recall_m: 0.5949\n",
            "Epoch 234/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0782 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9619 - recall_m: 0.9573 - val_loss: 4.0207 - val_acc: 0.5987 - val_f1_m: 0.5980 - val_precision_m: 0.6009 - val_recall_m: 0.5952\n",
            "Epoch 235/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0794 - acc: 0.9598 - f1_m: 0.9600 - precision_m: 0.9625 - recall_m: 0.9575 - val_loss: 4.0612 - val_acc: 0.5952 - val_f1_m: 0.5952 - val_precision_m: 0.5980 - val_recall_m: 0.5924\n",
            "Epoch 236/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0795 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9620 - recall_m: 0.9573 - val_loss: 4.0651 - val_acc: 0.5948 - val_f1_m: 0.5942 - val_precision_m: 0.5975 - val_recall_m: 0.5910\n",
            "Epoch 237/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0784 - acc: 0.9604 - f1_m: 0.9607 - precision_m: 0.9632 - recall_m: 0.9582 - val_loss: 4.0427 - val_acc: 0.5926 - val_f1_m: 0.5919 - val_precision_m: 0.5952 - val_recall_m: 0.5886\n",
            "Epoch 238/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0810 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9622 - recall_m: 0.9571 - val_loss: 4.0287 - val_acc: 0.5897 - val_f1_m: 0.5897 - val_precision_m: 0.5932 - val_recall_m: 0.5863\n",
            "Epoch 239/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0811 - acc: 0.9603 - f1_m: 0.9604 - precision_m: 0.9628 - recall_m: 0.9581 - val_loss: 3.8886 - val_acc: 0.5922 - val_f1_m: 0.5911 - val_precision_m: 0.5948 - val_recall_m: 0.5874\n",
            "Epoch 240/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0814 - acc: 0.9596 - f1_m: 0.9599 - precision_m: 0.9625 - recall_m: 0.9573 - val_loss: 3.9065 - val_acc: 0.5916 - val_f1_m: 0.5910 - val_precision_m: 0.5949 - val_recall_m: 0.5872\n",
            "Epoch 241/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0783 - acc: 0.9598 - f1_m: 0.9601 - precision_m: 0.9626 - recall_m: 0.9577 - val_loss: 4.0062 - val_acc: 0.5949 - val_f1_m: 0.5951 - val_precision_m: 0.5985 - val_recall_m: 0.5918\n",
            "Epoch 242/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0794 - acc: 0.9601 - f1_m: 0.9601 - precision_m: 0.9625 - recall_m: 0.9578 - val_loss: 4.0345 - val_acc: 0.5943 - val_f1_m: 0.5938 - val_precision_m: 0.5966 - val_recall_m: 0.5911\n",
            "Epoch 243/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0775 - acc: 0.9604 - f1_m: 0.9605 - precision_m: 0.9630 - recall_m: 0.9579 - val_loss: 4.0336 - val_acc: 0.5976 - val_f1_m: 0.5969 - val_precision_m: 0.6001 - val_recall_m: 0.5938\n",
            "Epoch 244/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0783 - acc: 0.9598 - f1_m: 0.9601 - precision_m: 0.9624 - recall_m: 0.9577 - val_loss: 4.0180 - val_acc: 0.5965 - val_f1_m: 0.5958 - val_precision_m: 0.5987 - val_recall_m: 0.5928\n",
            "Epoch 245/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0802 - acc: 0.9591 - f1_m: 0.9593 - precision_m: 0.9619 - recall_m: 0.9568 - val_loss: 3.9471 - val_acc: 0.5928 - val_f1_m: 0.5916 - val_precision_m: 0.5958 - val_recall_m: 0.5876\n",
            "Epoch 246/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.0777 - acc: 0.9602 - f1_m: 0.9606 - precision_m: 0.9631 - recall_m: 0.9580 - val_loss: 4.0421 - val_acc: 0.5960 - val_f1_m: 0.5958 - val_precision_m: 0.5991 - val_recall_m: 0.5927\n",
            "Epoch 247/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0816 - acc: 0.9596 - f1_m: 0.9598 - precision_m: 0.9623 - recall_m: 0.9573 - val_loss: 3.9481 - val_acc: 0.5920 - val_f1_m: 0.5911 - val_precision_m: 0.5949 - val_recall_m: 0.5873\n",
            "Epoch 248/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0798 - acc: 0.9594 - f1_m: 0.9596 - precision_m: 0.9622 - recall_m: 0.9570 - val_loss: 4.0340 - val_acc: 0.5978 - val_f1_m: 0.5974 - val_precision_m: 0.6006 - val_recall_m: 0.5942\n",
            "Epoch 249/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0795 - acc: 0.9591 - f1_m: 0.9593 - precision_m: 0.9618 - recall_m: 0.9569 - val_loss: 3.9561 - val_acc: 0.5917 - val_f1_m: 0.5908 - val_precision_m: 0.5950 - val_recall_m: 0.5867\n",
            "Epoch 250/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0787 - acc: 0.9599 - f1_m: 0.9603 - precision_m: 0.9626 - recall_m: 0.9581 - val_loss: 4.0868 - val_acc: 0.5964 - val_f1_m: 0.5963 - val_precision_m: 0.5989 - val_recall_m: 0.5938\n",
            "Epoch 251/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0792 - acc: 0.9601 - f1_m: 0.9603 - precision_m: 0.9626 - recall_m: 0.9581 - val_loss: 4.0029 - val_acc: 0.5968 - val_f1_m: 0.5965 - val_precision_m: 0.6002 - val_recall_m: 0.5930\n",
            "Epoch 252/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0782 - acc: 0.9606 - f1_m: 0.9608 - precision_m: 0.9636 - recall_m: 0.9581 - val_loss: 3.9879 - val_acc: 0.5925 - val_f1_m: 0.5922 - val_precision_m: 0.5959 - val_recall_m: 0.5886\n",
            "Epoch 253/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0781 - acc: 0.9595 - f1_m: 0.9595 - precision_m: 0.9620 - recall_m: 0.9570 - val_loss: 3.9852 - val_acc: 0.5927 - val_f1_m: 0.5921 - val_precision_m: 0.5957 - val_recall_m: 0.5887\n",
            "Epoch 254/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0782 - acc: 0.9597 - f1_m: 0.9598 - precision_m: 0.9624 - recall_m: 0.9573 - val_loss: 3.9710 - val_acc: 0.5950 - val_f1_m: 0.5946 - val_precision_m: 0.5981 - val_recall_m: 0.5911\n",
            "Epoch 255/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0778 - acc: 0.9601 - f1_m: 0.9604 - precision_m: 0.9629 - recall_m: 0.9580 - val_loss: 3.9748 - val_acc: 0.5984 - val_f1_m: 0.5980 - val_precision_m: 0.6012 - val_recall_m: 0.5950\n",
            "Epoch 256/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0780 - acc: 0.9608 - f1_m: 0.9609 - precision_m: 0.9633 - recall_m: 0.9585 - val_loss: 3.9726 - val_acc: 0.5973 - val_f1_m: 0.5965 - val_precision_m: 0.6002 - val_recall_m: 0.5928\n",
            "Epoch 257/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0768 - acc: 0.9604 - f1_m: 0.9607 - precision_m: 0.9635 - recall_m: 0.9580 - val_loss: 4.0676 - val_acc: 0.6000 - val_f1_m: 0.5992 - val_precision_m: 0.6024 - val_recall_m: 0.5962\n",
            "Epoch 258/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0779 - acc: 0.9602 - f1_m: 0.9605 - precision_m: 0.9631 - recall_m: 0.9578 - val_loss: 4.0334 - val_acc: 0.5920 - val_f1_m: 0.5915 - val_precision_m: 0.5950 - val_recall_m: 0.5881\n",
            "Epoch 259/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0825 - acc: 0.9594 - f1_m: 0.9597 - precision_m: 0.9623 - recall_m: 0.9572 - val_loss: 4.0018 - val_acc: 0.5938 - val_f1_m: 0.5930 - val_precision_m: 0.5968 - val_recall_m: 0.5894\n",
            "Epoch 260/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0782 - acc: 0.9606 - f1_m: 0.9608 - precision_m: 0.9634 - recall_m: 0.9582 - val_loss: 3.9698 - val_acc: 0.5976 - val_f1_m: 0.5966 - val_precision_m: 0.6004 - val_recall_m: 0.5929\n",
            "Epoch 261/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0778 - acc: 0.9605 - f1_m: 0.9607 - precision_m: 0.9632 - recall_m: 0.9582 - val_loss: 4.1174 - val_acc: 0.5960 - val_f1_m: 0.5953 - val_precision_m: 0.5982 - val_recall_m: 0.5925\n",
            "Epoch 262/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0775 - acc: 0.9604 - f1_m: 0.9604 - precision_m: 0.9629 - recall_m: 0.9580 - val_loss: 4.0883 - val_acc: 0.5990 - val_f1_m: 0.5985 - val_precision_m: 0.6015 - val_recall_m: 0.5955\n",
            "Epoch 263/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0771 - acc: 0.9600 - f1_m: 0.9601 - precision_m: 0.9628 - recall_m: 0.9575 - val_loss: 4.0493 - val_acc: 0.5922 - val_f1_m: 0.5921 - val_precision_m: 0.5954 - val_recall_m: 0.5889\n",
            "Epoch 264/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0767 - acc: 0.9606 - f1_m: 0.9608 - precision_m: 0.9635 - recall_m: 0.9581 - val_loss: 4.1093 - val_acc: 0.5986 - val_f1_m: 0.5977 - val_precision_m: 0.6007 - val_recall_m: 0.5948\n",
            "Epoch 265/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0775 - acc: 0.9610 - f1_m: 0.9610 - precision_m: 0.9634 - recall_m: 0.9586 - val_loss: 3.9407 - val_acc: 0.5971 - val_f1_m: 0.5965 - val_precision_m: 0.6003 - val_recall_m: 0.5928\n",
            "Epoch 266/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0794 - acc: 0.9588 - f1_m: 0.9591 - precision_m: 0.9617 - recall_m: 0.9566 - val_loss: 4.0073 - val_acc: 0.5951 - val_f1_m: 0.5943 - val_precision_m: 0.5976 - val_recall_m: 0.5910\n",
            "Epoch 267/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0777 - acc: 0.9605 - f1_m: 0.9607 - precision_m: 0.9632 - recall_m: 0.9582 - val_loss: 4.0189 - val_acc: 0.5947 - val_f1_m: 0.5942 - val_precision_m: 0.5976 - val_recall_m: 0.5909\n",
            "Epoch 268/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0787 - acc: 0.9592 - f1_m: 0.9595 - precision_m: 0.9622 - recall_m: 0.9568 - val_loss: 4.0421 - val_acc: 0.5980 - val_f1_m: 0.5970 - val_precision_m: 0.5999 - val_recall_m: 0.5942\n",
            "Epoch 269/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0784 - acc: 0.9605 - f1_m: 0.9607 - precision_m: 0.9632 - recall_m: 0.9582 - val_loss: 3.9486 - val_acc: 0.5947 - val_f1_m: 0.5937 - val_precision_m: 0.5975 - val_recall_m: 0.5899\n",
            "Epoch 270/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0767 - acc: 0.9610 - f1_m: 0.9612 - precision_m: 0.9638 - recall_m: 0.9586 - val_loss: 4.1467 - val_acc: 0.5974 - val_f1_m: 0.5966 - val_precision_m: 0.5994 - val_recall_m: 0.5938\n",
            "Epoch 271/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0766 - acc: 0.9612 - f1_m: 0.9615 - precision_m: 0.9638 - recall_m: 0.9591 - val_loss: 4.0481 - val_acc: 0.5896 - val_f1_m: 0.5888 - val_precision_m: 0.5920 - val_recall_m: 0.5857\n",
            "Epoch 272/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0757 - acc: 0.9609 - f1_m: 0.9612 - precision_m: 0.9637 - recall_m: 0.9587 - val_loss: 3.9865 - val_acc: 0.5914 - val_f1_m: 0.5909 - val_precision_m: 0.5938 - val_recall_m: 0.5880\n",
            "Epoch 273/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2770 - acc: 0.9496 - f1_m: 0.9498 - precision_m: 0.9518 - recall_m: 0.9478 - val_loss: 4.1187 - val_acc: 0.5914 - val_f1_m: 0.5910 - val_precision_m: 0.5942 - val_recall_m: 0.5878\n",
            "Epoch 274/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2991 - acc: 0.9462 - f1_m: 0.9466 - precision_m: 0.9492 - recall_m: 0.9440 - val_loss: 4.1523 - val_acc: 0.5938 - val_f1_m: 0.5931 - val_precision_m: 0.5963 - val_recall_m: 0.5899\n",
            "Epoch 275/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2996 - acc: 0.9448 - f1_m: 0.9451 - precision_m: 0.9480 - recall_m: 0.9421 - val_loss: 4.0548 - val_acc: 0.5909 - val_f1_m: 0.5899 - val_precision_m: 0.5944 - val_recall_m: 0.5856\n",
            "Epoch 276/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2953 - acc: 0.9465 - f1_m: 0.9468 - precision_m: 0.9499 - recall_m: 0.9438 - val_loss: 4.2201 - val_acc: 0.5918 - val_f1_m: 0.5911 - val_precision_m: 0.5947 - val_recall_m: 0.5875\n",
            "Epoch 277/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3099 - acc: 0.9447 - f1_m: 0.9449 - precision_m: 0.9478 - recall_m: 0.9421 - val_loss: 4.0829 - val_acc: 0.5904 - val_f1_m: 0.5893 - val_precision_m: 0.5934 - val_recall_m: 0.5852\n",
            "Epoch 278/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.2892 - acc: 0.9468 - f1_m: 0.9468 - precision_m: 0.9505 - recall_m: 0.9431 - val_loss: 4.1402 - val_acc: 0.5919 - val_f1_m: 0.5911 - val_precision_m: 0.5951 - val_recall_m: 0.5871\n",
            "Epoch 279/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2731 - acc: 0.9480 - f1_m: 0.9486 - precision_m: 0.9528 - recall_m: 0.9445 - val_loss: 4.1476 - val_acc: 0.5900 - val_f1_m: 0.5891 - val_precision_m: 0.5948 - val_recall_m: 0.5835\n",
            "Epoch 280/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2739 - acc: 0.9461 - f1_m: 0.9467 - precision_m: 0.9510 - recall_m: 0.9424 - val_loss: 4.1246 - val_acc: 0.5932 - val_f1_m: 0.5923 - val_precision_m: 0.5972 - val_recall_m: 0.5876\n",
            "Epoch 281/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2664 - acc: 0.9467 - f1_m: 0.9472 - precision_m: 0.9518 - recall_m: 0.9427 - val_loss: 4.0290 - val_acc: 0.5903 - val_f1_m: 0.5893 - val_precision_m: 0.5954 - val_recall_m: 0.5834\n",
            "Epoch 282/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2585 - acc: 0.9472 - f1_m: 0.9482 - precision_m: 0.9528 - recall_m: 0.9436 - val_loss: 4.1821 - val_acc: 0.5908 - val_f1_m: 0.5893 - val_precision_m: 0.5946 - val_recall_m: 0.5842\n",
            "Epoch 283/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2698 - acc: 0.9453 - f1_m: 0.9456 - precision_m: 0.9511 - recall_m: 0.9403 - val_loss: 4.0864 - val_acc: 0.5919 - val_f1_m: 0.5909 - val_precision_m: 0.5959 - val_recall_m: 0.5861\n",
            "Epoch 284/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2687 - acc: 0.9461 - f1_m: 0.9469 - precision_m: 0.9518 - recall_m: 0.9422 - val_loss: 4.1018 - val_acc: 0.5913 - val_f1_m: 0.5903 - val_precision_m: 0.5960 - val_recall_m: 0.5847\n",
            "Epoch 285/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2513 - acc: 0.9472 - f1_m: 0.9482 - precision_m: 0.9542 - recall_m: 0.9424 - val_loss: 4.0854 - val_acc: 0.5947 - val_f1_m: 0.5934 - val_precision_m: 0.5989 - val_recall_m: 0.5881\n",
            "Epoch 286/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2292 - acc: 0.9468 - f1_m: 0.9479 - precision_m: 0.9549 - recall_m: 0.9410 - val_loss: 4.0576 - val_acc: 0.5918 - val_f1_m: 0.5903 - val_precision_m: 0.5970 - val_recall_m: 0.5837\n",
            "Epoch 287/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2333 - acc: 0.9460 - f1_m: 0.9472 - precision_m: 0.9532 - recall_m: 0.9414 - val_loss: 4.1277 - val_acc: 0.5941 - val_f1_m: 0.5930 - val_precision_m: 0.5983 - val_recall_m: 0.5877\n",
            "Epoch 288/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2149 - acc: 0.9477 - f1_m: 0.9489 - precision_m: 0.9555 - recall_m: 0.9424 - val_loss: 4.0616 - val_acc: 0.5973 - val_f1_m: 0.5957 - val_precision_m: 0.6016 - val_recall_m: 0.5899\n",
            "Epoch 289/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2223 - acc: 0.9480 - f1_m: 0.9494 - precision_m: 0.9551 - recall_m: 0.9437 - val_loss: 4.1202 - val_acc: 0.5983 - val_f1_m: 0.5970 - val_precision_m: 0.6032 - val_recall_m: 0.5911\n",
            "Epoch 290/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2043 - acc: 0.9476 - f1_m: 0.9492 - precision_m: 0.9557 - recall_m: 0.9428 - val_loss: 3.9950 - val_acc: 0.5934 - val_f1_m: 0.5924 - val_precision_m: 0.5991 - val_recall_m: 0.5860\n",
            "Epoch 291/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1968 - acc: 0.9482 - f1_m: 0.9498 - precision_m: 0.9567 - recall_m: 0.9431 - val_loss: 4.1660 - val_acc: 0.5894 - val_f1_m: 0.5884 - val_precision_m: 0.5944 - val_recall_m: 0.5825\n",
            "Epoch 292/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2072 - acc: 0.9472 - f1_m: 0.9488 - precision_m: 0.9561 - recall_m: 0.9417 - val_loss: 4.0069 - val_acc: 0.5925 - val_f1_m: 0.5908 - val_precision_m: 0.5978 - val_recall_m: 0.5840\n",
            "Epoch 293/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2020 - acc: 0.9469 - f1_m: 0.9486 - precision_m: 0.9558 - recall_m: 0.9417 - val_loss: 4.1223 - val_acc: 0.5946 - val_f1_m: 0.5933 - val_precision_m: 0.5991 - val_recall_m: 0.5877\n",
            "Epoch 294/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1936 - acc: 0.9478 - f1_m: 0.9494 - precision_m: 0.9566 - recall_m: 0.9423 - val_loss: 4.0858 - val_acc: 0.5953 - val_f1_m: 0.5946 - val_precision_m: 0.6009 - val_recall_m: 0.5884\n",
            "Epoch 295/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1914 - acc: 0.9482 - f1_m: 0.9500 - precision_m: 0.9570 - recall_m: 0.9431 - val_loss: 4.0938 - val_acc: 0.5945 - val_f1_m: 0.5940 - val_precision_m: 0.6002 - val_recall_m: 0.5880\n",
            "Epoch 296/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1891 - acc: 0.9480 - f1_m: 0.9498 - precision_m: 0.9569 - recall_m: 0.9429 - val_loss: 4.0908 - val_acc: 0.5980 - val_f1_m: 0.5968 - val_precision_m: 0.6029 - val_recall_m: 0.5909\n",
            "Epoch 297/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1861 - acc: 0.9497 - f1_m: 0.9515 - precision_m: 0.9585 - recall_m: 0.9448 - val_loss: 4.1152 - val_acc: 0.5977 - val_f1_m: 0.5969 - val_precision_m: 0.6027 - val_recall_m: 0.5913\n",
            "Epoch 298/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1979 - acc: 0.9481 - f1_m: 0.9498 - precision_m: 0.9564 - recall_m: 0.9433 - val_loss: 4.0233 - val_acc: 0.5969 - val_f1_m: 0.5953 - val_precision_m: 0.6014 - val_recall_m: 0.5893\n",
            "Epoch 299/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1938 - acc: 0.9490 - f1_m: 0.9508 - precision_m: 0.9577 - recall_m: 0.9440 - val_loss: 4.0685 - val_acc: 0.5937 - val_f1_m: 0.5932 - val_precision_m: 0.5992 - val_recall_m: 0.5873\n",
            "Epoch 300/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1926 - acc: 0.9487 - f1_m: 0.9505 - precision_m: 0.9576 - recall_m: 0.9436 - val_loss: 4.0544 - val_acc: 0.5936 - val_f1_m: 0.5920 - val_precision_m: 0.5981 - val_recall_m: 0.5861\n",
            "Epoch 301/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1896 - acc: 0.9487 - f1_m: 0.9504 - precision_m: 0.9572 - recall_m: 0.9437 - val_loss: 4.1175 - val_acc: 0.5935 - val_f1_m: 0.5927 - val_precision_m: 0.5991 - val_recall_m: 0.5864\n",
            "Epoch 302/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1884 - acc: 0.9500 - f1_m: 0.9516 - precision_m: 0.9582 - recall_m: 0.9452 - val_loss: 4.1367 - val_acc: 0.5899 - val_f1_m: 0.5888 - val_precision_m: 0.5947 - val_recall_m: 0.5830\n",
            "Epoch 303/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1877 - acc: 0.9495 - f1_m: 0.9513 - precision_m: 0.9577 - recall_m: 0.9450 - val_loss: 4.1278 - val_acc: 0.5857 - val_f1_m: 0.5842 - val_precision_m: 0.5910 - val_recall_m: 0.5777\n",
            "Epoch 304/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1861 - acc: 0.9480 - f1_m: 0.9499 - precision_m: 0.9569 - recall_m: 0.9430 - val_loss: 4.0606 - val_acc: 0.5903 - val_f1_m: 0.5895 - val_precision_m: 0.5957 - val_recall_m: 0.5835\n",
            "Epoch 305/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1821 - acc: 0.9498 - f1_m: 0.9515 - precision_m: 0.9586 - recall_m: 0.9445 - val_loss: 4.1414 - val_acc: 0.5899 - val_f1_m: 0.5888 - val_precision_m: 0.5944 - val_recall_m: 0.5834\n",
            "Epoch 306/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1838 - acc: 0.9488 - f1_m: 0.9509 - precision_m: 0.9579 - recall_m: 0.9440 - val_loss: 4.1895 - val_acc: 0.5914 - val_f1_m: 0.5909 - val_precision_m: 0.5972 - val_recall_m: 0.5848\n",
            "Epoch 307/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1782 - acc: 0.9488 - f1_m: 0.9508 - precision_m: 0.9581 - recall_m: 0.9437 - val_loss: 4.1342 - val_acc: 0.5929 - val_f1_m: 0.5919 - val_precision_m: 0.5990 - val_recall_m: 0.5851\n",
            "Epoch 308/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1780 - acc: 0.9493 - f1_m: 0.9512 - precision_m: 0.9586 - recall_m: 0.9440 - val_loss: 4.0375 - val_acc: 0.5938 - val_f1_m: 0.5928 - val_precision_m: 0.5994 - val_recall_m: 0.5865\n",
            "Epoch 309/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1772 - acc: 0.9491 - f1_m: 0.9512 - precision_m: 0.9585 - recall_m: 0.9440 - val_loss: 4.1236 - val_acc: 0.5936 - val_f1_m: 0.5927 - val_precision_m: 0.5991 - val_recall_m: 0.5865\n",
            "Epoch 310/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1836 - acc: 0.9493 - f1_m: 0.9511 - precision_m: 0.9578 - recall_m: 0.9444 - val_loss: 4.1761 - val_acc: 0.5838 - val_f1_m: 0.5830 - val_precision_m: 0.5894 - val_recall_m: 0.5769\n",
            "Epoch 311/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1880 - acc: 0.9496 - f1_m: 0.9513 - precision_m: 0.9579 - recall_m: 0.9448 - val_loss: 3.9902 - val_acc: 0.5895 - val_f1_m: 0.5883 - val_precision_m: 0.5952 - val_recall_m: 0.5816\n",
            "Epoch 312/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1892 - acc: 0.9487 - f1_m: 0.9505 - precision_m: 0.9574 - recall_m: 0.9436 - val_loss: 4.1237 - val_acc: 0.5883 - val_f1_m: 0.5873 - val_precision_m: 0.5935 - val_recall_m: 0.5813\n",
            "Epoch 313/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1917 - acc: 0.9489 - f1_m: 0.9507 - precision_m: 0.9572 - recall_m: 0.9442 - val_loss: 4.1235 - val_acc: 0.5866 - val_f1_m: 0.5852 - val_precision_m: 0.5913 - val_recall_m: 0.5793\n",
            "Epoch 314/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1884 - acc: 0.9490 - f1_m: 0.9508 - precision_m: 0.9573 - recall_m: 0.9445 - val_loss: 4.0195 - val_acc: 0.5843 - val_f1_m: 0.5833 - val_precision_m: 0.5903 - val_recall_m: 0.5765\n",
            "Epoch 315/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1797 - acc: 0.9490 - f1_m: 0.9508 - precision_m: 0.9578 - recall_m: 0.9439 - val_loss: 4.1118 - val_acc: 0.5903 - val_f1_m: 0.5891 - val_precision_m: 0.5960 - val_recall_m: 0.5823\n",
            "Epoch 316/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1771 - acc: 0.9494 - f1_m: 0.9512 - precision_m: 0.9581 - recall_m: 0.9444 - val_loss: 4.1695 - val_acc: 0.5906 - val_f1_m: 0.5896 - val_precision_m: 0.5962 - val_recall_m: 0.5832\n",
            "Epoch 317/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1788 - acc: 0.9492 - f1_m: 0.9511 - precision_m: 0.9582 - recall_m: 0.9441 - val_loss: 4.1064 - val_acc: 0.5943 - val_f1_m: 0.5937 - val_precision_m: 0.6002 - val_recall_m: 0.5874\n",
            "Epoch 318/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1781 - acc: 0.9508 - f1_m: 0.9527 - precision_m: 0.9598 - recall_m: 0.9458 - val_loss: 4.0852 - val_acc: 0.5954 - val_f1_m: 0.5941 - val_precision_m: 0.6002 - val_recall_m: 0.5882\n",
            "Epoch 319/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1810 - acc: 0.9498 - f1_m: 0.9517 - precision_m: 0.9584 - recall_m: 0.9452 - val_loss: 4.1534 - val_acc: 0.5886 - val_f1_m: 0.5878 - val_precision_m: 0.5938 - val_recall_m: 0.5820\n",
            "Epoch 320/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1824 - acc: 0.9497 - f1_m: 0.9514 - precision_m: 0.9584 - recall_m: 0.9447 - val_loss: 4.1155 - val_acc: 0.5910 - val_f1_m: 0.5904 - val_precision_m: 0.5962 - val_recall_m: 0.5847\n",
            "Epoch 321/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1863 - acc: 0.9491 - f1_m: 0.9509 - precision_m: 0.9578 - recall_m: 0.9441 - val_loss: 4.0804 - val_acc: 0.5873 - val_f1_m: 0.5861 - val_precision_m: 0.5925 - val_recall_m: 0.5799\n",
            "Epoch 322/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1856 - acc: 0.9491 - f1_m: 0.9508 - precision_m: 0.9574 - recall_m: 0.9444 - val_loss: 4.2101 - val_acc: 0.5889 - val_f1_m: 0.5885 - val_precision_m: 0.5941 - val_recall_m: 0.5831\n",
            "Epoch 323/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1848 - acc: 0.9486 - f1_m: 0.9505 - precision_m: 0.9575 - recall_m: 0.9438 - val_loss: 4.1652 - val_acc: 0.5890 - val_f1_m: 0.5879 - val_precision_m: 0.5939 - val_recall_m: 0.5819\n",
            "Epoch 324/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1895 - acc: 0.9487 - f1_m: 0.9505 - precision_m: 0.9573 - recall_m: 0.9439 - val_loss: 4.1604 - val_acc: 0.5912 - val_f1_m: 0.5906 - val_precision_m: 0.5966 - val_recall_m: 0.5848\n",
            "Epoch 325/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1889 - acc: 0.9489 - f1_m: 0.9508 - precision_m: 0.9576 - recall_m: 0.9441 - val_loss: 4.1368 - val_acc: 0.5879 - val_f1_m: 0.5870 - val_precision_m: 0.5933 - val_recall_m: 0.5808\n",
            "Epoch 326/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1949 - acc: 0.9495 - f1_m: 0.9513 - precision_m: 0.9576 - recall_m: 0.9452 - val_loss: 4.0732 - val_acc: 0.5903 - val_f1_m: 0.5895 - val_precision_m: 0.5961 - val_recall_m: 0.5832\n",
            "Epoch 327/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1887 - acc: 0.9500 - f1_m: 0.9516 - precision_m: 0.9581 - recall_m: 0.9452 - val_loss: 4.1484 - val_acc: 0.5874 - val_f1_m: 0.5865 - val_precision_m: 0.5925 - val_recall_m: 0.5806\n",
            "Epoch 328/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1865 - acc: 0.9504 - f1_m: 0.9522 - precision_m: 0.9586 - recall_m: 0.9459 - val_loss: 4.1529 - val_acc: 0.5915 - val_f1_m: 0.5907 - val_precision_m: 0.5968 - val_recall_m: 0.5849\n",
            "Epoch 329/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1799 - acc: 0.9508 - f1_m: 0.9527 - precision_m: 0.9595 - recall_m: 0.9460 - val_loss: 4.1603 - val_acc: 0.5865 - val_f1_m: 0.5855 - val_precision_m: 0.5919 - val_recall_m: 0.5793\n",
            "Epoch 330/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1830 - acc: 0.9500 - f1_m: 0.9518 - precision_m: 0.9584 - recall_m: 0.9453 - val_loss: 4.1455 - val_acc: 0.5878 - val_f1_m: 0.5871 - val_precision_m: 0.5930 - val_recall_m: 0.5814\n",
            "Epoch 331/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1784 - acc: 0.9505 - f1_m: 0.9524 - precision_m: 0.9590 - recall_m: 0.9459 - val_loss: 4.1503 - val_acc: 0.5899 - val_f1_m: 0.5887 - val_precision_m: 0.5946 - val_recall_m: 0.5829\n",
            "Epoch 332/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1797 - acc: 0.9505 - f1_m: 0.9522 - precision_m: 0.9589 - recall_m: 0.9457 - val_loss: 4.1571 - val_acc: 0.5918 - val_f1_m: 0.5911 - val_precision_m: 0.5965 - val_recall_m: 0.5859\n",
            "Epoch 333/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1880 - acc: 0.9496 - f1_m: 0.9513 - precision_m: 0.9579 - recall_m: 0.9448 - val_loss: 4.1317 - val_acc: 0.5878 - val_f1_m: 0.5861 - val_precision_m: 0.5930 - val_recall_m: 0.5794\n",
            "Epoch 334/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1785 - acc: 0.9499 - f1_m: 0.9520 - precision_m: 0.9593 - recall_m: 0.9449 - val_loss: 4.0414 - val_acc: 0.5911 - val_f1_m: 0.5896 - val_precision_m: 0.5973 - val_recall_m: 0.5823\n",
            "Epoch 335/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1661 - acc: 0.9491 - f1_m: 0.9513 - precision_m: 0.9594 - recall_m: 0.9434 - val_loss: 4.0748 - val_acc: 0.5947 - val_f1_m: 0.5931 - val_precision_m: 0.6013 - val_recall_m: 0.5852\n",
            "Epoch 336/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1563 - acc: 0.9488 - f1_m: 0.9511 - precision_m: 0.9599 - recall_m: 0.9426 - val_loss: 4.0206 - val_acc: 0.5905 - val_f1_m: 0.5898 - val_precision_m: 0.5980 - val_recall_m: 0.5818\n",
            "Epoch 337/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1538 - acc: 0.9490 - f1_m: 0.9518 - precision_m: 0.9606 - recall_m: 0.9433 - val_loss: 4.0982 - val_acc: 0.5956 - val_f1_m: 0.5949 - val_precision_m: 0.6035 - val_recall_m: 0.5866\n",
            "Epoch 338/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1879 - acc: 0.9481 - f1_m: 0.9500 - precision_m: 0.9576 - recall_m: 0.9426 - val_loss: 4.1577 - val_acc: 0.5946 - val_f1_m: 0.5930 - val_precision_m: 0.6003 - val_recall_m: 0.5859\n",
            "Epoch 339/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1856 - acc: 0.9483 - f1_m: 0.9499 - precision_m: 0.9572 - recall_m: 0.9428 - val_loss: 4.1039 - val_acc: 0.5887 - val_f1_m: 0.5877 - val_precision_m: 0.5950 - val_recall_m: 0.5807\n",
            "Epoch 340/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1747 - acc: 0.9489 - f1_m: 0.9507 - precision_m: 0.9585 - recall_m: 0.9431 - val_loss: 4.0951 - val_acc: 0.5927 - val_f1_m: 0.5928 - val_precision_m: 0.5996 - val_recall_m: 0.5861\n",
            "Epoch 341/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1616 - acc: 0.9500 - f1_m: 0.9523 - precision_m: 0.9604 - recall_m: 0.9444 - val_loss: 4.1630 - val_acc: 0.5866 - val_f1_m: 0.5865 - val_precision_m: 0.5954 - val_recall_m: 0.5780\n",
            "Epoch 342/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1406 - acc: 0.9488 - f1_m: 0.9517 - precision_m: 0.9615 - recall_m: 0.9422 - val_loss: 4.1275 - val_acc: 0.5905 - val_f1_m: 0.5895 - val_precision_m: 0.5985 - val_recall_m: 0.5809\n",
            "Epoch 343/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1376 - acc: 0.9483 - f1_m: 0.9516 - precision_m: 0.9612 - recall_m: 0.9423 - val_loss: 4.0215 - val_acc: 0.5863 - val_f1_m: 0.5847 - val_precision_m: 0.5942 - val_recall_m: 0.5756\n",
            "Epoch 344/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1323 - acc: 0.9482 - f1_m: 0.9516 - precision_m: 0.9617 - recall_m: 0.9418 - val_loss: 4.0804 - val_acc: 0.5867 - val_f1_m: 0.5853 - val_precision_m: 0.5943 - val_recall_m: 0.5767\n",
            "Epoch 345/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1256 - acc: 0.9495 - f1_m: 0.9529 - precision_m: 0.9629 - recall_m: 0.9431 - val_loss: 4.0723 - val_acc: 0.5893 - val_f1_m: 0.5873 - val_precision_m: 0.5968 - val_recall_m: 0.5781\n",
            "Epoch 346/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1227 - acc: 0.9493 - f1_m: 0.9527 - precision_m: 0.9632 - recall_m: 0.9425 - val_loss: 4.0496 - val_acc: 0.5895 - val_f1_m: 0.5882 - val_precision_m: 0.5985 - val_recall_m: 0.5783\n",
            "Epoch 347/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1182 - acc: 0.9479 - f1_m: 0.9515 - precision_m: 0.9625 - recall_m: 0.9409 - val_loss: 4.0453 - val_acc: 0.5825 - val_f1_m: 0.5817 - val_precision_m: 0.5916 - val_recall_m: 0.5721\n",
            "Epoch 348/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1373 - acc: 0.9491 - f1_m: 0.9518 - precision_m: 0.9619 - recall_m: 0.9420 - val_loss: 4.0356 - val_acc: 0.5921 - val_f1_m: 0.5916 - val_precision_m: 0.6005 - val_recall_m: 0.5831\n",
            "Epoch 349/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1242 - acc: 0.9488 - f1_m: 0.9522 - precision_m: 0.9625 - recall_m: 0.9422 - val_loss: 4.1060 - val_acc: 0.5932 - val_f1_m: 0.5916 - val_precision_m: 0.6007 - val_recall_m: 0.5829\n",
            "Epoch 350/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1109 - acc: 0.9497 - f1_m: 0.9533 - precision_m: 0.9641 - recall_m: 0.9428 - val_loss: 4.0220 - val_acc: 0.5897 - val_f1_m: 0.5887 - val_precision_m: 0.5979 - val_recall_m: 0.5799\n",
            "Epoch 351/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1108 - acc: 0.9501 - f1_m: 0.9535 - precision_m: 0.9638 - recall_m: 0.9434 - val_loss: 4.1277 - val_acc: 0.5891 - val_f1_m: 0.5883 - val_precision_m: 0.5972 - val_recall_m: 0.5798\n",
            "Epoch 352/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1122 - acc: 0.9497 - f1_m: 0.9527 - precision_m: 0.9634 - recall_m: 0.9423 - val_loss: 4.0368 - val_acc: 0.5910 - val_f1_m: 0.5900 - val_precision_m: 0.5989 - val_recall_m: 0.5814\n",
            "Epoch 353/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1157 - acc: 0.9496 - f1_m: 0.9526 - precision_m: 0.9629 - recall_m: 0.9426 - val_loss: 4.0362 - val_acc: 0.5945 - val_f1_m: 0.5938 - val_precision_m: 0.6026 - val_recall_m: 0.5852\n",
            "Epoch 354/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1158 - acc: 0.9498 - f1_m: 0.9526 - precision_m: 0.9633 - recall_m: 0.9422 - val_loss: 4.0340 - val_acc: 0.5913 - val_f1_m: 0.5909 - val_precision_m: 0.5998 - val_recall_m: 0.5823\n",
            "Epoch 355/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1138 - acc: 0.9494 - f1_m: 0.9525 - precision_m: 0.9633 - recall_m: 0.9420 - val_loss: 4.1699 - val_acc: 0.5885 - val_f1_m: 0.5875 - val_precision_m: 0.5963 - val_recall_m: 0.5789\n",
            "Epoch 356/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1149 - acc: 0.9498 - f1_m: 0.9530 - precision_m: 0.9635 - recall_m: 0.9428 - val_loss: 4.0573 - val_acc: 0.5950 - val_f1_m: 0.5942 - val_precision_m: 0.6031 - val_recall_m: 0.5855\n",
            "Epoch 357/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1261 - acc: 0.9504 - f1_m: 0.9533 - precision_m: 0.9628 - recall_m: 0.9439 - val_loss: 4.1083 - val_acc: 0.5930 - val_f1_m: 0.5928 - val_precision_m: 0.6007 - val_recall_m: 0.5851\n",
            "Epoch 358/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1361 - acc: 0.9501 - f1_m: 0.9529 - precision_m: 0.9620 - recall_m: 0.9440 - val_loss: 3.9439 - val_acc: 0.5923 - val_f1_m: 0.5910 - val_precision_m: 0.6010 - val_recall_m: 0.5814\n",
            "Epoch 359/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1374 - acc: 0.9497 - f1_m: 0.9526 - precision_m: 0.9619 - recall_m: 0.9436 - val_loss: 3.9818 - val_acc: 0.5886 - val_f1_m: 0.5878 - val_precision_m: 0.5970 - val_recall_m: 0.5790\n",
            "Epoch 360/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1285 - acc: 0.9498 - f1_m: 0.9527 - precision_m: 0.9627 - recall_m: 0.9430 - val_loss: 4.0718 - val_acc: 0.5944 - val_f1_m: 0.5936 - val_precision_m: 0.6023 - val_recall_m: 0.5852\n",
            "Epoch 361/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1272 - acc: 0.9498 - f1_m: 0.9529 - precision_m: 0.9624 - recall_m: 0.9436 - val_loss: 4.0870 - val_acc: 0.5915 - val_f1_m: 0.5917 - val_precision_m: 0.6001 - val_recall_m: 0.5835\n",
            "Epoch 362/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1173 - acc: 0.9502 - f1_m: 0.9529 - precision_m: 0.9633 - recall_m: 0.9428 - val_loss: 4.1104 - val_acc: 0.5937 - val_f1_m: 0.5921 - val_precision_m: 0.6020 - val_recall_m: 0.5827\n",
            "Epoch 363/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1189 - acc: 0.9487 - f1_m: 0.9518 - precision_m: 0.9618 - recall_m: 0.9420 - val_loss: 4.1319 - val_acc: 0.5919 - val_f1_m: 0.5910 - val_precision_m: 0.5996 - val_recall_m: 0.5827\n",
            "Epoch 364/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1163 - acc: 0.9495 - f1_m: 0.9525 - precision_m: 0.9628 - recall_m: 0.9425 - val_loss: 4.0471 - val_acc: 0.5966 - val_f1_m: 0.5953 - val_precision_m: 0.6043 - val_recall_m: 0.5867\n",
            "Epoch 365/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1121 - acc: 0.9494 - f1_m: 0.9525 - precision_m: 0.9631 - recall_m: 0.9423 - val_loss: 4.0926 - val_acc: 0.5944 - val_f1_m: 0.5938 - val_precision_m: 0.6023 - val_recall_m: 0.5856\n",
            "Epoch 366/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1075 - acc: 0.9501 - f1_m: 0.9531 - precision_m: 0.9640 - recall_m: 0.9426 - val_loss: 4.2693 - val_acc: 0.5937 - val_f1_m: 0.5932 - val_precision_m: 0.6007 - val_recall_m: 0.5859\n",
            "Epoch 367/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1056 - acc: 0.9504 - f1_m: 0.9536 - precision_m: 0.9642 - recall_m: 0.9434 - val_loss: 4.2179 - val_acc: 0.5889 - val_f1_m: 0.5883 - val_precision_m: 0.5966 - val_recall_m: 0.5802\n",
            "Epoch 368/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1102 - acc: 0.9512 - f1_m: 0.9540 - precision_m: 0.9641 - recall_m: 0.9442 - val_loss: 4.0843 - val_acc: 0.5930 - val_f1_m: 0.5917 - val_precision_m: 0.6009 - val_recall_m: 0.5829\n",
            "Epoch 369/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1178 - acc: 0.9501 - f1_m: 0.9528 - precision_m: 0.9631 - recall_m: 0.9429 - val_loss: 4.1329 - val_acc: 0.5955 - val_f1_m: 0.5949 - val_precision_m: 0.6034 - val_recall_m: 0.5867\n",
            "Epoch 370/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1119 - acc: 0.9500 - f1_m: 0.9532 - precision_m: 0.9635 - recall_m: 0.9433 - val_loss: 4.2588 - val_acc: 0.5890 - val_f1_m: 0.5880 - val_precision_m: 0.5965 - val_recall_m: 0.5798\n",
            "Epoch 371/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1103 - acc: 0.9505 - f1_m: 0.9534 - precision_m: 0.9640 - recall_m: 0.9432 - val_loss: 4.0739 - val_acc: 0.5943 - val_f1_m: 0.5932 - val_precision_m: 0.6022 - val_recall_m: 0.5845\n",
            "Epoch 372/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1078 - acc: 0.9501 - f1_m: 0.9539 - precision_m: 0.9643 - recall_m: 0.9438 - val_loss: 4.1035 - val_acc: 0.5968 - val_f1_m: 0.5958 - val_precision_m: 0.6045 - val_recall_m: 0.5873\n",
            "Epoch 373/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1072 - acc: 0.9497 - f1_m: 0.9532 - precision_m: 0.9635 - recall_m: 0.9432 - val_loss: 4.1162 - val_acc: 0.5937 - val_f1_m: 0.5922 - val_precision_m: 0.6006 - val_recall_m: 0.5840\n",
            "Epoch 374/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1056 - acc: 0.9505 - f1_m: 0.9540 - precision_m: 0.9645 - recall_m: 0.9438 - val_loss: 4.1765 - val_acc: 0.5871 - val_f1_m: 0.5856 - val_precision_m: 0.5945 - val_recall_m: 0.5769\n",
            "Epoch 375/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1105 - acc: 0.9502 - f1_m: 0.9531 - precision_m: 0.9636 - recall_m: 0.9428 - val_loss: 4.1183 - val_acc: 0.5936 - val_f1_m: 0.5930 - val_precision_m: 0.6016 - val_recall_m: 0.5848\n",
            "Epoch 376/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1085 - acc: 0.9503 - f1_m: 0.9533 - precision_m: 0.9640 - recall_m: 0.9429 - val_loss: 4.1010 - val_acc: 0.5923 - val_f1_m: 0.5914 - val_precision_m: 0.6007 - val_recall_m: 0.5823\n",
            "Epoch 377/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1065 - acc: 0.9509 - f1_m: 0.9538 - precision_m: 0.9648 - recall_m: 0.9431 - val_loss: 4.0895 - val_acc: 0.5933 - val_f1_m: 0.5927 - val_precision_m: 0.6025 - val_recall_m: 0.5832\n",
            "Epoch 378/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1087 - acc: 0.9505 - f1_m: 0.9538 - precision_m: 0.9647 - recall_m: 0.9433 - val_loss: 4.1201 - val_acc: 0.5858 - val_f1_m: 0.5852 - val_precision_m: 0.5940 - val_recall_m: 0.5767\n",
            "Epoch 379/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1112 - acc: 0.9496 - f1_m: 0.9527 - precision_m: 0.9637 - recall_m: 0.9420 - val_loss: 3.9958 - val_acc: 0.5917 - val_f1_m: 0.5911 - val_precision_m: 0.5997 - val_recall_m: 0.5827\n",
            "Epoch 380/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1157 - acc: 0.9493 - f1_m: 0.9523 - precision_m: 0.9630 - recall_m: 0.9419 - val_loss: 4.1443 - val_acc: 0.5886 - val_f1_m: 0.5876 - val_precision_m: 0.5958 - val_recall_m: 0.5798\n",
            "Epoch 381/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1145 - acc: 0.9509 - f1_m: 0.9539 - precision_m: 0.9640 - recall_m: 0.9441 - val_loss: 4.1154 - val_acc: 0.5882 - val_f1_m: 0.5877 - val_precision_m: 0.5966 - val_recall_m: 0.5791\n",
            "Epoch 382/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1247 - acc: 0.9501 - f1_m: 0.9530 - precision_m: 0.9625 - recall_m: 0.9436 - val_loss: 4.1600 - val_acc: 0.5916 - val_f1_m: 0.5913 - val_precision_m: 0.5993 - val_recall_m: 0.5835\n",
            "Epoch 383/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1202 - acc: 0.9493 - f1_m: 0.9522 - precision_m: 0.9625 - recall_m: 0.9422 - val_loss: 4.1716 - val_acc: 0.5901 - val_f1_m: 0.5892 - val_precision_m: 0.5982 - val_recall_m: 0.5805\n",
            "Epoch 384/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1154 - acc: 0.9498 - f1_m: 0.9528 - precision_m: 0.9629 - recall_m: 0.9430 - val_loss: 4.1826 - val_acc: 0.5852 - val_f1_m: 0.5844 - val_precision_m: 0.5929 - val_recall_m: 0.5763\n",
            "Epoch 385/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1115 - acc: 0.9502 - f1_m: 0.9531 - precision_m: 0.9636 - recall_m: 0.9430 - val_loss: 4.1077 - val_acc: 0.5912 - val_f1_m: 0.5909 - val_precision_m: 0.5997 - val_recall_m: 0.5824\n",
            "Epoch 386/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1156 - acc: 0.9494 - f1_m: 0.9524 - precision_m: 0.9625 - recall_m: 0.9427 - val_loss: 4.1859 - val_acc: 0.5817 - val_f1_m: 0.5811 - val_precision_m: 0.5903 - val_recall_m: 0.5723\n",
            "Epoch 387/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1190 - acc: 0.9505 - f1_m: 0.9533 - precision_m: 0.9633 - recall_m: 0.9436 - val_loss: 4.1514 - val_acc: 0.5890 - val_f1_m: 0.5881 - val_precision_m: 0.5975 - val_recall_m: 0.5792\n",
            "Epoch 388/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1128 - acc: 0.9505 - f1_m: 0.9536 - precision_m: 0.9640 - recall_m: 0.9436 - val_loss: 4.2337 - val_acc: 0.5903 - val_f1_m: 0.5900 - val_precision_m: 0.5979 - val_recall_m: 0.5825\n",
            "Epoch 389/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1089 - acc: 0.9504 - f1_m: 0.9533 - precision_m: 0.9636 - recall_m: 0.9434 - val_loss: 4.1168 - val_acc: 0.5920 - val_f1_m: 0.5917 - val_precision_m: 0.6005 - val_recall_m: 0.5832\n",
            "Epoch 390/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1091 - acc: 0.9504 - f1_m: 0.9533 - precision_m: 0.9638 - recall_m: 0.9431 - val_loss: 4.1604 - val_acc: 0.5823 - val_f1_m: 0.5820 - val_precision_m: 0.5909 - val_recall_m: 0.5734\n",
            "Epoch 391/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1135 - acc: 0.9497 - f1_m: 0.9531 - precision_m: 0.9634 - recall_m: 0.9431 - val_loss: 4.0886 - val_acc: 0.5912 - val_f1_m: 0.5897 - val_precision_m: 0.5990 - val_recall_m: 0.5809\n",
            "Epoch 392/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1057 - acc: 0.9504 - f1_m: 0.9538 - precision_m: 0.9644 - recall_m: 0.9435 - val_loss: 4.1624 - val_acc: 0.5923 - val_f1_m: 0.5907 - val_precision_m: 0.6002 - val_recall_m: 0.5815\n",
            "Epoch 393/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1080 - acc: 0.9496 - f1_m: 0.9533 - precision_m: 0.9641 - recall_m: 0.9428 - val_loss: 4.0622 - val_acc: 0.5907 - val_f1_m: 0.5900 - val_precision_m: 0.5999 - val_recall_m: 0.5805\n",
            "Epoch 394/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1098 - acc: 0.9499 - f1_m: 0.9537 - precision_m: 0.9643 - recall_m: 0.9434 - val_loss: 4.0739 - val_acc: 0.5879 - val_f1_m: 0.5868 - val_precision_m: 0.5966 - val_recall_m: 0.5773\n",
            "Epoch 395/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1098 - acc: 0.9501 - f1_m: 0.9530 - precision_m: 0.9638 - recall_m: 0.9425 - val_loss: 4.1327 - val_acc: 0.5928 - val_f1_m: 0.5923 - val_precision_m: 0.6010 - val_recall_m: 0.5838\n",
            "Epoch 396/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1101 - acc: 0.9503 - f1_m: 0.9535 - precision_m: 0.9641 - recall_m: 0.9431 - val_loss: 4.1383 - val_acc: 0.5942 - val_f1_m: 0.5939 - val_precision_m: 0.6027 - val_recall_m: 0.5854\n",
            "Epoch 397/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1083 - acc: 0.9509 - f1_m: 0.9538 - precision_m: 0.9645 - recall_m: 0.9434 - val_loss: 4.1744 - val_acc: 0.5904 - val_f1_m: 0.5898 - val_precision_m: 0.5987 - val_recall_m: 0.5813\n",
            "Epoch 398/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1237 - acc: 0.9493 - f1_m: 0.9527 - precision_m: 0.9622 - recall_m: 0.9435 - val_loss: 4.0144 - val_acc: 0.5919 - val_f1_m: 0.5908 - val_precision_m: 0.6008 - val_recall_m: 0.5813\n",
            "Epoch 399/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1034 - acc: 0.9510 - f1_m: 0.9546 - precision_m: 0.9650 - recall_m: 0.9445 - val_loss: 4.2265 - val_acc: 0.5867 - val_f1_m: 0.5867 - val_precision_m: 0.5949 - val_recall_m: 0.5789\n",
            "Epoch 400/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1056 - acc: 0.9510 - f1_m: 0.9544 - precision_m: 0.9649 - recall_m: 0.9442 - val_loss: 4.1411 - val_acc: 0.5912 - val_f1_m: 0.5904 - val_precision_m: 0.5992 - val_recall_m: 0.5819\n",
            "Epoch 401/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1061 - acc: 0.9507 - f1_m: 0.9536 - precision_m: 0.9642 - recall_m: 0.9432 - val_loss: 4.1903 - val_acc: 0.5929 - val_f1_m: 0.5923 - val_precision_m: 0.6007 - val_recall_m: 0.5843\n",
            "Epoch 402/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1056 - acc: 0.9508 - f1_m: 0.9538 - precision_m: 0.9641 - recall_m: 0.9437 - val_loss: 4.1537 - val_acc: 0.5844 - val_f1_m: 0.5836 - val_precision_m: 0.5925 - val_recall_m: 0.5750\n",
            "Epoch 403/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1093 - acc: 0.9504 - f1_m: 0.9536 - precision_m: 0.9641 - recall_m: 0.9434 - val_loss: 4.1295 - val_acc: 0.5957 - val_f1_m: 0.5946 - val_precision_m: 0.6035 - val_recall_m: 0.5859\n",
            "Epoch 404/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1143 - acc: 0.9501 - f1_m: 0.9529 - precision_m: 0.9630 - recall_m: 0.9430 - val_loss: 4.0935 - val_acc: 0.5883 - val_f1_m: 0.5879 - val_precision_m: 0.5970 - val_recall_m: 0.5791\n",
            "Epoch 405/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1167 - acc: 0.9504 - f1_m: 0.9531 - precision_m: 0.9628 - recall_m: 0.9436 - val_loss: 4.0994 - val_acc: 0.5868 - val_f1_m: 0.5862 - val_precision_m: 0.5950 - val_recall_m: 0.5778\n",
            "Epoch 406/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1195 - acc: 0.9499 - f1_m: 0.9525 - precision_m: 0.9625 - recall_m: 0.9429 - val_loss: 4.1619 - val_acc: 0.5868 - val_f1_m: 0.5863 - val_precision_m: 0.5954 - val_recall_m: 0.5775\n",
            "Epoch 407/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1199 - acc: 0.9510 - f1_m: 0.9538 - precision_m: 0.9638 - recall_m: 0.9441 - val_loss: 4.2714 - val_acc: 0.5875 - val_f1_m: 0.5865 - val_precision_m: 0.5943 - val_recall_m: 0.5789\n",
            "Epoch 408/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1282 - acc: 0.9506 - f1_m: 0.9534 - precision_m: 0.9631 - recall_m: 0.9439 - val_loss: 4.1885 - val_acc: 0.5934 - val_f1_m: 0.5926 - val_precision_m: 0.6008 - val_recall_m: 0.5848\n",
            "Epoch 409/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1264 - acc: 0.9502 - f1_m: 0.9528 - precision_m: 0.9629 - recall_m: 0.9431 - val_loss: 4.1179 - val_acc: 0.5931 - val_f1_m: 0.5924 - val_precision_m: 0.6012 - val_recall_m: 0.5839\n",
            "Epoch 410/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1213 - acc: 0.9501 - f1_m: 0.9529 - precision_m: 0.9631 - recall_m: 0.9430 - val_loss: 4.1764 - val_acc: 0.5928 - val_f1_m: 0.5922 - val_precision_m: 0.6001 - val_recall_m: 0.5845\n",
            "Epoch 411/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1152 - acc: 0.9514 - f1_m: 0.9540 - precision_m: 0.9642 - recall_m: 0.9441 - val_loss: 4.1105 - val_acc: 0.5930 - val_f1_m: 0.5925 - val_precision_m: 0.6011 - val_recall_m: 0.5843\n",
            "Epoch 412/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1120 - acc: 0.9502 - f1_m: 0.9532 - precision_m: 0.9634 - recall_m: 0.9433 - val_loss: 4.1463 - val_acc: 0.5952 - val_f1_m: 0.5948 - val_precision_m: 0.6033 - val_recall_m: 0.5866\n",
            "Epoch 413/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1083 - acc: 0.9516 - f1_m: 0.9546 - precision_m: 0.9647 - recall_m: 0.9448 - val_loss: 4.1779 - val_acc: 0.5871 - val_f1_m: 0.5865 - val_precision_m: 0.5959 - val_recall_m: 0.5774\n",
            "Epoch 414/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1097 - acc: 0.9505 - f1_m: 0.9536 - precision_m: 0.9639 - recall_m: 0.9435 - val_loss: 4.2416 - val_acc: 0.5869 - val_f1_m: 0.5854 - val_precision_m: 0.5944 - val_recall_m: 0.5767\n",
            "Epoch 415/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1170 - acc: 0.9498 - f1_m: 0.9531 - precision_m: 0.9632 - recall_m: 0.9433 - val_loss: 4.2071 - val_acc: 0.5947 - val_f1_m: 0.5942 - val_precision_m: 0.6023 - val_recall_m: 0.5863\n",
            "Epoch 416/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1240 - acc: 0.9503 - f1_m: 0.9533 - precision_m: 0.9631 - recall_m: 0.9437 - val_loss: 4.2043 - val_acc: 0.5865 - val_f1_m: 0.5857 - val_precision_m: 0.5952 - val_recall_m: 0.5765\n",
            "Epoch 417/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1083 - acc: 0.9506 - f1_m: 0.9537 - precision_m: 0.9645 - recall_m: 0.9433 - val_loss: 4.1733 - val_acc: 0.5900 - val_f1_m: 0.5889 - val_precision_m: 0.5983 - val_recall_m: 0.5799\n",
            "Epoch 418/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1069 - acc: 0.9508 - f1_m: 0.9539 - precision_m: 0.9646 - recall_m: 0.9436 - val_loss: 4.2323 - val_acc: 0.5891 - val_f1_m: 0.5881 - val_precision_m: 0.5969 - val_recall_m: 0.5796\n",
            "Epoch 419/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1052 - acc: 0.9514 - f1_m: 0.9545 - precision_m: 0.9652 - recall_m: 0.9441 - val_loss: 4.1866 - val_acc: 0.5913 - val_f1_m: 0.5909 - val_precision_m: 0.5994 - val_recall_m: 0.5828\n",
            "Epoch 420/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1046 - acc: 0.9506 - f1_m: 0.9538 - precision_m: 0.9644 - recall_m: 0.9435 - val_loss: 4.2055 - val_acc: 0.5926 - val_f1_m: 0.5916 - val_precision_m: 0.6001 - val_recall_m: 0.5835\n",
            "Epoch 421/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1068 - acc: 0.9506 - f1_m: 0.9537 - precision_m: 0.9643 - recall_m: 0.9434 - val_loss: 4.2375 - val_acc: 0.5925 - val_f1_m: 0.5921 - val_precision_m: 0.6006 - val_recall_m: 0.5839\n",
            "Epoch 422/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1040 - acc: 0.9507 - f1_m: 0.9539 - precision_m: 0.9644 - recall_m: 0.9436 - val_loss: 4.1059 - val_acc: 0.5954 - val_f1_m: 0.5941 - val_precision_m: 0.6041 - val_recall_m: 0.5844\n",
            "Epoch 423/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1062 - acc: 0.9507 - f1_m: 0.9538 - precision_m: 0.9645 - recall_m: 0.9434 - val_loss: 4.0963 - val_acc: 0.5882 - val_f1_m: 0.5871 - val_precision_m: 0.5967 - val_recall_m: 0.5779\n",
            "Epoch 424/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1086 - acc: 0.9507 - f1_m: 0.9539 - precision_m: 0.9642 - recall_m: 0.9438 - val_loss: 4.1175 - val_acc: 0.5961 - val_f1_m: 0.5953 - val_precision_m: 0.6044 - val_recall_m: 0.5864\n",
            "Epoch 425/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1058 - acc: 0.9507 - f1_m: 0.9539 - precision_m: 0.9644 - recall_m: 0.9437 - val_loss: 4.1271 - val_acc: 0.5947 - val_f1_m: 0.5940 - val_precision_m: 0.6025 - val_recall_m: 0.5857\n",
            "Epoch 426/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1095 - acc: 0.9510 - f1_m: 0.9542 - precision_m: 0.9644 - recall_m: 0.9443 - val_loss: 4.1384 - val_acc: 0.5936 - val_f1_m: 0.5931 - val_precision_m: 0.6014 - val_recall_m: 0.5850\n",
            "Epoch 427/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1085 - acc: 0.9498 - f1_m: 0.9530 - precision_m: 0.9635 - recall_m: 0.9427 - val_loss: 4.1202 - val_acc: 0.5916 - val_f1_m: 0.5912 - val_precision_m: 0.5995 - val_recall_m: 0.5832\n",
            "Epoch 428/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1075 - acc: 0.9513 - f1_m: 0.9545 - precision_m: 0.9652 - recall_m: 0.9442 - val_loss: 4.1791 - val_acc: 0.5889 - val_f1_m: 0.5884 - val_precision_m: 0.5971 - val_recall_m: 0.5800\n",
            "Epoch 429/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1060 - acc: 0.9507 - f1_m: 0.9540 - precision_m: 0.9648 - recall_m: 0.9435 - val_loss: 4.1553 - val_acc: 0.5913 - val_f1_m: 0.5906 - val_precision_m: 0.6000 - val_recall_m: 0.5815\n",
            "Epoch 430/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1056 - acc: 0.9504 - f1_m: 0.9534 - precision_m: 0.9640 - recall_m: 0.9430 - val_loss: 4.1966 - val_acc: 0.5944 - val_f1_m: 0.5937 - val_precision_m: 0.6028 - val_recall_m: 0.5848\n",
            "Epoch 431/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1126 - acc: 0.9505 - f1_m: 0.9534 - precision_m: 0.9637 - recall_m: 0.9433 - val_loss: 4.1891 - val_acc: 0.5937 - val_f1_m: 0.5922 - val_precision_m: 0.6011 - val_recall_m: 0.5837\n",
            "Epoch 432/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1110 - acc: 0.9510 - f1_m: 0.9540 - precision_m: 0.9644 - recall_m: 0.9439 - val_loss: 4.1690 - val_acc: 0.5903 - val_f1_m: 0.5892 - val_precision_m: 0.5984 - val_recall_m: 0.5805\n",
            "Epoch 433/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1112 - acc: 0.9503 - f1_m: 0.9536 - precision_m: 0.9641 - recall_m: 0.9433 - val_loss: 4.1861 - val_acc: 0.5954 - val_f1_m: 0.5939 - val_precision_m: 0.6035 - val_recall_m: 0.5847\n",
            "Epoch 434/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1096 - acc: 0.9505 - f1_m: 0.9535 - precision_m: 0.9642 - recall_m: 0.9431 - val_loss: 4.1217 - val_acc: 0.5887 - val_f1_m: 0.5877 - val_precision_m: 0.5968 - val_recall_m: 0.5789\n",
            "Epoch 435/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1024 - acc: 0.9517 - f1_m: 0.9548 - precision_m: 0.9655 - recall_m: 0.9443 - val_loss: 4.2084 - val_acc: 0.5925 - val_f1_m: 0.5918 - val_precision_m: 0.6010 - val_recall_m: 0.5830\n",
            "Epoch 436/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1093 - acc: 0.9508 - f1_m: 0.9539 - precision_m: 0.9645 - recall_m: 0.9436 - val_loss: 4.0522 - val_acc: 0.5881 - val_f1_m: 0.5865 - val_precision_m: 0.5963 - val_recall_m: 0.5770\n",
            "Epoch 437/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1024 - acc: 0.9518 - f1_m: 0.9550 - precision_m: 0.9655 - recall_m: 0.9448 - val_loss: 4.1449 - val_acc: 0.5895 - val_f1_m: 0.5886 - val_precision_m: 0.5975 - val_recall_m: 0.5800\n",
            "Epoch 438/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1066 - acc: 0.9511 - f1_m: 0.9542 - precision_m: 0.9647 - recall_m: 0.9439 - val_loss: 4.1602 - val_acc: 0.5945 - val_f1_m: 0.5936 - val_precision_m: 0.6023 - val_recall_m: 0.5853\n",
            "Epoch 439/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1085 - acc: 0.9509 - f1_m: 0.9540 - precision_m: 0.9644 - recall_m: 0.9439 - val_loss: 4.1811 - val_acc: 0.5932 - val_f1_m: 0.5925 - val_precision_m: 0.6013 - val_recall_m: 0.5840\n",
            "Epoch 440/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1219 - acc: 0.9502 - f1_m: 0.9534 - precision_m: 0.9630 - recall_m: 0.9440 - val_loss: 4.0666 - val_acc: 0.5913 - val_f1_m: 0.5899 - val_precision_m: 0.6005 - val_recall_m: 0.5798\n",
            "Epoch 441/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1176 - acc: 0.9504 - f1_m: 0.9535 - precision_m: 0.9637 - recall_m: 0.9435 - val_loss: 4.0761 - val_acc: 0.5969 - val_f1_m: 0.5956 - val_precision_m: 0.6065 - val_recall_m: 0.5851\n",
            "Epoch 442/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1063 - acc: 0.9514 - f1_m: 0.9544 - precision_m: 0.9653 - recall_m: 0.9440 - val_loss: 4.1466 - val_acc: 0.5875 - val_f1_m: 0.5863 - val_precision_m: 0.5955 - val_recall_m: 0.5775\n",
            "Epoch 443/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1045 - acc: 0.9508 - f1_m: 0.9540 - precision_m: 0.9647 - recall_m: 0.9435 - val_loss: 4.1516 - val_acc: 0.5953 - val_f1_m: 0.5947 - val_precision_m: 0.6041 - val_recall_m: 0.5857\n",
            "Epoch 444/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1046 - acc: 0.9515 - f1_m: 0.9546 - precision_m: 0.9651 - recall_m: 0.9445 - val_loss: 4.2000 - val_acc: 0.5942 - val_f1_m: 0.5933 - val_precision_m: 0.6022 - val_recall_m: 0.5847\n",
            "Epoch 445/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1018 - acc: 0.9512 - f1_m: 0.9545 - precision_m: 0.9649 - recall_m: 0.9444 - val_loss: 4.0823 - val_acc: 0.5953 - val_f1_m: 0.5944 - val_precision_m: 0.6043 - val_recall_m: 0.5850\n",
            "Epoch 446/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1032 - acc: 0.9511 - f1_m: 0.9544 - precision_m: 0.9649 - recall_m: 0.9441 - val_loss: 4.2423 - val_acc: 0.5940 - val_f1_m: 0.5934 - val_precision_m: 0.6016 - val_recall_m: 0.5855\n",
            "Epoch 447/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1046 - acc: 0.9518 - f1_m: 0.9550 - precision_m: 0.9656 - recall_m: 0.9447 - val_loss: 4.1465 - val_acc: 0.5906 - val_f1_m: 0.5891 - val_precision_m: 0.5988 - val_recall_m: 0.5798\n",
            "Epoch 448/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1050 - acc: 0.9505 - f1_m: 0.9537 - precision_m: 0.9644 - recall_m: 0.9432 - val_loss: 4.1346 - val_acc: 0.5905 - val_f1_m: 0.5895 - val_precision_m: 0.5990 - val_recall_m: 0.5804\n",
            "Epoch 449/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1063 - acc: 0.9514 - f1_m: 0.9546 - precision_m: 0.9651 - recall_m: 0.9445 - val_loss: 4.1009 - val_acc: 0.5913 - val_f1_m: 0.5903 - val_precision_m: 0.5992 - val_recall_m: 0.5817\n",
            "Epoch 450/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1060 - acc: 0.9510 - f1_m: 0.9543 - precision_m: 0.9648 - recall_m: 0.9440 - val_loss: 4.1241 - val_acc: 0.5841 - val_f1_m: 0.5821 - val_precision_m: 0.5922 - val_recall_m: 0.5724\n",
            "Epoch 451/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1093 - acc: 0.9510 - f1_m: 0.9539 - precision_m: 0.9645 - recall_m: 0.9435 - val_loss: 4.1480 - val_acc: 0.5918 - val_f1_m: 0.5900 - val_precision_m: 0.5986 - val_recall_m: 0.5817\n",
            "Epoch 452/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1191 - acc: 0.9510 - f1_m: 0.9537 - precision_m: 0.9634 - recall_m: 0.9442 - val_loss: 4.1396 - val_acc: 0.5902 - val_f1_m: 0.5890 - val_precision_m: 0.5980 - val_recall_m: 0.5804\n",
            "Epoch 453/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1189 - acc: 0.9517 - f1_m: 0.9545 - precision_m: 0.9643 - recall_m: 0.9450 - val_loss: 4.2925 - val_acc: 0.5891 - val_f1_m: 0.5883 - val_precision_m: 0.5966 - val_recall_m: 0.5803\n",
            "Epoch 454/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1199 - acc: 0.9504 - f1_m: 0.9532 - precision_m: 0.9629 - recall_m: 0.9437 - val_loss: 4.2544 - val_acc: 0.5923 - val_f1_m: 0.5909 - val_precision_m: 0.5993 - val_recall_m: 0.5829\n",
            "Epoch 455/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1244 - acc: 0.9497 - f1_m: 0.9526 - precision_m: 0.9622 - recall_m: 0.9432 - val_loss: 4.1827 - val_acc: 0.5956 - val_f1_m: 0.5946 - val_precision_m: 0.6033 - val_recall_m: 0.5861\n",
            "Epoch 456/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1065 - acc: 0.9513 - f1_m: 0.9545 - precision_m: 0.9649 - recall_m: 0.9444 - val_loss: 4.2045 - val_acc: 0.5932 - val_f1_m: 0.5928 - val_precision_m: 0.6017 - val_recall_m: 0.5842\n",
            "Epoch 457/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1058 - acc: 0.9512 - f1_m: 0.9545 - precision_m: 0.9653 - recall_m: 0.9441 - val_loss: 4.1794 - val_acc: 0.5875 - val_f1_m: 0.5872 - val_precision_m: 0.5965 - val_recall_m: 0.5783\n",
            "Epoch 458/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1105 - acc: 0.9505 - f1_m: 0.9535 - precision_m: 0.9639 - recall_m: 0.9434 - val_loss: 4.2484 - val_acc: 0.5891 - val_f1_m: 0.5880 - val_precision_m: 0.5967 - val_recall_m: 0.5797\n",
            "Epoch 459/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1054 - acc: 0.9512 - f1_m: 0.9544 - precision_m: 0.9652 - recall_m: 0.9439 - val_loss: 4.2022 - val_acc: 0.5851 - val_f1_m: 0.5848 - val_precision_m: 0.5937 - val_recall_m: 0.5761\n",
            "Epoch 460/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1072 - acc: 0.9513 - f1_m: 0.9544 - precision_m: 0.9648 - recall_m: 0.9444 - val_loss: 4.2253 - val_acc: 0.5939 - val_f1_m: 0.5932 - val_precision_m: 0.6014 - val_recall_m: 0.5852\n",
            "Epoch 461/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1046 - acc: 0.9516 - f1_m: 0.9547 - precision_m: 0.9652 - recall_m: 0.9444 - val_loss: 4.2035 - val_acc: 0.5911 - val_f1_m: 0.5898 - val_precision_m: 0.5988 - val_recall_m: 0.5811\n",
            "Epoch 462/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1024 - acc: 0.9521 - f1_m: 0.9552 - precision_m: 0.9660 - recall_m: 0.9448 - val_loss: 4.1770 - val_acc: 0.5924 - val_f1_m: 0.5912 - val_precision_m: 0.6013 - val_recall_m: 0.5815\n",
            "Epoch 463/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1088 - acc: 0.9515 - f1_m: 0.9544 - precision_m: 0.9650 - recall_m: 0.9442 - val_loss: 4.1798 - val_acc: 0.5918 - val_f1_m: 0.5906 - val_precision_m: 0.6001 - val_recall_m: 0.5815\n",
            "Epoch 464/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1032 - acc: 0.9509 - f1_m: 0.9540 - precision_m: 0.9645 - recall_m: 0.9438 - val_loss: 4.1985 - val_acc: 0.5941 - val_f1_m: 0.5936 - val_precision_m: 0.6027 - val_recall_m: 0.5848\n",
            "Epoch 465/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1077 - acc: 0.9508 - f1_m: 0.9539 - precision_m: 0.9645 - recall_m: 0.9436 - val_loss: 4.1034 - val_acc: 0.5905 - val_f1_m: 0.5897 - val_precision_m: 0.5986 - val_recall_m: 0.5811\n",
            "Epoch 466/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1028 - acc: 0.9516 - f1_m: 0.9547 - precision_m: 0.9651 - recall_m: 0.9446 - val_loss: 4.2173 - val_acc: 0.5927 - val_f1_m: 0.5923 - val_precision_m: 0.6010 - val_recall_m: 0.5838\n",
            "Epoch 467/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1085 - acc: 0.9514 - f1_m: 0.9543 - precision_m: 0.9647 - recall_m: 0.9442 - val_loss: 4.1138 - val_acc: 0.5918 - val_f1_m: 0.5911 - val_precision_m: 0.5999 - val_recall_m: 0.5826\n",
            "Epoch 468/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1075 - acc: 0.9511 - f1_m: 0.9542 - precision_m: 0.9646 - recall_m: 0.9441 - val_loss: 4.1012 - val_acc: 0.5924 - val_f1_m: 0.5921 - val_precision_m: 0.6010 - val_recall_m: 0.5835\n",
            "Epoch 469/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1031 - acc: 0.9513 - f1_m: 0.9544 - precision_m: 0.9649 - recall_m: 0.9442 - val_loss: 4.1326 - val_acc: 0.5937 - val_f1_m: 0.5933 - val_precision_m: 0.6016 - val_recall_m: 0.5853\n",
            "Epoch 470/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1028 - acc: 0.9514 - f1_m: 0.9546 - precision_m: 0.9651 - recall_m: 0.9444 - val_loss: 4.3115 - val_acc: 0.5940 - val_f1_m: 0.5929 - val_precision_m: 0.6012 - val_recall_m: 0.5849\n",
            "Epoch 471/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1080 - acc: 0.9510 - f1_m: 0.9540 - precision_m: 0.9643 - recall_m: 0.9440 - val_loss: 4.0811 - val_acc: 0.5929 - val_f1_m: 0.5922 - val_precision_m: 0.6020 - val_recall_m: 0.5829\n",
            "Epoch 472/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1127 - acc: 0.9515 - f1_m: 0.9545 - precision_m: 0.9643 - recall_m: 0.9448 - val_loss: 4.2133 - val_acc: 0.5931 - val_f1_m: 0.5916 - val_precision_m: 0.5994 - val_recall_m: 0.5840\n",
            "Epoch 473/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1312 - acc: 0.9514 - f1_m: 0.9538 - precision_m: 0.9631 - recall_m: 0.9447 - val_loss: 4.2132 - val_acc: 0.5906 - val_f1_m: 0.5899 - val_precision_m: 0.5988 - val_recall_m: 0.5813\n",
            "Epoch 474/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1246 - acc: 0.9508 - f1_m: 0.9535 - precision_m: 0.9631 - recall_m: 0.9442 - val_loss: 4.0752 - val_acc: 0.5929 - val_f1_m: 0.5916 - val_precision_m: 0.6022 - val_recall_m: 0.5815\n",
            "Epoch 475/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1068 - acc: 0.9510 - f1_m: 0.9540 - precision_m: 0.9648 - recall_m: 0.9436 - val_loss: 4.1887 - val_acc: 0.5897 - val_f1_m: 0.5888 - val_precision_m: 0.5985 - val_recall_m: 0.5795\n",
            "Epoch 476/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1067 - acc: 0.9514 - f1_m: 0.9545 - precision_m: 0.9650 - recall_m: 0.9443 - val_loss: 4.0946 - val_acc: 0.5907 - val_f1_m: 0.5902 - val_precision_m: 0.5998 - val_recall_m: 0.5809\n",
            "Epoch 477/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1075 - acc: 0.9517 - f1_m: 0.9549 - precision_m: 0.9654 - recall_m: 0.9447 - val_loss: 4.1703 - val_acc: 0.5906 - val_f1_m: 0.5899 - val_precision_m: 0.5997 - val_recall_m: 0.5806\n",
            "Epoch 478/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1180 - acc: 0.9507 - f1_m: 0.9537 - precision_m: 0.9641 - recall_m: 0.9436 - val_loss: 4.0560 - val_acc: 0.5906 - val_f1_m: 0.5894 - val_precision_m: 0.5995 - val_recall_m: 0.5796\n",
            "Epoch 479/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1050 - acc: 0.9514 - f1_m: 0.9544 - precision_m: 0.9653 - recall_m: 0.9439 - val_loss: 4.1911 - val_acc: 0.5909 - val_f1_m: 0.5903 - val_precision_m: 0.5999 - val_recall_m: 0.5810\n",
            "Epoch 480/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1071 - acc: 0.9505 - f1_m: 0.9535 - precision_m: 0.9642 - recall_m: 0.9432 - val_loss: 4.1010 - val_acc: 0.5874 - val_f1_m: 0.5859 - val_precision_m: 0.5959 - val_recall_m: 0.5763\n",
            "Epoch 481/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1040 - acc: 0.9513 - f1_m: 0.9543 - precision_m: 0.9652 - recall_m: 0.9438 - val_loss: 4.0747 - val_acc: 0.5904 - val_f1_m: 0.5890 - val_precision_m: 0.5997 - val_recall_m: 0.5787\n",
            "Epoch 482/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1057 - acc: 0.9514 - f1_m: 0.9546 - precision_m: 0.9653 - recall_m: 0.9443 - val_loss: 4.2222 - val_acc: 0.5904 - val_f1_m: 0.5898 - val_precision_m: 0.5995 - val_recall_m: 0.5805\n",
            "Epoch 483/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1016 - acc: 0.9526 - f1_m: 0.9558 - precision_m: 0.9663 - recall_m: 0.9455 - val_loss: 4.2043 - val_acc: 0.5877 - val_f1_m: 0.5869 - val_precision_m: 0.5962 - val_recall_m: 0.5779\n",
            "Epoch 484/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1011 - acc: 0.9522 - f1_m: 0.9553 - precision_m: 0.9657 - recall_m: 0.9453 - val_loss: 4.2570 - val_acc: 0.5949 - val_f1_m: 0.5935 - val_precision_m: 0.6029 - val_recall_m: 0.5843\n",
            "Epoch 485/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1063 - acc: 0.9509 - f1_m: 0.9542 - precision_m: 0.9648 - recall_m: 0.9438 - val_loss: 4.2031 - val_acc: 0.5923 - val_f1_m: 0.5913 - val_precision_m: 0.6004 - val_recall_m: 0.5825\n",
            "Epoch 486/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1034 - acc: 0.9516 - f1_m: 0.9546 - precision_m: 0.9650 - recall_m: 0.9446 - val_loss: 4.3062 - val_acc: 0.5891 - val_f1_m: 0.5883 - val_precision_m: 0.5971 - val_recall_m: 0.5799\n",
            "Epoch 487/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1054 - acc: 0.9510 - f1_m: 0.9541 - precision_m: 0.9646 - recall_m: 0.9438 - val_loss: 4.2066 - val_acc: 0.5899 - val_f1_m: 0.5896 - val_precision_m: 0.5989 - val_recall_m: 0.5807\n",
            "Epoch 488/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1016 - acc: 0.9521 - f1_m: 0.9552 - precision_m: 0.9658 - recall_m: 0.9450 - val_loss: 4.1791 - val_acc: 0.5928 - val_f1_m: 0.5914 - val_precision_m: 0.6012 - val_recall_m: 0.5819\n",
            "Epoch 489/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1023 - acc: 0.9516 - f1_m: 0.9550 - precision_m: 0.9657 - recall_m: 0.9446 - val_loss: 4.1539 - val_acc: 0.5853 - val_f1_m: 0.5847 - val_precision_m: 0.5945 - val_recall_m: 0.5753\n",
            "Epoch 490/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1071 - acc: 0.9504 - f1_m: 0.9535 - precision_m: 0.9641 - recall_m: 0.9433 - val_loss: 4.2388 - val_acc: 0.5867 - val_f1_m: 0.5861 - val_precision_m: 0.5952 - val_recall_m: 0.5772\n",
            "Epoch 491/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1027 - acc: 0.9514 - f1_m: 0.9547 - precision_m: 0.9652 - recall_m: 0.9444 - val_loss: 4.3077 - val_acc: 0.5886 - val_f1_m: 0.5873 - val_precision_m: 0.5963 - val_recall_m: 0.5785\n",
            "Epoch 492/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1022 - acc: 0.9512 - f1_m: 0.9543 - precision_m: 0.9646 - recall_m: 0.9443 - val_loss: 4.2332 - val_acc: 0.5910 - val_f1_m: 0.5902 - val_precision_m: 0.5992 - val_recall_m: 0.5815\n",
            "Epoch 493/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0999 - acc: 0.9525 - f1_m: 0.9556 - precision_m: 0.9660 - recall_m: 0.9454 - val_loss: 4.1567 - val_acc: 0.5877 - val_f1_m: 0.5857 - val_precision_m: 0.5966 - val_recall_m: 0.5753\n",
            "Epoch 494/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1045 - acc: 0.9518 - f1_m: 0.9548 - precision_m: 0.9652 - recall_m: 0.9448 - val_loss: 4.3343 - val_acc: 0.5832 - val_f1_m: 0.5821 - val_precision_m: 0.5909 - val_recall_m: 0.5736\n",
            "Epoch 495/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1069 - acc: 0.9514 - f1_m: 0.9545 - precision_m: 0.9649 - recall_m: 0.9445 - val_loss: 4.2263 - val_acc: 0.5930 - val_f1_m: 0.5920 - val_precision_m: 0.6018 - val_recall_m: 0.5827\n",
            "Epoch 496/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1055 - acc: 0.9520 - f1_m: 0.9549 - precision_m: 0.9655 - recall_m: 0.9446 - val_loss: 4.1970 - val_acc: 0.5867 - val_f1_m: 0.5860 - val_precision_m: 0.5959 - val_recall_m: 0.5765\n",
            "Epoch 497/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1038 - acc: 0.9513 - f1_m: 0.9544 - precision_m: 0.9650 - recall_m: 0.9440 - val_loss: 4.0715 - val_acc: 0.5974 - val_f1_m: 0.5961 - val_precision_m: 0.6062 - val_recall_m: 0.5865\n",
            "Epoch 498/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1131 - acc: 0.9513 - f1_m: 0.9543 - precision_m: 0.9647 - recall_m: 0.9442 - val_loss: 4.2873 - val_acc: 0.5933 - val_f1_m: 0.5921 - val_precision_m: 0.6008 - val_recall_m: 0.5838\n",
            "Epoch 499/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.1162 - acc: 0.9510 - f1_m: 0.9540 - precision_m: 0.9641 - recall_m: 0.9441 - val_loss: 4.2146 - val_acc: 0.5918 - val_f1_m: 0.5904 - val_precision_m: 0.5995 - val_recall_m: 0.5816\n",
            "Epoch 500/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1147 - acc: 0.9509 - f1_m: 0.9539 - precision_m: 0.9642 - recall_m: 0.9439 - val_loss: 4.2015 - val_acc: 0.5921 - val_f1_m: 0.5911 - val_precision_m: 0.6006 - val_recall_m: 0.5821\n",
            "Epoch 501/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1061 - acc: 0.9518 - f1_m: 0.9549 - precision_m: 0.9653 - recall_m: 0.9449 - val_loss: 4.2837 - val_acc: 0.5866 - val_f1_m: 0.5850 - val_precision_m: 0.5942 - val_recall_m: 0.5761\n",
            "Epoch 502/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1082 - acc: 0.9509 - f1_m: 0.9540 - precision_m: 0.9645 - recall_m: 0.9438 - val_loss: 4.2432 - val_acc: 0.5930 - val_f1_m: 0.5919 - val_precision_m: 0.6014 - val_recall_m: 0.5827\n",
            "Epoch 503/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1061 - acc: 0.9516 - f1_m: 0.9548 - precision_m: 0.9653 - recall_m: 0.9445 - val_loss: 4.2907 - val_acc: 0.5896 - val_f1_m: 0.5893 - val_precision_m: 0.5984 - val_recall_m: 0.5805\n",
            "Epoch 504/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1097 - acc: 0.9515 - f1_m: 0.9545 - precision_m: 0.9647 - recall_m: 0.9445 - val_loss: 4.3782 - val_acc: 0.5845 - val_f1_m: 0.5833 - val_precision_m: 0.5915 - val_recall_m: 0.5754\n",
            "Epoch 505/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1057 - acc: 0.9515 - f1_m: 0.9546 - precision_m: 0.9648 - recall_m: 0.9446 - val_loss: 4.1120 - val_acc: 0.5919 - val_f1_m: 0.5915 - val_precision_m: 0.6009 - val_recall_m: 0.5825\n",
            "Epoch 506/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1030 - acc: 0.9521 - f1_m: 0.9555 - precision_m: 0.9661 - recall_m: 0.9452 - val_loss: 4.1167 - val_acc: 0.5793 - val_f1_m: 0.5785 - val_precision_m: 0.5887 - val_recall_m: 0.5686\n",
            "Epoch 507/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1019 - acc: 0.9518 - f1_m: 0.9549 - precision_m: 0.9652 - recall_m: 0.9449 - val_loss: 4.2241 - val_acc: 0.5893 - val_f1_m: 0.5890 - val_precision_m: 0.5979 - val_recall_m: 0.5805\n",
            "Epoch 508/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1035 - acc: 0.9517 - f1_m: 0.9548 - precision_m: 0.9651 - recall_m: 0.9448 - val_loss: 4.1285 - val_acc: 0.5934 - val_f1_m: 0.5922 - val_precision_m: 0.6021 - val_recall_m: 0.5827\n",
            "Epoch 509/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1142 - acc: 0.9515 - f1_m: 0.9542 - precision_m: 0.9645 - recall_m: 0.9443 - val_loss: 4.1783 - val_acc: 0.5884 - val_f1_m: 0.5874 - val_precision_m: 0.5977 - val_recall_m: 0.5775\n",
            "Epoch 510/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1030 - acc: 0.9518 - f1_m: 0.9548 - precision_m: 0.9653 - recall_m: 0.9446 - val_loss: 4.2464 - val_acc: 0.5896 - val_f1_m: 0.5890 - val_precision_m: 0.5983 - val_recall_m: 0.5801\n",
            "Epoch 511/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1044 - acc: 0.9514 - f1_m: 0.9545 - precision_m: 0.9647 - recall_m: 0.9447 - val_loss: 4.3266 - val_acc: 0.5858 - val_f1_m: 0.5853 - val_precision_m: 0.5941 - val_recall_m: 0.5767\n",
            "Epoch 512/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1019 - acc: 0.9517 - f1_m: 0.9548 - precision_m: 0.9653 - recall_m: 0.9447 - val_loss: 4.2514 - val_acc: 0.5946 - val_f1_m: 0.5934 - val_precision_m: 0.6024 - val_recall_m: 0.5848\n",
            "Epoch 513/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1075 - acc: 0.9520 - f1_m: 0.9550 - precision_m: 0.9653 - recall_m: 0.9450 - val_loss: 4.2450 - val_acc: 0.5849 - val_f1_m: 0.5840 - val_precision_m: 0.5931 - val_recall_m: 0.5753\n",
            "Epoch 514/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1084 - acc: 0.9515 - f1_m: 0.9545 - precision_m: 0.9650 - recall_m: 0.9442 - val_loss: 4.2248 - val_acc: 0.5875 - val_f1_m: 0.5862 - val_precision_m: 0.5960 - val_recall_m: 0.5769\n",
            "Epoch 515/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1024 - acc: 0.9521 - f1_m: 0.9552 - precision_m: 0.9656 - recall_m: 0.9450 - val_loss: 4.2820 - val_acc: 0.5871 - val_f1_m: 0.5860 - val_precision_m: 0.5950 - val_recall_m: 0.5774\n",
            "Epoch 516/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1029 - acc: 0.9516 - f1_m: 0.9548 - precision_m: 0.9657 - recall_m: 0.9442 - val_loss: 4.2323 - val_acc: 0.5875 - val_f1_m: 0.5864 - val_precision_m: 0.5965 - val_recall_m: 0.5767\n",
            "Epoch 517/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1043 - acc: 0.9521 - f1_m: 0.9552 - precision_m: 0.9657 - recall_m: 0.9451 - val_loss: 4.2014 - val_acc: 0.5940 - val_f1_m: 0.5926 - val_precision_m: 0.6019 - val_recall_m: 0.5837\n",
            "Epoch 518/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1024 - acc: 0.9517 - f1_m: 0.9547 - precision_m: 0.9651 - recall_m: 0.9446 - val_loss: 4.1891 - val_acc: 0.5916 - val_f1_m: 0.5906 - val_precision_m: 0.5998 - val_recall_m: 0.5817\n",
            "Epoch 519/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.1021 - acc: 0.9521 - f1_m: 0.9553 - precision_m: 0.9657 - recall_m: 0.9453 - val_loss: 4.2689 - val_acc: 0.5904 - val_f1_m: 0.5887 - val_precision_m: 0.5982 - val_recall_m: 0.5795\n",
            "Epoch 520/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.0997 - acc: 0.9520 - f1_m: 0.9553 - precision_m: 0.9659 - recall_m: 0.9449 - val_loss: 4.2906 - val_acc: 0.5909 - val_f1_m: 0.5895 - val_precision_m: 0.5993 - val_recall_m: 0.5801\n",
            "Epoch 521/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.0997 - acc: 0.9518 - f1_m: 0.9551 - precision_m: 0.9655 - recall_m: 0.9450 - val_loss: 4.2776 - val_acc: 0.5872 - val_f1_m: 0.5862 - val_precision_m: 0.5960 - val_recall_m: 0.5769\n",
            "Epoch 522/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1083 - acc: 0.9500 - f1_m: 0.9532 - precision_m: 0.9643 - recall_m: 0.9425 - val_loss: 4.2244 - val_acc: 0.5925 - val_f1_m: 0.5919 - val_precision_m: 0.6015 - val_recall_m: 0.5827\n",
            "Epoch 523/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.1070 - acc: 0.9507 - f1_m: 0.9536 - precision_m: 0.9645 - recall_m: 0.9431 - val_loss: 4.2520 - val_acc: 0.5909 - val_f1_m: 0.5901 - val_precision_m: 0.5997 - val_recall_m: 0.5809\n",
            "Epoch 524/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2932 - acc: 0.9416 - f1_m: 0.9443 - precision_m: 0.9538 - recall_m: 0.9349 - val_loss: 4.7556 - val_acc: 0.5675 - val_f1_m: 0.5665 - val_precision_m: 0.5745 - val_recall_m: 0.5588\n",
            "Epoch 525/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 1.0117 - acc: 0.9000 - f1_m: 0.9022 - precision_m: 0.9100 - recall_m: 0.8945 - val_loss: 4.8869 - val_acc: 0.5594 - val_f1_m: 0.5588 - val_precision_m: 0.5653 - val_recall_m: 0.5525\n",
            "Epoch 526/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 1.0743 - acc: 0.8980 - f1_m: 0.8997 - precision_m: 0.9072 - recall_m: 0.8923 - val_loss: 4.7991 - val_acc: 0.5682 - val_f1_m: 0.5670 - val_precision_m: 0.5744 - val_recall_m: 0.5599\n",
            "Epoch 527/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9889 - acc: 0.8998 - f1_m: 0.9025 - precision_m: 0.9114 - recall_m: 0.8938 - val_loss: 4.5976 - val_acc: 0.5721 - val_f1_m: 0.5712 - val_precision_m: 0.5833 - val_recall_m: 0.5596\n",
            "Epoch 528/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9201 - acc: 0.9021 - f1_m: 0.9053 - precision_m: 0.9144 - recall_m: 0.8964 - val_loss: 4.7679 - val_acc: 0.5739 - val_f1_m: 0.5730 - val_precision_m: 0.5794 - val_recall_m: 0.5668\n",
            "Epoch 529/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9531 - acc: 0.9033 - f1_m: 0.9053 - precision_m: 0.9126 - recall_m: 0.8981 - val_loss: 4.7046 - val_acc: 0.5803 - val_f1_m: 0.5792 - val_precision_m: 0.5871 - val_recall_m: 0.5715\n",
            "Epoch 530/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9256 - acc: 0.9061 - f1_m: 0.9080 - precision_m: 0.9161 - recall_m: 0.9000 - val_loss: 4.7100 - val_acc: 0.5757 - val_f1_m: 0.5739 - val_precision_m: 0.5816 - val_recall_m: 0.5666\n",
            "Epoch 531/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.9057 - acc: 0.9075 - f1_m: 0.9093 - precision_m: 0.9179 - recall_m: 0.9009 - val_loss: 4.6836 - val_acc: 0.5745 - val_f1_m: 0.5728 - val_precision_m: 0.5816 - val_recall_m: 0.5644\n",
            "Epoch 532/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.9186 - acc: 0.9055 - f1_m: 0.9078 - precision_m: 0.9165 - recall_m: 0.8993 - val_loss: 4.7493 - val_acc: 0.5779 - val_f1_m: 0.5763 - val_precision_m: 0.5852 - val_recall_m: 0.5677\n",
            "Epoch 533/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9288 - acc: 0.9034 - f1_m: 0.9059 - precision_m: 0.9144 - recall_m: 0.8976 - val_loss: 4.8084 - val_acc: 0.5754 - val_f1_m: 0.5746 - val_precision_m: 0.5823 - val_recall_m: 0.5672\n",
            "Epoch 534/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9217 - acc: 0.9059 - f1_m: 0.9082 - precision_m: 0.9167 - recall_m: 0.9000 - val_loss: 4.8192 - val_acc: 0.5763 - val_f1_m: 0.5753 - val_precision_m: 0.5834 - val_recall_m: 0.5674\n",
            "Epoch 535/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.9073 - acc: 0.9058 - f1_m: 0.9084 - precision_m: 0.9171 - recall_m: 0.9001 - val_loss: 4.7820 - val_acc: 0.5777 - val_f1_m: 0.5763 - val_precision_m: 0.5844 - val_recall_m: 0.5685\n",
            "Epoch 536/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9394 - acc: 0.9041 - f1_m: 0.9062 - precision_m: 0.9149 - recall_m: 0.8977 - val_loss: 4.9110 - val_acc: 0.5739 - val_f1_m: 0.5714 - val_precision_m: 0.5802 - val_recall_m: 0.5630\n",
            "Epoch 537/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9527 - acc: 0.9045 - f1_m: 0.9063 - precision_m: 0.9154 - recall_m: 0.8975 - val_loss: 4.8869 - val_acc: 0.5748 - val_f1_m: 0.5724 - val_precision_m: 0.5808 - val_recall_m: 0.5643\n",
            "Epoch 538/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9459 - acc: 0.9053 - f1_m: 0.9070 - precision_m: 0.9160 - recall_m: 0.8982 - val_loss: 4.9265 - val_acc: 0.5721 - val_f1_m: 0.5703 - val_precision_m: 0.5787 - val_recall_m: 0.5622\n",
            "Epoch 539/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.9491 - acc: 0.9049 - f1_m: 0.9068 - precision_m: 0.9154 - recall_m: 0.8983 - val_loss: 4.9016 - val_acc: 0.5693 - val_f1_m: 0.5669 - val_precision_m: 0.5751 - val_recall_m: 0.5591\n",
            "Epoch 540/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9504 - acc: 0.9053 - f1_m: 0.9072 - precision_m: 0.9161 - recall_m: 0.8984 - val_loss: 4.8539 - val_acc: 0.5744 - val_f1_m: 0.5719 - val_precision_m: 0.5810 - val_recall_m: 0.5631\n",
            "Epoch 541/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9516 - acc: 0.9043 - f1_m: 0.9060 - precision_m: 0.9151 - recall_m: 0.8973 - val_loss: 4.9424 - val_acc: 0.5720 - val_f1_m: 0.5702 - val_precision_m: 0.5783 - val_recall_m: 0.5624\n",
            "Epoch 542/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9538 - acc: 0.9037 - f1_m: 0.9055 - precision_m: 0.9144 - recall_m: 0.8969 - val_loss: 4.8603 - val_acc: 0.5750 - val_f1_m: 0.5729 - val_precision_m: 0.5819 - val_recall_m: 0.5643\n",
            "Epoch 543/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.9473 - acc: 0.9043 - f1_m: 0.9061 - precision_m: 0.9151 - recall_m: 0.8974 - val_loss: 4.8709 - val_acc: 0.5738 - val_f1_m: 0.5714 - val_precision_m: 0.5801 - val_recall_m: 0.5630\n",
            "Epoch 544/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9584 - acc: 0.9028 - f1_m: 0.9049 - precision_m: 0.9140 - recall_m: 0.8960 - val_loss: 4.7445 - val_acc: 0.5736 - val_f1_m: 0.5712 - val_precision_m: 0.5804 - val_recall_m: 0.5623\n",
            "Epoch 545/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9484 - acc: 0.9033 - f1_m: 0.9056 - precision_m: 0.9152 - recall_m: 0.8963 - val_loss: 4.8007 - val_acc: 0.5710 - val_f1_m: 0.5686 - val_precision_m: 0.5783 - val_recall_m: 0.5594\n",
            "Epoch 546/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9635 - acc: 0.9026 - f1_m: 0.9044 - precision_m: 0.9141 - recall_m: 0.8949 - val_loss: 4.7912 - val_acc: 0.5666 - val_f1_m: 0.5642 - val_precision_m: 0.5744 - val_recall_m: 0.5544\n",
            "Epoch 547/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9584 - acc: 0.9028 - f1_m: 0.9045 - precision_m: 0.9138 - recall_m: 0.8955 - val_loss: 4.8276 - val_acc: 0.5756 - val_f1_m: 0.5738 - val_precision_m: 0.5828 - val_recall_m: 0.5652\n",
            "Epoch 548/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9496 - acc: 0.9048 - f1_m: 0.9066 - precision_m: 0.9155 - recall_m: 0.8979 - val_loss: 4.8285 - val_acc: 0.5783 - val_f1_m: 0.5765 - val_precision_m: 0.5850 - val_recall_m: 0.5683\n",
            "Epoch 549/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9426 - acc: 0.9033 - f1_m: 0.9064 - precision_m: 0.9153 - recall_m: 0.8978 - val_loss: 4.8069 - val_acc: 0.5746 - val_f1_m: 0.5748 - val_precision_m: 0.5838 - val_recall_m: 0.5660\n",
            "Epoch 550/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9412 - acc: 0.9044 - f1_m: 0.9068 - precision_m: 0.9157 - recall_m: 0.8982 - val_loss: 4.9029 - val_acc: 0.5690 - val_f1_m: 0.5671 - val_precision_m: 0.5757 - val_recall_m: 0.5588\n",
            "Epoch 551/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9430 - acc: 0.9057 - f1_m: 0.9073 - precision_m: 0.9162 - recall_m: 0.8986 - val_loss: 4.9009 - val_acc: 0.5710 - val_f1_m: 0.5684 - val_precision_m: 0.5772 - val_recall_m: 0.5600\n",
            "Epoch 552/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9306 - acc: 0.9061 - f1_m: 0.9075 - precision_m: 0.9163 - recall_m: 0.8990 - val_loss: 4.8299 - val_acc: 0.5785 - val_f1_m: 0.5765 - val_precision_m: 0.5848 - val_recall_m: 0.5685\n",
            "Epoch 553/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9497 - acc: 0.9050 - f1_m: 0.9066 - precision_m: 0.9153 - recall_m: 0.8981 - val_loss: 4.8140 - val_acc: 0.5766 - val_f1_m: 0.5741 - val_precision_m: 0.5828 - val_recall_m: 0.5657\n",
            "Epoch 554/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9404 - acc: 0.9057 - f1_m: 0.9073 - precision_m: 0.9159 - recall_m: 0.8990 - val_loss: 4.8137 - val_acc: 0.5712 - val_f1_m: 0.5689 - val_precision_m: 0.5768 - val_recall_m: 0.5614\n",
            "Epoch 555/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9481 - acc: 0.9031 - f1_m: 0.9055 - precision_m: 0.9147 - recall_m: 0.8965 - val_loss: 4.7652 - val_acc: 0.5678 - val_f1_m: 0.5677 - val_precision_m: 0.5776 - val_recall_m: 0.5581\n",
            "Epoch 556/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9476 - acc: 0.9018 - f1_m: 0.9053 - precision_m: 0.9153 - recall_m: 0.8957 - val_loss: 4.7377 - val_acc: 0.5708 - val_f1_m: 0.5709 - val_precision_m: 0.5804 - val_recall_m: 0.5618\n",
            "Epoch 557/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9479 - acc: 0.9021 - f1_m: 0.9056 - precision_m: 0.9153 - recall_m: 0.8962 - val_loss: 4.7587 - val_acc: 0.5652 - val_f1_m: 0.5664 - val_precision_m: 0.5758 - val_recall_m: 0.5573\n",
            "Epoch 558/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9413 - acc: 0.9037 - f1_m: 0.9070 - precision_m: 0.9165 - recall_m: 0.8977 - val_loss: 4.8123 - val_acc: 0.5723 - val_f1_m: 0.5698 - val_precision_m: 0.5788 - val_recall_m: 0.5611\n",
            "Epoch 559/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9511 - acc: 0.9051 - f1_m: 0.9065 - precision_m: 0.9153 - recall_m: 0.8979 - val_loss: 4.7830 - val_acc: 0.5734 - val_f1_m: 0.5711 - val_precision_m: 0.5798 - val_recall_m: 0.5627\n",
            "Epoch 560/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.9498 - acc: 0.9038 - f1_m: 0.9058 - precision_m: 0.9156 - recall_m: 0.8963 - val_loss: 4.7841 - val_acc: 0.5751 - val_f1_m: 0.5735 - val_precision_m: 0.5820 - val_recall_m: 0.5653\n",
            "Epoch 561/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.8672 - acc: 0.9021 - f1_m: 0.9061 - precision_m: 0.9200 - recall_m: 0.8926 - val_loss: 4.5906 - val_acc: 0.5767 - val_f1_m: 0.5757 - val_precision_m: 0.5885 - val_recall_m: 0.5635\n",
            "Epoch 562/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.8410 - acc: 0.9029 - f1_m: 0.9071 - precision_m: 0.9212 - recall_m: 0.8936 - val_loss: 4.7423 - val_acc: 0.5662 - val_f1_m: 0.5671 - val_precision_m: 0.5773 - val_recall_m: 0.5572\n",
            "Epoch 563/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.8560 - acc: 0.9030 - f1_m: 0.9074 - precision_m: 0.9200 - recall_m: 0.8953 - val_loss: 4.5955 - val_acc: 0.5741 - val_f1_m: 0.5756 - val_precision_m: 0.5872 - val_recall_m: 0.5646\n",
            "Epoch 564/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.8236 - acc: 0.9041 - f1_m: 0.9092 - precision_m: 0.9232 - recall_m: 0.8958 - val_loss: 4.5798 - val_acc: 0.5757 - val_f1_m: 0.5772 - val_precision_m: 0.5890 - val_recall_m: 0.5660\n",
            "Epoch 565/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.8176 - acc: 0.9038 - f1_m: 0.9090 - precision_m: 0.9239 - recall_m: 0.8947 - val_loss: 4.6420 - val_acc: 0.5720 - val_f1_m: 0.5724 - val_precision_m: 0.5841 - val_recall_m: 0.5613\n",
            "Epoch 566/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.8124 - acc: 0.9059 - f1_m: 0.9102 - precision_m: 0.9250 - recall_m: 0.8961 - val_loss: 4.6531 - val_acc: 0.5796 - val_f1_m: 0.5780 - val_precision_m: 0.5891 - val_recall_m: 0.5673\n",
            "Epoch 567/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.7545 - acc: 0.9069 - f1_m: 0.9117 - precision_m: 0.9285 - recall_m: 0.8956 - val_loss: 4.4419 - val_acc: 0.5796 - val_f1_m: 0.5781 - val_precision_m: 0.5941 - val_recall_m: 0.5631\n",
            "Epoch 568/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.7105 - acc: 0.9044 - f1_m: 0.9106 - precision_m: 0.9307 - recall_m: 0.8915 - val_loss: 4.6285 - val_acc: 0.5662 - val_f1_m: 0.5643 - val_precision_m: 0.5802 - val_recall_m: 0.5494\n",
            "Epoch 569/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.7220 - acc: 0.9003 - f1_m: 0.9075 - precision_m: 0.9296 - recall_m: 0.8866 - val_loss: 4.4146 - val_acc: 0.5746 - val_f1_m: 0.5732 - val_precision_m: 0.5920 - val_recall_m: 0.5558\n",
            "Epoch 570/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.6902 - acc: 0.9013 - f1_m: 0.9087 - precision_m: 0.9319 - recall_m: 0.8869 - val_loss: 4.3901 - val_acc: 0.5703 - val_f1_m: 0.5681 - val_precision_m: 0.5871 - val_recall_m: 0.5505\n",
            "Epoch 571/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.7142 - acc: 0.9020 - f1_m: 0.9094 - precision_m: 0.9311 - recall_m: 0.8889 - val_loss: 4.7587 - val_acc: 0.5758 - val_f1_m: 0.5750 - val_precision_m: 0.5864 - val_recall_m: 0.5641\n",
            "Epoch 572/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.6838 - acc: 0.9027 - f1_m: 0.9101 - precision_m: 0.9327 - recall_m: 0.8888 - val_loss: 4.4039 - val_acc: 0.5719 - val_f1_m: 0.5709 - val_precision_m: 0.5905 - val_recall_m: 0.5527\n",
            "Epoch 573/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.6699 - acc: 0.9026 - f1_m: 0.9107 - precision_m: 0.9342 - recall_m: 0.8885 - val_loss: 4.5873 - val_acc: 0.5692 - val_f1_m: 0.5683 - val_precision_m: 0.5851 - val_recall_m: 0.5526\n",
            "Epoch 574/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5897 - acc: 0.9018 - f1_m: 0.9114 - precision_m: 0.9396 - recall_m: 0.8850 - val_loss: 4.4174 - val_acc: 0.5660 - val_f1_m: 0.5645 - val_precision_m: 0.5853 - val_recall_m: 0.5453\n",
            "Epoch 575/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.5948 - acc: 0.9008 - f1_m: 0.9103 - precision_m: 0.9381 - recall_m: 0.8844 - val_loss: 4.2571 - val_acc: 0.5722 - val_f1_m: 0.5705 - val_precision_m: 0.5932 - val_recall_m: 0.5497\n",
            "Epoch 576/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5736 - acc: 0.9020 - f1_m: 0.9115 - precision_m: 0.9401 - recall_m: 0.8848 - val_loss: 4.4193 - val_acc: 0.5660 - val_f1_m: 0.5645 - val_precision_m: 0.5859 - val_recall_m: 0.5449\n",
            "Epoch 577/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5688 - acc: 0.9027 - f1_m: 0.9125 - precision_m: 0.9412 - recall_m: 0.8858 - val_loss: 4.3643 - val_acc: 0.5688 - val_f1_m: 0.5678 - val_precision_m: 0.5897 - val_recall_m: 0.5475\n",
            "Epoch 578/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.6041 - acc: 0.9030 - f1_m: 0.9122 - precision_m: 0.9393 - recall_m: 0.8868 - val_loss: 4.3940 - val_acc: 0.5733 - val_f1_m: 0.5726 - val_precision_m: 0.5943 - val_recall_m: 0.5527\n",
            "Epoch 579/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5689 - acc: 0.9004 - f1_m: 0.9109 - precision_m: 0.9404 - recall_m: 0.8833 - val_loss: 4.3302 - val_acc: 0.5698 - val_f1_m: 0.5679 - val_precision_m: 0.5910 - val_recall_m: 0.5468\n",
            "Epoch 580/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5477 - acc: 0.9018 - f1_m: 0.9122 - precision_m: 0.9430 - recall_m: 0.8835 - val_loss: 4.4264 - val_acc: 0.5703 - val_f1_m: 0.5686 - val_precision_m: 0.5907 - val_recall_m: 0.5482\n",
            "Epoch 581/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5552 - acc: 0.9023 - f1_m: 0.9125 - precision_m: 0.9427 - recall_m: 0.8844 - val_loss: 4.3046 - val_acc: 0.5736 - val_f1_m: 0.5719 - val_precision_m: 0.5949 - val_recall_m: 0.5508\n",
            "Epoch 582/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.5561 - acc: 0.9024 - f1_m: 0.9130 - precision_m: 0.9433 - recall_m: 0.8848 - val_loss: 4.3879 - val_acc: 0.5728 - val_f1_m: 0.5731 - val_precision_m: 0.5947 - val_recall_m: 0.5531\n",
            "Epoch 583/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5782 - acc: 0.9022 - f1_m: 0.9124 - precision_m: 0.9411 - recall_m: 0.8857 - val_loss: 4.4088 - val_acc: 0.5656 - val_f1_m: 0.5666 - val_precision_m: 0.5884 - val_recall_m: 0.5465\n",
            "Epoch 584/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.5456 - acc: 0.9024 - f1_m: 0.9127 - precision_m: 0.9429 - recall_m: 0.8845 - val_loss: 4.3899 - val_acc: 0.5704 - val_f1_m: 0.5702 - val_precision_m: 0.5912 - val_recall_m: 0.5508\n",
            "Epoch 585/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.5208 - acc: 0.9024 - f1_m: 0.9132 - precision_m: 0.9444 - recall_m: 0.8842 - val_loss: 4.2629 - val_acc: 0.5707 - val_f1_m: 0.5682 - val_precision_m: 0.5927 - val_recall_m: 0.5459\n",
            "Epoch 586/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.4616 - acc: 0.9028 - f1_m: 0.9145 - precision_m: 0.9473 - recall_m: 0.8842 - val_loss: 4.2161 - val_acc: 0.5748 - val_f1_m: 0.5739 - val_precision_m: 0.5983 - val_recall_m: 0.5515\n",
            "Epoch 587/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.4213 - acc: 0.9027 - f1_m: 0.9151 - precision_m: 0.9507 - recall_m: 0.8824 - val_loss: 4.1840 - val_acc: 0.5745 - val_f1_m: 0.5763 - val_precision_m: 0.6051 - val_recall_m: 0.5503\n",
            "Epoch 588/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3943 - acc: 0.9036 - f1_m: 0.9156 - precision_m: 0.9530 - recall_m: 0.8812 - val_loss: 4.1900 - val_acc: 0.5746 - val_f1_m: 0.5741 - val_precision_m: 0.5999 - val_recall_m: 0.5506\n",
            "Epoch 589/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3931 - acc: 0.9043 - f1_m: 0.9164 - precision_m: 0.9532 - recall_m: 0.8826 - val_loss: 4.2045 - val_acc: 0.5738 - val_f1_m: 0.5742 - val_precision_m: 0.6007 - val_recall_m: 0.5501\n",
            "Epoch 590/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3833 - acc: 0.9057 - f1_m: 0.9179 - precision_m: 0.9551 - recall_m: 0.8837 - val_loss: 4.2565 - val_acc: 0.5765 - val_f1_m: 0.5762 - val_precision_m: 0.6022 - val_recall_m: 0.5525\n",
            "Epoch 591/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3861 - acc: 0.9051 - f1_m: 0.9172 - precision_m: 0.9545 - recall_m: 0.8829 - val_loss: 4.2335 - val_acc: 0.5725 - val_f1_m: 0.5720 - val_precision_m: 0.5981 - val_recall_m: 0.5483\n",
            "Epoch 592/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3841 - acc: 0.9053 - f1_m: 0.9174 - precision_m: 0.9537 - recall_m: 0.8841 - val_loss: 4.1784 - val_acc: 0.5685 - val_f1_m: 0.5704 - val_precision_m: 0.5978 - val_recall_m: 0.5457\n",
            "Epoch 593/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.4014 - acc: 0.9049 - f1_m: 0.9171 - precision_m: 0.9519 - recall_m: 0.8851 - val_loss: 4.0725 - val_acc: 0.5771 - val_f1_m: 0.5786 - val_precision_m: 0.6074 - val_recall_m: 0.5526\n",
            "Epoch 594/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.3801 - acc: 0.9044 - f1_m: 0.9173 - precision_m: 0.9540 - recall_m: 0.8836 - val_loss: 4.2705 - val_acc: 0.5702 - val_f1_m: 0.5723 - val_precision_m: 0.5993 - val_recall_m: 0.5477\n",
            "Epoch 595/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3568 - acc: 0.9064 - f1_m: 0.9189 - precision_m: 0.9561 - recall_m: 0.8847 - val_loss: 4.0720 - val_acc: 0.5769 - val_f1_m: 0.5749 - val_precision_m: 0.6025 - val_recall_m: 0.5498\n",
            "Epoch 596/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3149 - acc: 0.9056 - f1_m: 0.9188 - precision_m: 0.9588 - recall_m: 0.8822 - val_loss: 3.9894 - val_acc: 0.5730 - val_f1_m: 0.5714 - val_precision_m: 0.6009 - val_recall_m: 0.5449\n",
            "Epoch 597/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3348 - acc: 0.9063 - f1_m: 0.9187 - precision_m: 0.9580 - recall_m: 0.8827 - val_loss: 4.1760 - val_acc: 0.5764 - val_f1_m: 0.5745 - val_precision_m: 0.6020 - val_recall_m: 0.5496\n",
            "Epoch 598/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3487 - acc: 0.9059 - f1_m: 0.9182 - precision_m: 0.9570 - recall_m: 0.8827 - val_loss: 4.2441 - val_acc: 0.5800 - val_f1_m: 0.5790 - val_precision_m: 0.6060 - val_recall_m: 0.5545\n",
            "Epoch 599/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3492 - acc: 0.9057 - f1_m: 0.9179 - precision_m: 0.9566 - recall_m: 0.8825 - val_loss: 4.0710 - val_acc: 0.5779 - val_f1_m: 0.5765 - val_precision_m: 0.6044 - val_recall_m: 0.5512\n",
            "Epoch 600/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3446 - acc: 0.9060 - f1_m: 0.9182 - precision_m: 0.9567 - recall_m: 0.8829 - val_loss: 4.1004 - val_acc: 0.5771 - val_f1_m: 0.5760 - val_precision_m: 0.6040 - val_recall_m: 0.5506\n",
            "Epoch 601/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3549 - acc: 0.9057 - f1_m: 0.9181 - precision_m: 0.9564 - recall_m: 0.8830 - val_loss: 4.0736 - val_acc: 0.5735 - val_f1_m: 0.5715 - val_precision_m: 0.5994 - val_recall_m: 0.5463\n",
            "Epoch 602/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3883 - acc: 0.9047 - f1_m: 0.9166 - precision_m: 0.9536 - recall_m: 0.8827 - val_loss: 4.1526 - val_acc: 0.5764 - val_f1_m: 0.5750 - val_precision_m: 0.6024 - val_recall_m: 0.5501\n",
            "Epoch 603/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3739 - acc: 0.9057 - f1_m: 0.9178 - precision_m: 0.9548 - recall_m: 0.8838 - val_loss: 4.1031 - val_acc: 0.5751 - val_f1_m: 0.5735 - val_precision_m: 0.6010 - val_recall_m: 0.5486\n",
            "Epoch 604/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.4063 - acc: 0.9050 - f1_m: 0.9164 - precision_m: 0.9520 - recall_m: 0.8837 - val_loss: 4.2123 - val_acc: 0.5749 - val_f1_m: 0.5732 - val_precision_m: 0.5981 - val_recall_m: 0.5505\n",
            "Epoch 605/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.4117 - acc: 0.9057 - f1_m: 0.9168 - precision_m: 0.9512 - recall_m: 0.8850 - val_loss: 4.1240 - val_acc: 0.5756 - val_f1_m: 0.5742 - val_precision_m: 0.6016 - val_recall_m: 0.5494\n",
            "Epoch 606/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3761 - acc: 0.9069 - f1_m: 0.9186 - precision_m: 0.9543 - recall_m: 0.8857 - val_loss: 4.1548 - val_acc: 0.5778 - val_f1_m: 0.5767 - val_precision_m: 0.6022 - val_recall_m: 0.5534\n",
            "Epoch 607/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3794 - acc: 0.9072 - f1_m: 0.9188 - precision_m: 0.9541 - recall_m: 0.8862 - val_loss: 4.0623 - val_acc: 0.5765 - val_f1_m: 0.5747 - val_precision_m: 0.6013 - val_recall_m: 0.5505\n",
            "Epoch 608/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.3763 - acc: 0.9072 - f1_m: 0.9187 - precision_m: 0.9546 - recall_m: 0.8857 - val_loss: 4.1278 - val_acc: 0.5759 - val_f1_m: 0.5757 - val_precision_m: 0.6017 - val_recall_m: 0.5521\n",
            "Epoch 609/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3654 - acc: 0.9072 - f1_m: 0.9188 - precision_m: 0.9553 - recall_m: 0.8852 - val_loss: 4.2139 - val_acc: 0.5769 - val_f1_m: 0.5761 - val_precision_m: 0.6016 - val_recall_m: 0.5529\n",
            "Epoch 610/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3390 - acc: 0.9078 - f1_m: 0.9195 - precision_m: 0.9571 - recall_m: 0.8850 - val_loss: 4.1186 - val_acc: 0.5728 - val_f1_m: 0.5719 - val_precision_m: 0.5982 - val_recall_m: 0.5480\n",
            "Epoch 611/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3437 - acc: 0.9084 - f1_m: 0.9201 - precision_m: 0.9572 - recall_m: 0.8860 - val_loss: 4.1629 - val_acc: 0.5799 - val_f1_m: 0.5781 - val_precision_m: 0.6055 - val_recall_m: 0.5532\n",
            "Epoch 612/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3097 - acc: 0.9078 - f1_m: 0.9200 - precision_m: 0.9591 - recall_m: 0.8841 - val_loss: 3.9937 - val_acc: 0.5721 - val_f1_m: 0.5709 - val_precision_m: 0.5983 - val_recall_m: 0.5460\n",
            "Epoch 613/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3077 - acc: 0.9080 - f1_m: 0.9202 - precision_m: 0.9592 - recall_m: 0.8844 - val_loss: 4.0018 - val_acc: 0.5780 - val_f1_m: 0.5770 - val_precision_m: 0.6046 - val_recall_m: 0.5520\n",
            "Epoch 614/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3054 - acc: 0.9069 - f1_m: 0.9191 - precision_m: 0.9587 - recall_m: 0.8829 - val_loss: 3.9962 - val_acc: 0.5763 - val_f1_m: 0.5746 - val_precision_m: 0.6027 - val_recall_m: 0.5491\n",
            "Epoch 615/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2988 - acc: 0.9073 - f1_m: 0.9194 - precision_m: 0.9596 - recall_m: 0.8827 - val_loss: 4.1056 - val_acc: 0.5824 - val_f1_m: 0.5796 - val_precision_m: 0.6103 - val_recall_m: 0.5519\n",
            "Epoch 616/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2523 - acc: 0.9077 - f1_m: 0.9200 - precision_m: 0.9636 - recall_m: 0.8805 - val_loss: 3.9724 - val_acc: 0.5800 - val_f1_m: 0.5764 - val_precision_m: 0.6093 - val_recall_m: 0.5470\n",
            "Epoch 617/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2619 - acc: 0.9069 - f1_m: 0.9193 - precision_m: 0.9627 - recall_m: 0.8800 - val_loss: 4.1229 - val_acc: 0.5742 - val_f1_m: 0.5713 - val_precision_m: 0.6006 - val_recall_m: 0.5449\n",
            "Epoch 618/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2987 - acc: 0.9064 - f1_m: 0.9186 - precision_m: 0.9603 - recall_m: 0.8806 - val_loss: 3.9996 - val_acc: 0.5769 - val_f1_m: 0.5733 - val_precision_m: 0.6055 - val_recall_m: 0.5445\n",
            "Epoch 619/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2508 - acc: 0.9071 - f1_m: 0.9195 - precision_m: 0.9634 - recall_m: 0.8797 - val_loss: 4.0864 - val_acc: 0.5795 - val_f1_m: 0.5765 - val_precision_m: 0.6076 - val_recall_m: 0.5486\n",
            "Epoch 620/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2407 - acc: 0.9079 - f1_m: 0.9199 - precision_m: 0.9645 - recall_m: 0.8795 - val_loss: 3.9420 - val_acc: 0.5751 - val_f1_m: 0.5710 - val_precision_m: 0.6036 - val_recall_m: 0.5419\n",
            "Epoch 621/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2376 - acc: 0.9078 - f1_m: 0.9205 - precision_m: 0.9654 - recall_m: 0.8798 - val_loss: 3.9429 - val_acc: 0.5800 - val_f1_m: 0.5766 - val_precision_m: 0.6086 - val_recall_m: 0.5479\n",
            "Epoch 622/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2398 - acc: 0.9080 - f1_m: 0.9203 - precision_m: 0.9646 - recall_m: 0.8802 - val_loss: 4.0534 - val_acc: 0.5753 - val_f1_m: 0.5716 - val_precision_m: 0.6038 - val_recall_m: 0.5428\n",
            "Epoch 623/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2380 - acc: 0.9077 - f1_m: 0.9202 - precision_m: 0.9642 - recall_m: 0.8804 - val_loss: 4.0649 - val_acc: 0.5752 - val_f1_m: 0.5719 - val_precision_m: 0.6033 - val_recall_m: 0.5438\n",
            "Epoch 624/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2246 - acc: 0.9083 - f1_m: 0.9209 - precision_m: 0.9656 - recall_m: 0.8804 - val_loss: 4.0082 - val_acc: 0.5773 - val_f1_m: 0.5741 - val_precision_m: 0.6063 - val_recall_m: 0.5454\n",
            "Epoch 625/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2268 - acc: 0.9076 - f1_m: 0.9203 - precision_m: 0.9651 - recall_m: 0.8798 - val_loss: 4.0254 - val_acc: 0.5786 - val_f1_m: 0.5753 - val_precision_m: 0.6070 - val_recall_m: 0.5470\n",
            "Epoch 626/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2229 - acc: 0.9081 - f1_m: 0.9206 - precision_m: 0.9654 - recall_m: 0.8801 - val_loss: 4.0740 - val_acc: 0.5774 - val_f1_m: 0.5740 - val_precision_m: 0.6051 - val_recall_m: 0.5461\n",
            "Epoch 627/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2220 - acc: 0.9077 - f1_m: 0.9204 - precision_m: 0.9654 - recall_m: 0.8798 - val_loss: 3.9653 - val_acc: 0.5763 - val_f1_m: 0.5725 - val_precision_m: 0.6051 - val_recall_m: 0.5435\n",
            "Epoch 628/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2173 - acc: 0.9078 - f1_m: 0.9207 - precision_m: 0.9665 - recall_m: 0.8794 - val_loss: 3.9792 - val_acc: 0.5797 - val_f1_m: 0.5756 - val_precision_m: 0.6078 - val_recall_m: 0.5468\n",
            "Epoch 629/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2112 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9662 - recall_m: 0.8801 - val_loss: 4.0176 - val_acc: 0.5763 - val_f1_m: 0.5725 - val_precision_m: 0.6040 - val_recall_m: 0.5444\n",
            "Epoch 630/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2110 - acc: 0.9084 - f1_m: 0.9212 - precision_m: 0.9662 - recall_m: 0.8805 - val_loss: 4.0471 - val_acc: 0.5765 - val_f1_m: 0.5728 - val_precision_m: 0.6052 - val_recall_m: 0.5439\n",
            "Epoch 631/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2172 - acc: 0.9080 - f1_m: 0.9209 - precision_m: 0.9660 - recall_m: 0.8801 - val_loss: 3.9951 - val_acc: 0.5749 - val_f1_m: 0.5710 - val_precision_m: 0.6032 - val_recall_m: 0.5423\n",
            "Epoch 632/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2632 - acc: 0.9070 - f1_m: 0.9195 - precision_m: 0.9626 - recall_m: 0.8805 - val_loss: 3.9492 - val_acc: 0.5778 - val_f1_m: 0.5737 - val_precision_m: 0.6053 - val_recall_m: 0.5455\n",
            "Epoch 633/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2263 - acc: 0.9084 - f1_m: 0.9206 - precision_m: 0.9653 - recall_m: 0.8803 - val_loss: 4.0194 - val_acc: 0.5764 - val_f1_m: 0.5725 - val_precision_m: 0.6037 - val_recall_m: 0.5445\n",
            "Epoch 634/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2202 - acc: 0.9079 - f1_m: 0.9206 - precision_m: 0.9653 - recall_m: 0.8803 - val_loss: 4.1162 - val_acc: 0.5717 - val_f1_m: 0.5690 - val_precision_m: 0.5997 - val_recall_m: 0.5414\n",
            "Epoch 635/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2258 - acc: 0.9078 - f1_m: 0.9204 - precision_m: 0.9652 - recall_m: 0.8799 - val_loss: 3.8957 - val_acc: 0.5767 - val_f1_m: 0.5731 - val_precision_m: 0.6051 - val_recall_m: 0.5445\n",
            "Epoch 636/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2224 - acc: 0.9073 - f1_m: 0.9200 - precision_m: 0.9650 - recall_m: 0.8794 - val_loss: 4.0458 - val_acc: 0.5753 - val_f1_m: 0.5718 - val_precision_m: 0.6035 - val_recall_m: 0.5435\n",
            "Epoch 637/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2305 - acc: 0.9080 - f1_m: 0.9208 - precision_m: 0.9651 - recall_m: 0.8807 - val_loss: 4.0644 - val_acc: 0.5783 - val_f1_m: 0.5755 - val_precision_m: 0.6065 - val_recall_m: 0.5477\n",
            "Epoch 638/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2861 - acc: 0.9088 - f1_m: 0.9211 - precision_m: 0.9614 - recall_m: 0.8842 - val_loss: 4.1025 - val_acc: 0.5773 - val_f1_m: 0.5752 - val_precision_m: 0.6032 - val_recall_m: 0.5499\n",
            "Epoch 639/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2943 - acc: 0.9085 - f1_m: 0.9209 - precision_m: 0.9608 - recall_m: 0.8845 - val_loss: 4.0490 - val_acc: 0.5767 - val_f1_m: 0.5752 - val_precision_m: 0.6038 - val_recall_m: 0.5493\n",
            "Epoch 640/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2489 - acc: 0.9090 - f1_m: 0.9216 - precision_m: 0.9643 - recall_m: 0.8828 - val_loss: 4.0806 - val_acc: 0.5755 - val_f1_m: 0.5719 - val_precision_m: 0.6038 - val_recall_m: 0.5435\n",
            "Epoch 641/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2316 - acc: 0.9078 - f1_m: 0.9206 - precision_m: 0.9653 - recall_m: 0.8802 - val_loss: 3.9606 - val_acc: 0.5785 - val_f1_m: 0.5754 - val_precision_m: 0.6063 - val_recall_m: 0.5477\n",
            "Epoch 642/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2243 - acc: 0.9068 - f1_m: 0.9198 - precision_m: 0.9648 - recall_m: 0.8792 - val_loss: 4.0634 - val_acc: 0.5738 - val_f1_m: 0.5701 - val_precision_m: 0.6021 - val_recall_m: 0.5415\n",
            "Epoch 643/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2199 - acc: 0.9087 - f1_m: 0.9214 - precision_m: 0.9664 - recall_m: 0.8808 - val_loss: 3.9843 - val_acc: 0.5789 - val_f1_m: 0.5764 - val_precision_m: 0.6073 - val_recall_m: 0.5486\n",
            "Epoch 644/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2592 - acc: 0.9072 - f1_m: 0.9196 - precision_m: 0.9619 - recall_m: 0.8812 - val_loss: 4.0846 - val_acc: 0.5701 - val_f1_m: 0.5669 - val_precision_m: 0.5977 - val_recall_m: 0.5394\n",
            "Epoch 645/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2290 - acc: 0.9079 - f1_m: 0.9209 - precision_m: 0.9651 - recall_m: 0.8809 - val_loss: 3.9312 - val_acc: 0.5788 - val_f1_m: 0.5756 - val_precision_m: 0.6086 - val_recall_m: 0.5462\n",
            "Epoch 646/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2271 - acc: 0.9090 - f1_m: 0.9215 - precision_m: 0.9658 - recall_m: 0.8814 - val_loss: 3.9668 - val_acc: 0.5759 - val_f1_m: 0.5718 - val_precision_m: 0.6040 - val_recall_m: 0.5431\n",
            "Epoch 647/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2204 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9661 - recall_m: 0.8811 - val_loss: 4.1695 - val_acc: 0.5734 - val_f1_m: 0.5702 - val_precision_m: 0.6014 - val_recall_m: 0.5423\n",
            "Epoch 648/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2342 - acc: 0.9078 - f1_m: 0.9205 - precision_m: 0.9648 - recall_m: 0.8803 - val_loss: 4.0245 - val_acc: 0.5785 - val_f1_m: 0.5748 - val_precision_m: 0.6066 - val_recall_m: 0.5464\n",
            "Epoch 649/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2384 - acc: 0.9073 - f1_m: 0.9196 - precision_m: 0.9640 - recall_m: 0.8795 - val_loss: 3.9244 - val_acc: 0.5791 - val_f1_m: 0.5765 - val_precision_m: 0.6080 - val_recall_m: 0.5483\n",
            "Epoch 650/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2268 - acc: 0.9084 - f1_m: 0.9213 - precision_m: 0.9660 - recall_m: 0.8810 - val_loss: 3.9255 - val_acc: 0.5783 - val_f1_m: 0.5745 - val_precision_m: 0.6069 - val_recall_m: 0.5455\n",
            "Epoch 651/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2202 - acc: 0.9081 - f1_m: 0.9209 - precision_m: 0.9661 - recall_m: 0.8800 - val_loss: 3.9260 - val_acc: 0.5820 - val_f1_m: 0.5787 - val_precision_m: 0.6110 - val_recall_m: 0.5499\n",
            "Epoch 652/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2157 - acc: 0.9085 - f1_m: 0.9216 - precision_m: 0.9665 - recall_m: 0.8810 - val_loss: 3.9927 - val_acc: 0.5777 - val_f1_m: 0.5744 - val_precision_m: 0.6062 - val_recall_m: 0.5461\n",
            "Epoch 653/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2310 - acc: 0.9084 - f1_m: 0.9209 - precision_m: 0.9653 - recall_m: 0.8806 - val_loss: 4.0101 - val_acc: 0.5752 - val_f1_m: 0.5718 - val_precision_m: 0.6037 - val_recall_m: 0.5433\n",
            "Epoch 654/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2234 - acc: 0.9074 - f1_m: 0.9201 - precision_m: 0.9647 - recall_m: 0.8797 - val_loss: 4.0838 - val_acc: 0.5767 - val_f1_m: 0.5727 - val_precision_m: 0.6039 - val_recall_m: 0.5448\n",
            "Epoch 655/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2220 - acc: 0.9085 - f1_m: 0.9213 - precision_m: 0.9660 - recall_m: 0.8809 - val_loss: 4.0046 - val_acc: 0.5792 - val_f1_m: 0.5759 - val_precision_m: 0.6074 - val_recall_m: 0.5477\n",
            "Epoch 656/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2159 - acc: 0.9082 - f1_m: 0.9209 - precision_m: 0.9661 - recall_m: 0.8801 - val_loss: 4.0368 - val_acc: 0.5778 - val_f1_m: 0.5737 - val_precision_m: 0.6054 - val_recall_m: 0.5455\n",
            "Epoch 657/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2173 - acc: 0.9080 - f1_m: 0.9208 - precision_m: 0.9657 - recall_m: 0.8803 - val_loss: 4.0274 - val_acc: 0.5787 - val_f1_m: 0.5747 - val_precision_m: 0.6068 - val_recall_m: 0.5461\n",
            "Epoch 658/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2218 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9662 - recall_m: 0.8807 - val_loss: 4.0111 - val_acc: 0.5738 - val_f1_m: 0.5703 - val_precision_m: 0.6022 - val_recall_m: 0.5417\n",
            "Epoch 659/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2282 - acc: 0.9076 - f1_m: 0.9202 - precision_m: 0.9648 - recall_m: 0.8799 - val_loss: 4.0586 - val_acc: 0.5754 - val_f1_m: 0.5718 - val_precision_m: 0.6036 - val_recall_m: 0.5434\n",
            "Epoch 660/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2192 - acc: 0.9087 - f1_m: 0.9214 - precision_m: 0.9662 - recall_m: 0.8809 - val_loss: 4.0234 - val_acc: 0.5751 - val_f1_m: 0.5713 - val_precision_m: 0.6031 - val_recall_m: 0.5428\n",
            "Epoch 661/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2156 - acc: 0.9081 - f1_m: 0.9208 - precision_m: 0.9658 - recall_m: 0.8802 - val_loss: 4.0972 - val_acc: 0.5764 - val_f1_m: 0.5728 - val_precision_m: 0.6044 - val_recall_m: 0.5445\n",
            "Epoch 662/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2209 - acc: 0.9096 - f1_m: 0.9223 - precision_m: 0.9669 - recall_m: 0.8819 - val_loss: 4.0774 - val_acc: 0.5699 - val_f1_m: 0.5663 - val_precision_m: 0.5982 - val_recall_m: 0.5379\n",
            "Epoch 663/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2181 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9661 - recall_m: 0.8805 - val_loss: 3.9536 - val_acc: 0.5683 - val_f1_m: 0.5643 - val_precision_m: 0.5977 - val_recall_m: 0.5346\n",
            "Epoch 664/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2200 - acc: 0.9071 - f1_m: 0.9200 - precision_m: 0.9646 - recall_m: 0.8797 - val_loss: 3.9707 - val_acc: 0.5769 - val_f1_m: 0.5732 - val_precision_m: 0.6055 - val_recall_m: 0.5444\n",
            "Epoch 665/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2159 - acc: 0.9083 - f1_m: 0.9213 - precision_m: 0.9663 - recall_m: 0.8806 - val_loss: 4.0561 - val_acc: 0.5773 - val_f1_m: 0.5743 - val_precision_m: 0.6055 - val_recall_m: 0.5464\n",
            "Epoch 666/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2207 - acc: 0.9070 - f1_m: 0.9198 - precision_m: 0.9655 - recall_m: 0.8786 - val_loss: 3.8696 - val_acc: 0.5804 - val_f1_m: 0.5767 - val_precision_m: 0.6094 - val_recall_m: 0.5475\n",
            "Epoch 667/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2257 - acc: 0.9068 - f1_m: 0.9196 - precision_m: 0.9645 - recall_m: 0.8790 - val_loss: 4.0375 - val_acc: 0.5738 - val_f1_m: 0.5702 - val_precision_m: 0.6023 - val_recall_m: 0.5416\n",
            "Epoch 668/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2232 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9657 - recall_m: 0.8806 - val_loss: 4.0064 - val_acc: 0.5759 - val_f1_m: 0.5722 - val_precision_m: 0.6046 - val_recall_m: 0.5433\n",
            "Epoch 669/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2137 - acc: 0.9092 - f1_m: 0.9220 - precision_m: 0.9668 - recall_m: 0.8814 - val_loss: 4.0327 - val_acc: 0.5746 - val_f1_m: 0.5711 - val_precision_m: 0.6031 - val_recall_m: 0.5426\n",
            "Epoch 670/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2088 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9668 - recall_m: 0.8813 - val_loss: 4.0686 - val_acc: 0.5814 - val_f1_m: 0.5785 - val_precision_m: 0.6099 - val_recall_m: 0.5503\n",
            "Epoch 671/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2133 - acc: 0.9080 - f1_m: 0.9209 - precision_m: 0.9659 - recall_m: 0.8802 - val_loss: 3.9975 - val_acc: 0.5797 - val_f1_m: 0.5768 - val_precision_m: 0.6087 - val_recall_m: 0.5482\n",
            "Epoch 672/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2276 - acc: 0.9090 - f1_m: 0.9215 - precision_m: 0.9658 - recall_m: 0.8813 - val_loss: 3.9347 - val_acc: 0.5775 - val_f1_m: 0.5738 - val_precision_m: 0.6066 - val_recall_m: 0.5445\n",
            "Epoch 673/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2169 - acc: 0.9081 - f1_m: 0.9210 - precision_m: 0.9662 - recall_m: 0.8802 - val_loss: 3.9875 - val_acc: 0.5726 - val_f1_m: 0.5686 - val_precision_m: 0.6011 - val_recall_m: 0.5397\n",
            "Epoch 674/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2139 - acc: 0.9081 - f1_m: 0.9211 - precision_m: 0.9665 - recall_m: 0.8802 - val_loss: 4.0364 - val_acc: 0.5740 - val_f1_m: 0.5708 - val_precision_m: 0.6026 - val_recall_m: 0.5423\n",
            "Epoch 675/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2160 - acc: 0.9083 - f1_m: 0.9212 - precision_m: 0.9662 - recall_m: 0.8806 - val_loss: 4.0162 - val_acc: 0.5757 - val_f1_m: 0.5714 - val_precision_m: 0.6033 - val_recall_m: 0.5429\n",
            "Epoch 676/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2148 - acc: 0.9082 - f1_m: 0.9212 - precision_m: 0.9660 - recall_m: 0.8806 - val_loss: 4.0307 - val_acc: 0.5811 - val_f1_m: 0.5774 - val_precision_m: 0.6085 - val_recall_m: 0.5496\n",
            "Epoch 677/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2295 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9655 - recall_m: 0.8814 - val_loss: 3.9928 - val_acc: 0.5798 - val_f1_m: 0.5757 - val_precision_m: 0.6071 - val_recall_m: 0.5475\n",
            "Epoch 678/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2264 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9654 - recall_m: 0.8808 - val_loss: 4.0349 - val_acc: 0.5740 - val_f1_m: 0.5699 - val_precision_m: 0.6008 - val_recall_m: 0.5423\n",
            "Epoch 679/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2369 - acc: 0.9081 - f1_m: 0.9208 - precision_m: 0.9649 - recall_m: 0.8809 - val_loss: 4.0248 - val_acc: 0.5736 - val_f1_m: 0.5693 - val_precision_m: 0.6007 - val_recall_m: 0.5412\n",
            "Epoch 680/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2492 - acc: 0.9071 - f1_m: 0.9197 - precision_m: 0.9632 - recall_m: 0.8803 - val_loss: 3.9984 - val_acc: 0.5751 - val_f1_m: 0.5712 - val_precision_m: 0.6032 - val_recall_m: 0.5427\n",
            "Epoch 681/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.3212 - acc: 0.9067 - f1_m: 0.9186 - precision_m: 0.9589 - recall_m: 0.8818 - val_loss: 4.0639 - val_acc: 0.5810 - val_f1_m: 0.5785 - val_precision_m: 0.6087 - val_recall_m: 0.5513\n",
            "Epoch 682/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2451 - acc: 0.9077 - f1_m: 0.9203 - precision_m: 0.9642 - recall_m: 0.8806 - val_loss: 4.0897 - val_acc: 0.5789 - val_f1_m: 0.5760 - val_precision_m: 0.6068 - val_recall_m: 0.5483\n",
            "Epoch 683/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2323 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9655 - recall_m: 0.8812 - val_loss: 4.1071 - val_acc: 0.5726 - val_f1_m: 0.5690 - val_precision_m: 0.6000 - val_recall_m: 0.5412\n",
            "Epoch 684/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2250 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9665 - recall_m: 0.8817 - val_loss: 4.1485 - val_acc: 0.5696 - val_f1_m: 0.5662 - val_precision_m: 0.5974 - val_recall_m: 0.5383\n",
            "Epoch 685/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2235 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9660 - recall_m: 0.8811 - val_loss: 4.0504 - val_acc: 0.5769 - val_f1_m: 0.5739 - val_precision_m: 0.6052 - val_recall_m: 0.5459\n",
            "Epoch 686/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2885 - acc: 0.9076 - f1_m: 0.9200 - precision_m: 0.9601 - recall_m: 0.8834 - val_loss: 3.9957 - val_acc: 0.5809 - val_f1_m: 0.5782 - val_precision_m: 0.6097 - val_recall_m: 0.5501\n",
            "Epoch 687/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2178 - acc: 0.9085 - f1_m: 0.9212 - precision_m: 0.9664 - recall_m: 0.8803 - val_loss: 4.0861 - val_acc: 0.5785 - val_f1_m: 0.5756 - val_precision_m: 0.6073 - val_recall_m: 0.5471\n",
            "Epoch 688/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2124 - acc: 0.9084 - f1_m: 0.9214 - precision_m: 0.9662 - recall_m: 0.8808 - val_loss: 4.0324 - val_acc: 0.5763 - val_f1_m: 0.5733 - val_precision_m: 0.6049 - val_recall_m: 0.5450\n",
            "Epoch 689/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2098 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9670 - recall_m: 0.8812 - val_loss: 4.0389 - val_acc: 0.5751 - val_f1_m: 0.5715 - val_precision_m: 0.6034 - val_recall_m: 0.5430\n",
            "Epoch 690/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2144 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9661 - recall_m: 0.8805 - val_loss: 4.0243 - val_acc: 0.5740 - val_f1_m: 0.5708 - val_precision_m: 0.6025 - val_recall_m: 0.5424\n",
            "Epoch 691/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2192 - acc: 0.9085 - f1_m: 0.9213 - precision_m: 0.9662 - recall_m: 0.8808 - val_loss: 3.9988 - val_acc: 0.5739 - val_f1_m: 0.5700 - val_precision_m: 0.6020 - val_recall_m: 0.5414\n",
            "Epoch 692/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2272 - acc: 0.9088 - f1_m: 0.9213 - precision_m: 0.9658 - recall_m: 0.8811 - val_loss: 4.0393 - val_acc: 0.5775 - val_f1_m: 0.5736 - val_precision_m: 0.6043 - val_recall_m: 0.5462\n",
            "Epoch 693/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2243 - acc: 0.9093 - f1_m: 0.9220 - precision_m: 0.9664 - recall_m: 0.8818 - val_loss: 4.0487 - val_acc: 0.5775 - val_f1_m: 0.5738 - val_precision_m: 0.6048 - val_recall_m: 0.5461\n",
            "Epoch 694/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2194 - acc: 0.9089 - f1_m: 0.9217 - precision_m: 0.9664 - recall_m: 0.8813 - val_loss: 4.0896 - val_acc: 0.5768 - val_f1_m: 0.5742 - val_precision_m: 0.6051 - val_recall_m: 0.5465\n",
            "Epoch 695/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2181 - acc: 0.9090 - f1_m: 0.9218 - precision_m: 0.9668 - recall_m: 0.8812 - val_loss: 3.9645 - val_acc: 0.5738 - val_f1_m: 0.5704 - val_precision_m: 0.6027 - val_recall_m: 0.5416\n",
            "Epoch 696/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2186 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9663 - recall_m: 0.8807 - val_loss: 4.0104 - val_acc: 0.5781 - val_f1_m: 0.5741 - val_precision_m: 0.6059 - val_recall_m: 0.5457\n",
            "Epoch 697/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2189 - acc: 0.9078 - f1_m: 0.9207 - precision_m: 0.9656 - recall_m: 0.8801 - val_loss: 4.0509 - val_acc: 0.5740 - val_f1_m: 0.5707 - val_precision_m: 0.6018 - val_recall_m: 0.5428\n",
            "Epoch 698/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2163 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9662 - recall_m: 0.8804 - val_loss: 3.9809 - val_acc: 0.5724 - val_f1_m: 0.5692 - val_precision_m: 0.6004 - val_recall_m: 0.5413\n",
            "Epoch 699/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2167 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9669 - recall_m: 0.8813 - val_loss: 4.0339 - val_acc: 0.5797 - val_f1_m: 0.5763 - val_precision_m: 0.6072 - val_recall_m: 0.5486\n",
            "Epoch 700/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2160 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9657 - recall_m: 0.8806 - val_loss: 4.0720 - val_acc: 0.5787 - val_f1_m: 0.5757 - val_precision_m: 0.6061 - val_recall_m: 0.5484\n",
            "Epoch 701/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2137 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9661 - recall_m: 0.8811 - val_loss: 4.0092 - val_acc: 0.5787 - val_f1_m: 0.5759 - val_precision_m: 0.6073 - val_recall_m: 0.5477\n",
            "Epoch 702/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2205 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9665 - recall_m: 0.8808 - val_loss: 4.0083 - val_acc: 0.5789 - val_f1_m: 0.5757 - val_precision_m: 0.6075 - val_recall_m: 0.5473\n",
            "Epoch 703/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2203 - acc: 0.9093 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8813 - val_loss: 4.0888 - val_acc: 0.5779 - val_f1_m: 0.5744 - val_precision_m: 0.6057 - val_recall_m: 0.5463\n",
            "Epoch 704/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2178 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9662 - recall_m: 0.8808 - val_loss: 4.1008 - val_acc: 0.5774 - val_f1_m: 0.5742 - val_precision_m: 0.6055 - val_recall_m: 0.5462\n",
            "Epoch 705/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2184 - acc: 0.9087 - f1_m: 0.9215 - precision_m: 0.9665 - recall_m: 0.8808 - val_loss: 4.0404 - val_acc: 0.5721 - val_f1_m: 0.5675 - val_precision_m: 0.5995 - val_recall_m: 0.5390\n",
            "Epoch 706/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.2194 - acc: 0.9089 - f1_m: 0.9218 - precision_m: 0.9666 - recall_m: 0.8812 - val_loss: 4.0619 - val_acc: 0.5734 - val_f1_m: 0.5702 - val_precision_m: 0.6021 - val_recall_m: 0.5417\n",
            "Epoch 707/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2239 - acc: 0.9084 - f1_m: 0.9211 - precision_m: 0.9655 - recall_m: 0.8809 - val_loss: 4.1273 - val_acc: 0.5779 - val_f1_m: 0.5754 - val_precision_m: 0.6049 - val_recall_m: 0.5489\n",
            "Epoch 708/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2209 - acc: 0.9091 - f1_m: 0.9218 - precision_m: 0.9660 - recall_m: 0.8818 - val_loss: 4.0936 - val_acc: 0.5754 - val_f1_m: 0.5723 - val_precision_m: 0.6024 - val_recall_m: 0.5453\n",
            "Epoch 709/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2183 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9657 - recall_m: 0.8807 - val_loss: 4.0886 - val_acc: 0.5720 - val_f1_m: 0.5683 - val_precision_m: 0.5988 - val_recall_m: 0.5409\n",
            "Epoch 710/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2177 - acc: 0.9081 - f1_m: 0.9211 - precision_m: 0.9661 - recall_m: 0.8805 - val_loss: 4.0366 - val_acc: 0.5742 - val_f1_m: 0.5708 - val_precision_m: 0.6023 - val_recall_m: 0.5427\n",
            "Epoch 711/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2209 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9661 - recall_m: 0.8808 - val_loss: 4.0648 - val_acc: 0.5746 - val_f1_m: 0.5711 - val_precision_m: 0.6034 - val_recall_m: 0.5423\n",
            "Epoch 712/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2248 - acc: 0.9077 - f1_m: 0.9204 - precision_m: 0.9651 - recall_m: 0.8800 - val_loss: 3.9724 - val_acc: 0.5768 - val_f1_m: 0.5727 - val_precision_m: 0.6047 - val_recall_m: 0.5441\n",
            "Epoch 713/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2192 - acc: 0.9084 - f1_m: 0.9211 - precision_m: 0.9665 - recall_m: 0.8802 - val_loss: 4.0917 - val_acc: 0.5743 - val_f1_m: 0.5706 - val_precision_m: 0.6018 - val_recall_m: 0.5427\n",
            "Epoch 714/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2247 - acc: 0.9080 - f1_m: 0.9207 - precision_m: 0.9652 - recall_m: 0.8804 - val_loss: 4.1126 - val_acc: 0.5746 - val_f1_m: 0.5711 - val_precision_m: 0.6016 - val_recall_m: 0.5437\n",
            "Epoch 715/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2368 - acc: 0.9085 - f1_m: 0.9211 - precision_m: 0.9650 - recall_m: 0.8813 - val_loss: 4.0788 - val_acc: 0.5802 - val_f1_m: 0.5777 - val_precision_m: 0.6080 - val_recall_m: 0.5505\n",
            "Epoch 716/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2331 - acc: 0.9076 - f1_m: 0.9197 - precision_m: 0.9643 - recall_m: 0.8794 - val_loss: 4.0926 - val_acc: 0.5742 - val_f1_m: 0.5714 - val_precision_m: 0.6019 - val_recall_m: 0.5439\n",
            "Epoch 717/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2231 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9656 - recall_m: 0.8814 - val_loss: 4.1475 - val_acc: 0.5749 - val_f1_m: 0.5712 - val_precision_m: 0.6024 - val_recall_m: 0.5433\n",
            "Epoch 718/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2226 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9659 - recall_m: 0.8809 - val_loss: 4.1268 - val_acc: 0.5742 - val_f1_m: 0.5703 - val_precision_m: 0.6015 - val_recall_m: 0.5424\n",
            "Epoch 719/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2227 - acc: 0.9085 - f1_m: 0.9213 - precision_m: 0.9661 - recall_m: 0.8807 - val_loss: 4.0461 - val_acc: 0.5797 - val_f1_m: 0.5763 - val_precision_m: 0.6078 - val_recall_m: 0.5481\n",
            "Epoch 720/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2239 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9658 - recall_m: 0.8811 - val_loss: 3.9576 - val_acc: 0.5807 - val_f1_m: 0.5774 - val_precision_m: 0.6100 - val_recall_m: 0.5483\n",
            "Epoch 721/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.2144 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8819 - val_loss: 4.0494 - val_acc: 0.5804 - val_f1_m: 0.5775 - val_precision_m: 0.6079 - val_recall_m: 0.5503\n",
            "Epoch 722/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2222 - acc: 0.9089 - f1_m: 0.9216 - precision_m: 0.9661 - recall_m: 0.8813 - val_loss: 3.9477 - val_acc: 0.5789 - val_f1_m: 0.5756 - val_precision_m: 0.6074 - val_recall_m: 0.5471\n",
            "Epoch 723/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2192 - acc: 0.9093 - f1_m: 0.9221 - precision_m: 0.9667 - recall_m: 0.8818 - val_loss: 4.0185 - val_acc: 0.5841 - val_f1_m: 0.5811 - val_precision_m: 0.6121 - val_recall_m: 0.5533\n",
            "Epoch 724/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2576 - acc: 0.9088 - f1_m: 0.9207 - precision_m: 0.9634 - recall_m: 0.8820 - val_loss: 3.9394 - val_acc: 0.5787 - val_f1_m: 0.5750 - val_precision_m: 0.6067 - val_recall_m: 0.5466\n",
            "Epoch 725/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2288 - acc: 0.9079 - f1_m: 0.9205 - precision_m: 0.9650 - recall_m: 0.8802 - val_loss: 4.1463 - val_acc: 0.5783 - val_f1_m: 0.5756 - val_precision_m: 0.6059 - val_recall_m: 0.5483\n",
            "Epoch 726/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2168 - acc: 0.9085 - f1_m: 0.9213 - precision_m: 0.9662 - recall_m: 0.8807 - val_loss: 4.0698 - val_acc: 0.5728 - val_f1_m: 0.5694 - val_precision_m: 0.6005 - val_recall_m: 0.5415\n",
            "Epoch 727/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2192 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9663 - recall_m: 0.8804 - val_loss: 4.0117 - val_acc: 0.5781 - val_f1_m: 0.5752 - val_precision_m: 0.6066 - val_recall_m: 0.5471\n",
            "Epoch 728/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2237 - acc: 0.9079 - f1_m: 0.9208 - precision_m: 0.9658 - recall_m: 0.8801 - val_loss: 4.0079 - val_acc: 0.5802 - val_f1_m: 0.5768 - val_precision_m: 0.6081 - val_recall_m: 0.5487\n",
            "Epoch 729/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2273 - acc: 0.9085 - f1_m: 0.9210 - precision_m: 0.9654 - recall_m: 0.8808 - val_loss: 4.0852 - val_acc: 0.5789 - val_f1_m: 0.5758 - val_precision_m: 0.6074 - val_recall_m: 0.5476\n",
            "Epoch 730/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2268 - acc: 0.9084 - f1_m: 0.9213 - precision_m: 0.9656 - recall_m: 0.8811 - val_loss: 4.0037 - val_acc: 0.5785 - val_f1_m: 0.5753 - val_precision_m: 0.6072 - val_recall_m: 0.5467\n",
            "Epoch 731/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.2269 - acc: 0.9077 - f1_m: 0.9201 - precision_m: 0.9650 - recall_m: 0.8796 - val_loss: 3.9715 - val_acc: 0.5752 - val_f1_m: 0.5709 - val_precision_m: 0.6035 - val_recall_m: 0.5419\n",
            "Epoch 732/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2205 - acc: 0.9085 - f1_m: 0.9212 - precision_m: 0.9661 - recall_m: 0.8806 - val_loss: 4.0376 - val_acc: 0.5765 - val_f1_m: 0.5726 - val_precision_m: 0.6041 - val_recall_m: 0.5444\n",
            "Epoch 733/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2157 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9666 - recall_m: 0.8815 - val_loss: 4.0456 - val_acc: 0.5722 - val_f1_m: 0.5680 - val_precision_m: 0.5991 - val_recall_m: 0.5402\n",
            "Epoch 734/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2110 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9672 - recall_m: 0.8812 - val_loss: 4.0473 - val_acc: 0.5801 - val_f1_m: 0.5771 - val_precision_m: 0.6091 - val_recall_m: 0.5485\n",
            "Epoch 735/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2137 - acc: 0.9081 - f1_m: 0.9212 - precision_m: 0.9665 - recall_m: 0.8803 - val_loss: 4.0110 - val_acc: 0.5759 - val_f1_m: 0.5716 - val_precision_m: 0.6037 - val_recall_m: 0.5430\n",
            "Epoch 736/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2109 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9669 - recall_m: 0.8815 - val_loss: 4.1279 - val_acc: 0.5752 - val_f1_m: 0.5718 - val_precision_m: 0.6025 - val_recall_m: 0.5443\n",
            "Epoch 737/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.2167 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9663 - recall_m: 0.8817 - val_loss: 4.0357 - val_acc: 0.5748 - val_f1_m: 0.5720 - val_precision_m: 0.6028 - val_recall_m: 0.5443\n",
            "Epoch 738/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2125 - acc: 0.9089 - f1_m: 0.9217 - precision_m: 0.9663 - recall_m: 0.8813 - val_loss: 4.0436 - val_acc: 0.5747 - val_f1_m: 0.5713 - val_precision_m: 0.6020 - val_recall_m: 0.5437\n",
            "Epoch 739/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2128 - acc: 0.9083 - f1_m: 0.9213 - precision_m: 0.9664 - recall_m: 0.8806 - val_loss: 3.9603 - val_acc: 0.5695 - val_f1_m: 0.5665 - val_precision_m: 0.5997 - val_recall_m: 0.5371\n",
            "Epoch 740/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2119 - acc: 0.9089 - f1_m: 0.9218 - precision_m: 0.9669 - recall_m: 0.8810 - val_loss: 4.0638 - val_acc: 0.5702 - val_f1_m: 0.5672 - val_precision_m: 0.5986 - val_recall_m: 0.5390\n",
            "Epoch 741/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2118 - acc: 0.9082 - f1_m: 0.9213 - precision_m: 0.9663 - recall_m: 0.8805 - val_loss: 4.0630 - val_acc: 0.5783 - val_f1_m: 0.5756 - val_precision_m: 0.6069 - val_recall_m: 0.5476\n",
            "Epoch 742/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2130 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9664 - recall_m: 0.8808 - val_loss: 4.1023 - val_acc: 0.5761 - val_f1_m: 0.5730 - val_precision_m: 0.6042 - val_recall_m: 0.5451\n",
            "Epoch 743/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2130 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9669 - recall_m: 0.8812 - val_loss: 4.0743 - val_acc: 0.5746 - val_f1_m: 0.5709 - val_precision_m: 0.6027 - val_recall_m: 0.5424\n",
            "Epoch 744/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2135 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9672 - recall_m: 0.8811 - val_loss: 3.9699 - val_acc: 0.5760 - val_f1_m: 0.5729 - val_precision_m: 0.6056 - val_recall_m: 0.5438\n",
            "Epoch 745/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2107 - acc: 0.9089 - f1_m: 0.9221 - precision_m: 0.9678 - recall_m: 0.8808 - val_loss: 3.9727 - val_acc: 0.5796 - val_f1_m: 0.5764 - val_precision_m: 0.6086 - val_recall_m: 0.5476\n",
            "Epoch 746/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2141 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9666 - recall_m: 0.8808 - val_loss: 4.1018 - val_acc: 0.5766 - val_f1_m: 0.5745 - val_precision_m: 0.6049 - val_recall_m: 0.5472\n",
            "Epoch 747/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2104 - acc: 0.9092 - f1_m: 0.9222 - precision_m: 0.9675 - recall_m: 0.8813 - val_loss: 4.0505 - val_acc: 0.5801 - val_f1_m: 0.5776 - val_precision_m: 0.6084 - val_recall_m: 0.5499\n",
            "Epoch 748/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2119 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9662 - recall_m: 0.8810 - val_loss: 3.9649 - val_acc: 0.5779 - val_f1_m: 0.5746 - val_precision_m: 0.6074 - val_recall_m: 0.5454\n",
            "Epoch 749/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2141 - acc: 0.9080 - f1_m: 0.9207 - precision_m: 0.9657 - recall_m: 0.8801 - val_loss: 4.1322 - val_acc: 0.5732 - val_f1_m: 0.5696 - val_precision_m: 0.6010 - val_recall_m: 0.5416\n",
            "Epoch 750/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2333 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9650 - recall_m: 0.8813 - val_loss: 4.0637 - val_acc: 0.5819 - val_f1_m: 0.5788 - val_precision_m: 0.6102 - val_recall_m: 0.5506\n",
            "Epoch 751/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2292 - acc: 0.9096 - f1_m: 0.9223 - precision_m: 0.9663 - recall_m: 0.8825 - val_loss: 4.1516 - val_acc: 0.5786 - val_f1_m: 0.5760 - val_precision_m: 0.6065 - val_recall_m: 0.5487\n",
            "Epoch 752/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2199 - acc: 0.9088 - f1_m: 0.9215 - precision_m: 0.9662 - recall_m: 0.8811 - val_loss: 4.0771 - val_acc: 0.5811 - val_f1_m: 0.5779 - val_precision_m: 0.6090 - val_recall_m: 0.5501\n",
            "Epoch 753/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2204 - acc: 0.9090 - f1_m: 0.9218 - precision_m: 0.9663 - recall_m: 0.8816 - val_loss: 3.9855 - val_acc: 0.5775 - val_f1_m: 0.5740 - val_precision_m: 0.6059 - val_recall_m: 0.5455\n",
            "Epoch 754/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2139 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8820 - val_loss: 4.0139 - val_acc: 0.5731 - val_f1_m: 0.5694 - val_precision_m: 0.6017 - val_recall_m: 0.5407\n",
            "Epoch 755/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2105 - acc: 0.9102 - f1_m: 0.9232 - precision_m: 0.9680 - recall_m: 0.8826 - val_loss: 4.0063 - val_acc: 0.5768 - val_f1_m: 0.5739 - val_precision_m: 0.6058 - val_recall_m: 0.5453\n",
            "Epoch 756/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2114 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9663 - recall_m: 0.8808 - val_loss: 3.9214 - val_acc: 0.5803 - val_f1_m: 0.5771 - val_precision_m: 0.6101 - val_recall_m: 0.5478\n",
            "Epoch 757/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2090 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9665 - recall_m: 0.8810 - val_loss: 4.1008 - val_acc: 0.5728 - val_f1_m: 0.5695 - val_precision_m: 0.6009 - val_recall_m: 0.5413\n",
            "Epoch 758/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2071 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9672 - recall_m: 0.8810 - val_loss: 4.0589 - val_acc: 0.5749 - val_f1_m: 0.5716 - val_precision_m: 0.6037 - val_recall_m: 0.5430\n",
            "Epoch 759/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2068 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9666 - recall_m: 0.8814 - val_loss: 4.0920 - val_acc: 0.5797 - val_f1_m: 0.5767 - val_precision_m: 0.6081 - val_recall_m: 0.5486\n",
            "Epoch 760/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2049 - acc: 0.9095 - f1_m: 0.9225 - precision_m: 0.9676 - recall_m: 0.8818 - val_loss: 4.1656 - val_acc: 0.5761 - val_f1_m: 0.5733 - val_precision_m: 0.6037 - val_recall_m: 0.5461\n",
            "Epoch 761/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2051 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9669 - recall_m: 0.8812 - val_loss: 4.1391 - val_acc: 0.5763 - val_f1_m: 0.5729 - val_precision_m: 0.6039 - val_recall_m: 0.5451\n",
            "Epoch 762/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2082 - acc: 0.9096 - f1_m: 0.9226 - precision_m: 0.9675 - recall_m: 0.8820 - val_loss: 4.0946 - val_acc: 0.5754 - val_f1_m: 0.5718 - val_precision_m: 0.6032 - val_recall_m: 0.5437\n",
            "Epoch 763/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2072 - acc: 0.9088 - f1_m: 0.9220 - precision_m: 0.9671 - recall_m: 0.8812 - val_loss: 4.0564 - val_acc: 0.5791 - val_f1_m: 0.5758 - val_precision_m: 0.6076 - val_recall_m: 0.5474\n",
            "Epoch 764/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2079 - acc: 0.9091 - f1_m: 0.9222 - precision_m: 0.9672 - recall_m: 0.8816 - val_loss: 4.1268 - val_acc: 0.5759 - val_f1_m: 0.5730 - val_precision_m: 0.6045 - val_recall_m: 0.5449\n",
            "Epoch 765/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2120 - acc: 0.9080 - f1_m: 0.9209 - precision_m: 0.9665 - recall_m: 0.8798 - val_loss: 4.0788 - val_acc: 0.5747 - val_f1_m: 0.5720 - val_precision_m: 0.6033 - val_recall_m: 0.5439\n",
            "Epoch 766/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2122 - acc: 0.9093 - f1_m: 0.9222 - precision_m: 0.9675 - recall_m: 0.8813 - val_loss: 4.1491 - val_acc: 0.5721 - val_f1_m: 0.5685 - val_precision_m: 0.6007 - val_recall_m: 0.5399\n",
            "Epoch 767/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2090 - acc: 0.9097 - f1_m: 0.9225 - precision_m: 0.9677 - recall_m: 0.8817 - val_loss: 4.1991 - val_acc: 0.5710 - val_f1_m: 0.5682 - val_precision_m: 0.5997 - val_recall_m: 0.5400\n",
            "Epoch 768/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2073 - acc: 0.9092 - f1_m: 0.9222 - precision_m: 0.9673 - recall_m: 0.8814 - val_loss: 4.0580 - val_acc: 0.5716 - val_f1_m: 0.5672 - val_precision_m: 0.5991 - val_recall_m: 0.5388\n",
            "Epoch 769/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2131 - acc: 0.9084 - f1_m: 0.9214 - precision_m: 0.9664 - recall_m: 0.8808 - val_loss: 4.1249 - val_acc: 0.5750 - val_f1_m: 0.5711 - val_precision_m: 0.6026 - val_recall_m: 0.5429\n",
            "Epoch 770/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2075 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8813 - val_loss: 4.0904 - val_acc: 0.5755 - val_f1_m: 0.5713 - val_precision_m: 0.6031 - val_recall_m: 0.5429\n",
            "Epoch 771/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2081 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9670 - recall_m: 0.8805 - val_loss: 4.0550 - val_acc: 0.5757 - val_f1_m: 0.5724 - val_precision_m: 0.6043 - val_recall_m: 0.5439\n",
            "Epoch 772/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2174 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9666 - recall_m: 0.8814 - val_loss: 4.0427 - val_acc: 0.5782 - val_f1_m: 0.5742 - val_precision_m: 0.6063 - val_recall_m: 0.5455\n",
            "Epoch 773/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2183 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9664 - recall_m: 0.8811 - val_loss: 4.0761 - val_acc: 0.5759 - val_f1_m: 0.5728 - val_precision_m: 0.6043 - val_recall_m: 0.5446\n",
            "Epoch 774/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2149 - acc: 0.9086 - f1_m: 0.9216 - precision_m: 0.9666 - recall_m: 0.8808 - val_loss: 4.0994 - val_acc: 0.5768 - val_f1_m: 0.5733 - val_precision_m: 0.6047 - val_recall_m: 0.5452\n",
            "Epoch 775/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2125 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9666 - recall_m: 0.8807 - val_loss: 4.1542 - val_acc: 0.5783 - val_f1_m: 0.5750 - val_precision_m: 0.6062 - val_recall_m: 0.5471\n",
            "Epoch 776/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2076 - acc: 0.9100 - f1_m: 0.9231 - precision_m: 0.9680 - recall_m: 0.8824 - val_loss: 4.0386 - val_acc: 0.5800 - val_f1_m: 0.5764 - val_precision_m: 0.6079 - val_recall_m: 0.5482\n",
            "Epoch 777/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2072 - acc: 0.9098 - f1_m: 0.9228 - precision_m: 0.9679 - recall_m: 0.8820 - val_loss: 4.1193 - val_acc: 0.5775 - val_f1_m: 0.5738 - val_precision_m: 0.6055 - val_recall_m: 0.5455\n",
            "Epoch 778/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2111 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9664 - recall_m: 0.8809 - val_loss: 4.0477 - val_acc: 0.5745 - val_f1_m: 0.5702 - val_precision_m: 0.6022 - val_recall_m: 0.5416\n",
            "Epoch 779/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2082 - acc: 0.9096 - f1_m: 0.9225 - precision_m: 0.9676 - recall_m: 0.8817 - val_loss: 4.1546 - val_acc: 0.5763 - val_f1_m: 0.5739 - val_precision_m: 0.6048 - val_recall_m: 0.5463\n",
            "Epoch 780/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2145 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9665 - recall_m: 0.8815 - val_loss: 4.0830 - val_acc: 0.5716 - val_f1_m: 0.5673 - val_precision_m: 0.5993 - val_recall_m: 0.5387\n",
            "Epoch 781/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2081 - acc: 0.9098 - f1_m: 0.9229 - precision_m: 0.9679 - recall_m: 0.8822 - val_loss: 4.1141 - val_acc: 0.5746 - val_f1_m: 0.5709 - val_precision_m: 0.6025 - val_recall_m: 0.5427\n",
            "Epoch 782/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2105 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8818 - val_loss: 4.1104 - val_acc: 0.5746 - val_f1_m: 0.5710 - val_precision_m: 0.6027 - val_recall_m: 0.5426\n",
            "Epoch 783/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2089 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9666 - recall_m: 0.8816 - val_loss: 4.0510 - val_acc: 0.5774 - val_f1_m: 0.5747 - val_precision_m: 0.6057 - val_recall_m: 0.5469\n",
            "Epoch 784/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.2101 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9671 - recall_m: 0.8812 - val_loss: 4.0725 - val_acc: 0.5804 - val_f1_m: 0.5764 - val_precision_m: 0.6082 - val_recall_m: 0.5481\n",
            "Epoch 785/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2098 - acc: 0.9087 - f1_m: 0.9215 - precision_m: 0.9666 - recall_m: 0.8807 - val_loss: 4.0445 - val_acc: 0.5726 - val_f1_m: 0.5690 - val_precision_m: 0.6006 - val_recall_m: 0.5407\n",
            "Epoch 786/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2080 - acc: 0.9089 - f1_m: 0.9220 - precision_m: 0.9665 - recall_m: 0.8817 - val_loss: 4.1669 - val_acc: 0.5733 - val_f1_m: 0.5699 - val_precision_m: 0.6013 - val_recall_m: 0.5419\n",
            "Epoch 787/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2107 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8817 - val_loss: 4.0174 - val_acc: 0.5744 - val_f1_m: 0.5712 - val_precision_m: 0.6033 - val_recall_m: 0.5426\n",
            "Epoch 788/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2074 - acc: 0.9089 - f1_m: 0.9220 - precision_m: 0.9672 - recall_m: 0.8811 - val_loss: 4.1472 - val_acc: 0.5749 - val_f1_m: 0.5717 - val_precision_m: 0.6025 - val_recall_m: 0.5441\n",
            "Epoch 789/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2104 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8813 - val_loss: 4.1362 - val_acc: 0.5707 - val_f1_m: 0.5666 - val_precision_m: 0.5980 - val_recall_m: 0.5385\n",
            "Epoch 790/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2108 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9666 - recall_m: 0.8806 - val_loss: 4.1382 - val_acc: 0.5722 - val_f1_m: 0.5689 - val_precision_m: 0.5995 - val_recall_m: 0.5414\n",
            "Epoch 791/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2115 - acc: 0.9084 - f1_m: 0.9213 - precision_m: 0.9669 - recall_m: 0.8801 - val_loss: 4.1318 - val_acc: 0.5785 - val_f1_m: 0.5754 - val_precision_m: 0.6059 - val_recall_m: 0.5481\n",
            "Epoch 792/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2101 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9667 - recall_m: 0.8815 - val_loss: 4.0901 - val_acc: 0.5810 - val_f1_m: 0.5782 - val_precision_m: 0.6091 - val_recall_m: 0.5505\n",
            "Epoch 793/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2131 - acc: 0.9088 - f1_m: 0.9217 - precision_m: 0.9665 - recall_m: 0.8811 - val_loss: 4.1710 - val_acc: 0.5730 - val_f1_m: 0.5695 - val_precision_m: 0.6008 - val_recall_m: 0.5414\n",
            "Epoch 794/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2119 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9666 - recall_m: 0.8816 - val_loss: 4.1400 - val_acc: 0.5740 - val_f1_m: 0.5701 - val_precision_m: 0.6018 - val_recall_m: 0.5417\n",
            "Epoch 795/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2110 - acc: 0.9084 - f1_m: 0.9214 - precision_m: 0.9669 - recall_m: 0.8803 - val_loss: 4.0366 - val_acc: 0.5768 - val_f1_m: 0.5737 - val_precision_m: 0.6056 - val_recall_m: 0.5452\n",
            "Epoch 796/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2138 - acc: 0.9088 - f1_m: 0.9217 - precision_m: 0.9668 - recall_m: 0.8811 - val_loss: 3.9863 - val_acc: 0.5740 - val_f1_m: 0.5702 - val_precision_m: 0.6034 - val_recall_m: 0.5406\n",
            "Epoch 797/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2060 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9678 - recall_m: 0.8812 - val_loss: 4.0340 - val_acc: 0.5722 - val_f1_m: 0.5681 - val_precision_m: 0.6012 - val_recall_m: 0.5386\n",
            "Epoch 798/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2072 - acc: 0.9088 - f1_m: 0.9218 - precision_m: 0.9670 - recall_m: 0.8809 - val_loss: 4.0855 - val_acc: 0.5780 - val_f1_m: 0.5745 - val_precision_m: 0.6066 - val_recall_m: 0.5459\n",
            "Epoch 799/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2047 - acc: 0.9094 - f1_m: 0.9225 - precision_m: 0.9677 - recall_m: 0.8816 - val_loss: 4.0479 - val_acc: 0.5740 - val_f1_m: 0.5709 - val_precision_m: 0.6033 - val_recall_m: 0.5420\n",
            "Epoch 800/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2448 - acc: 0.9097 - f1_m: 0.9223 - precision_m: 0.9652 - recall_m: 0.8834 - val_loss: 4.6059 - val_acc: 0.5720 - val_f1_m: 0.5722 - val_precision_m: 0.5843 - val_recall_m: 0.5608\n",
            "Epoch 801/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2344 - acc: 0.9088 - f1_m: 0.9214 - precision_m: 0.9650 - recall_m: 0.8819 - val_loss: 3.9975 - val_acc: 0.5765 - val_f1_m: 0.5727 - val_precision_m: 0.6057 - val_recall_m: 0.5434\n",
            "Epoch 802/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2175 - acc: 0.9087 - f1_m: 0.9216 - precision_m: 0.9667 - recall_m: 0.8808 - val_loss: 4.1128 - val_acc: 0.5781 - val_f1_m: 0.5752 - val_precision_m: 0.6071 - val_recall_m: 0.5468\n",
            "Epoch 803/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2151 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8818 - val_loss: 4.1404 - val_acc: 0.5787 - val_f1_m: 0.5755 - val_precision_m: 0.6074 - val_recall_m: 0.5471\n",
            "Epoch 804/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2075 - acc: 0.9099 - f1_m: 0.9230 - precision_m: 0.9681 - recall_m: 0.8823 - val_loss: 4.1167 - val_acc: 0.5775 - val_f1_m: 0.5745 - val_precision_m: 0.6060 - val_recall_m: 0.5463\n",
            "Epoch 805/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2106 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9678 - recall_m: 0.8815 - val_loss: 4.0150 - val_acc: 0.5765 - val_f1_m: 0.5727 - val_precision_m: 0.6053 - val_recall_m: 0.5436\n",
            "Epoch 806/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2081 - acc: 0.9094 - f1_m: 0.9222 - precision_m: 0.9673 - recall_m: 0.8815 - val_loss: 4.1312 - val_acc: 0.5786 - val_f1_m: 0.5761 - val_precision_m: 0.6084 - val_recall_m: 0.5472\n",
            "Epoch 807/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2100 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9668 - recall_m: 0.8814 - val_loss: 4.0150 - val_acc: 0.5734 - val_f1_m: 0.5701 - val_precision_m: 0.6025 - val_recall_m: 0.5412\n",
            "Epoch 808/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2107 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9667 - recall_m: 0.8815 - val_loss: 4.0037 - val_acc: 0.5755 - val_f1_m: 0.5719 - val_precision_m: 0.6047 - val_recall_m: 0.5427\n",
            "Epoch 809/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2086 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9674 - recall_m: 0.8813 - val_loss: 4.1645 - val_acc: 0.5739 - val_f1_m: 0.5709 - val_precision_m: 0.6025 - val_recall_m: 0.5427\n",
            "Epoch 810/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2094 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9674 - recall_m: 0.8815 - val_loss: 4.1324 - val_acc: 0.5667 - val_f1_m: 0.5633 - val_precision_m: 0.5953 - val_recall_m: 0.5348\n",
            "Epoch 811/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2108 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9670 - recall_m: 0.8812 - val_loss: 4.0128 - val_acc: 0.5797 - val_f1_m: 0.5774 - val_precision_m: 0.6092 - val_recall_m: 0.5489\n",
            "Epoch 812/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2137 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9666 - recall_m: 0.8809 - val_loss: 3.9607 - val_acc: 0.5721 - val_f1_m: 0.5692 - val_precision_m: 0.6022 - val_recall_m: 0.5398\n",
            "Epoch 813/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2118 - acc: 0.9080 - f1_m: 0.9210 - precision_m: 0.9660 - recall_m: 0.8803 - val_loss: 4.0801 - val_acc: 0.5787 - val_f1_m: 0.5757 - val_precision_m: 0.6077 - val_recall_m: 0.5471\n",
            "Epoch 814/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2104 - acc: 0.9097 - f1_m: 0.9225 - precision_m: 0.9676 - recall_m: 0.8818 - val_loss: 4.0733 - val_acc: 0.5785 - val_f1_m: 0.5758 - val_precision_m: 0.6082 - val_recall_m: 0.5470\n",
            "Epoch 815/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2094 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8818 - val_loss: 4.0997 - val_acc: 0.5757 - val_f1_m: 0.5717 - val_precision_m: 0.6039 - val_recall_m: 0.5430\n",
            "Epoch 816/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2136 - acc: 0.9089 - f1_m: 0.9216 - precision_m: 0.9663 - recall_m: 0.8812 - val_loss: 3.9850 - val_acc: 0.5794 - val_f1_m: 0.5762 - val_precision_m: 0.6087 - val_recall_m: 0.5473\n",
            "Epoch 817/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2113 - acc: 0.9089 - f1_m: 0.9217 - precision_m: 0.9668 - recall_m: 0.8809 - val_loss: 4.2056 - val_acc: 0.5656 - val_f1_m: 0.5614 - val_precision_m: 0.5925 - val_recall_m: 0.5336\n",
            "Epoch 818/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2185 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9664 - recall_m: 0.8805 - val_loss: 4.0737 - val_acc: 0.5750 - val_f1_m: 0.5705 - val_precision_m: 0.6028 - val_recall_m: 0.5417\n",
            "Epoch 819/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2201 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9671 - recall_m: 0.8815 - val_loss: 4.1362 - val_acc: 0.5768 - val_f1_m: 0.5736 - val_precision_m: 0.6050 - val_recall_m: 0.5455\n",
            "Epoch 820/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2185 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9670 - recall_m: 0.8819 - val_loss: 4.0546 - val_acc: 0.5732 - val_f1_m: 0.5694 - val_precision_m: 0.6009 - val_recall_m: 0.5412\n",
            "Epoch 821/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2154 - acc: 0.9082 - f1_m: 0.9210 - precision_m: 0.9661 - recall_m: 0.8804 - val_loss: 4.1727 - val_acc: 0.5729 - val_f1_m: 0.5696 - val_precision_m: 0.6001 - val_recall_m: 0.5422\n",
            "Epoch 822/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2136 - acc: 0.9090 - f1_m: 0.9221 - precision_m: 0.9672 - recall_m: 0.8813 - val_loss: 4.0562 - val_acc: 0.5752 - val_f1_m: 0.5716 - val_precision_m: 0.6039 - val_recall_m: 0.5427\n",
            "Epoch 823/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2142 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8813 - val_loss: 4.0895 - val_acc: 0.5807 - val_f1_m: 0.5779 - val_precision_m: 0.6090 - val_recall_m: 0.5501\n",
            "Epoch 824/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2167 - acc: 0.9095 - f1_m: 0.9223 - precision_m: 0.9670 - recall_m: 0.8819 - val_loss: 3.9815 - val_acc: 0.5770 - val_f1_m: 0.5730 - val_precision_m: 0.6052 - val_recall_m: 0.5443\n",
            "Epoch 825/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2160 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9666 - recall_m: 0.8815 - val_loss: 4.0863 - val_acc: 0.5778 - val_f1_m: 0.5745 - val_precision_m: 0.6060 - val_recall_m: 0.5464\n",
            "Epoch 826/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2143 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9670 - recall_m: 0.8814 - val_loss: 4.1450 - val_acc: 0.5744 - val_f1_m: 0.5709 - val_precision_m: 0.6017 - val_recall_m: 0.5433\n",
            "Epoch 827/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2137 - acc: 0.9086 - f1_m: 0.9213 - precision_m: 0.9664 - recall_m: 0.8806 - val_loss: 4.1796 - val_acc: 0.5692 - val_f1_m: 0.5660 - val_precision_m: 0.5968 - val_recall_m: 0.5385\n",
            "Epoch 828/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2736 - acc: 0.9077 - f1_m: 0.9197 - precision_m: 0.9624 - recall_m: 0.8809 - val_loss: 4.0549 - val_acc: 0.5759 - val_f1_m: 0.5727 - val_precision_m: 0.6045 - val_recall_m: 0.5442\n",
            "Epoch 829/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2533 - acc: 0.9088 - f1_m: 0.9210 - precision_m: 0.9642 - recall_m: 0.8818 - val_loss: 4.1625 - val_acc: 0.5767 - val_f1_m: 0.5731 - val_precision_m: 0.6047 - val_recall_m: 0.5449\n",
            "Epoch 830/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2246 - acc: 0.9103 - f1_m: 0.9229 - precision_m: 0.9674 - recall_m: 0.8827 - val_loss: 4.2304 - val_acc: 0.5770 - val_f1_m: 0.5747 - val_precision_m: 0.6053 - val_recall_m: 0.5472\n",
            "Epoch 831/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2169 - acc: 0.9091 - f1_m: 0.9219 - precision_m: 0.9668 - recall_m: 0.8814 - val_loss: 4.1753 - val_acc: 0.5737 - val_f1_m: 0.5701 - val_precision_m: 0.6013 - val_recall_m: 0.5421\n",
            "Epoch 832/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2190 - acc: 0.9090 - f1_m: 0.9218 - precision_m: 0.9667 - recall_m: 0.8813 - val_loss: 4.1373 - val_acc: 0.5798 - val_f1_m: 0.5769 - val_precision_m: 0.6080 - val_recall_m: 0.5491\n",
            "Epoch 833/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2121 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9669 - recall_m: 0.8813 - val_loss: 4.2040 - val_acc: 0.5763 - val_f1_m: 0.5729 - val_precision_m: 0.6037 - val_recall_m: 0.5452\n",
            "Epoch 834/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2106 - acc: 0.9089 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8812 - val_loss: 4.1669 - val_acc: 0.5712 - val_f1_m: 0.5674 - val_precision_m: 0.5989 - val_recall_m: 0.5392\n",
            "Epoch 835/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2173 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9666 - recall_m: 0.8817 - val_loss: 4.0516 - val_acc: 0.5748 - val_f1_m: 0.5707 - val_precision_m: 0.6034 - val_recall_m: 0.5416\n",
            "Epoch 836/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2123 - acc: 0.9095 - f1_m: 0.9221 - precision_m: 0.9670 - recall_m: 0.8815 - val_loss: 4.0987 - val_acc: 0.5772 - val_f1_m: 0.5741 - val_precision_m: 0.6055 - val_recall_m: 0.5460\n",
            "Epoch 837/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2174 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9669 - recall_m: 0.8820 - val_loss: 4.1370 - val_acc: 0.5781 - val_f1_m: 0.5747 - val_precision_m: 0.6068 - val_recall_m: 0.5461\n",
            "Epoch 838/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2120 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9666 - recall_m: 0.8809 - val_loss: 4.2261 - val_acc: 0.5797 - val_f1_m: 0.5771 - val_precision_m: 0.6079 - val_recall_m: 0.5494\n",
            "Epoch 839/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2122 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9669 - recall_m: 0.8816 - val_loss: 4.1537 - val_acc: 0.5678 - val_f1_m: 0.5641 - val_precision_m: 0.5958 - val_recall_m: 0.5358\n",
            "Epoch 840/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2113 - acc: 0.9097 - f1_m: 0.9227 - precision_m: 0.9678 - recall_m: 0.8820 - val_loss: 4.0812 - val_acc: 0.5736 - val_f1_m: 0.5703 - val_precision_m: 0.6023 - val_recall_m: 0.5417\n",
            "Epoch 841/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2138 - acc: 0.9090 - f1_m: 0.9221 - precision_m: 0.9673 - recall_m: 0.8812 - val_loss: 4.1570 - val_acc: 0.5725 - val_f1_m: 0.5688 - val_precision_m: 0.6008 - val_recall_m: 0.5404\n",
            "Epoch 842/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2126 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9668 - recall_m: 0.8806 - val_loss: 4.0634 - val_acc: 0.5789 - val_f1_m: 0.5755 - val_precision_m: 0.6078 - val_recall_m: 0.5467\n",
            "Epoch 843/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2157 - acc: 0.9086 - f1_m: 0.9216 - precision_m: 0.9667 - recall_m: 0.8809 - val_loss: 4.1331 - val_acc: 0.5775 - val_f1_m: 0.5744 - val_precision_m: 0.6063 - val_recall_m: 0.5458\n",
            "Epoch 844/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2130 - acc: 0.9083 - f1_m: 0.9211 - precision_m: 0.9664 - recall_m: 0.8802 - val_loss: 4.0562 - val_acc: 0.5738 - val_f1_m: 0.5700 - val_precision_m: 0.6022 - val_recall_m: 0.5413\n",
            "Epoch 845/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2095 - acc: 0.9093 - f1_m: 0.9221 - precision_m: 0.9672 - recall_m: 0.8814 - val_loss: 4.0515 - val_acc: 0.5722 - val_f1_m: 0.5683 - val_precision_m: 0.6002 - val_recall_m: 0.5398\n",
            "Epoch 846/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2079 - acc: 0.9098 - f1_m: 0.9226 - precision_m: 0.9678 - recall_m: 0.8818 - val_loss: 4.0849 - val_acc: 0.5734 - val_f1_m: 0.5697 - val_precision_m: 0.6021 - val_recall_m: 0.5408\n",
            "Epoch 847/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2080 - acc: 0.9101 - f1_m: 0.9232 - precision_m: 0.9682 - recall_m: 0.8824 - val_loss: 4.0083 - val_acc: 0.5781 - val_f1_m: 0.5750 - val_precision_m: 0.6073 - val_recall_m: 0.5462\n",
            "Epoch 848/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2101 - acc: 0.9095 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8815 - val_loss: 3.9980 - val_acc: 0.5802 - val_f1_m: 0.5775 - val_precision_m: 0.6103 - val_recall_m: 0.5483\n",
            "Epoch 849/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2115 - acc: 0.9078 - f1_m: 0.9208 - precision_m: 0.9660 - recall_m: 0.8800 - val_loss: 4.0894 - val_acc: 0.5768 - val_f1_m: 0.5729 - val_precision_m: 0.6053 - val_recall_m: 0.5440\n",
            "Epoch 850/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2112 - acc: 0.9089 - f1_m: 0.9220 - precision_m: 0.9672 - recall_m: 0.8811 - val_loss: 4.0942 - val_acc: 0.5730 - val_f1_m: 0.5691 - val_precision_m: 0.6009 - val_recall_m: 0.5406\n",
            "Epoch 851/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2089 - acc: 0.9093 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8816 - val_loss: 3.9965 - val_acc: 0.5769 - val_f1_m: 0.5729 - val_precision_m: 0.6052 - val_recall_m: 0.5441\n",
            "Epoch 852/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2110 - acc: 0.9091 - f1_m: 0.9220 - precision_m: 0.9672 - recall_m: 0.8812 - val_loss: 4.1643 - val_acc: 0.5778 - val_f1_m: 0.5741 - val_precision_m: 0.6060 - val_recall_m: 0.5457\n",
            "Epoch 853/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2088 - acc: 0.9090 - f1_m: 0.9221 - precision_m: 0.9676 - recall_m: 0.8811 - val_loss: 4.1356 - val_acc: 0.5752 - val_f1_m: 0.5717 - val_precision_m: 0.6033 - val_recall_m: 0.5435\n",
            "Epoch 854/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2086 - acc: 0.9094 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8815 - val_loss: 4.0538 - val_acc: 0.5705 - val_f1_m: 0.5661 - val_precision_m: 0.5988 - val_recall_m: 0.5370\n",
            "Epoch 855/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2105 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9674 - recall_m: 0.8816 - val_loss: 4.1701 - val_acc: 0.5721 - val_f1_m: 0.5684 - val_precision_m: 0.6001 - val_recall_m: 0.5400\n",
            "Epoch 856/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2102 - acc: 0.9093 - f1_m: 0.9224 - precision_m: 0.9676 - recall_m: 0.8815 - val_loss: 4.0540 - val_acc: 0.5779 - val_f1_m: 0.5740 - val_precision_m: 0.6064 - val_recall_m: 0.5451\n",
            "Epoch 857/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2106 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9667 - recall_m: 0.8806 - val_loss: 4.1218 - val_acc: 0.5752 - val_f1_m: 0.5705 - val_precision_m: 0.6023 - val_recall_m: 0.5422\n",
            "Epoch 858/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2102 - acc: 0.9092 - f1_m: 0.9223 - precision_m: 0.9675 - recall_m: 0.8815 - val_loss: 3.9616 - val_acc: 0.5740 - val_f1_m: 0.5698 - val_precision_m: 0.6023 - val_recall_m: 0.5408\n",
            "Epoch 859/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2026 - acc: 0.9100 - f1_m: 0.9231 - precision_m: 0.9682 - recall_m: 0.8823 - val_loss: 4.1725 - val_acc: 0.5766 - val_f1_m: 0.5727 - val_precision_m: 0.6045 - val_recall_m: 0.5444\n",
            "Epoch 860/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2093 - acc: 0.9087 - f1_m: 0.9217 - precision_m: 0.9671 - recall_m: 0.8807 - val_loss: 4.0679 - val_acc: 0.5736 - val_f1_m: 0.5704 - val_precision_m: 0.6022 - val_recall_m: 0.5421\n",
            "Epoch 861/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2084 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9671 - recall_m: 0.8811 - val_loss: 4.1331 - val_acc: 0.5777 - val_f1_m: 0.5746 - val_precision_m: 0.6054 - val_recall_m: 0.5470\n",
            "Epoch 862/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2096 - acc: 0.9095 - f1_m: 0.9226 - precision_m: 0.9679 - recall_m: 0.8816 - val_loss: 3.8504 - val_acc: 0.5802 - val_f1_m: 0.5760 - val_precision_m: 0.6097 - val_recall_m: 0.5461\n",
            "Epoch 863/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2084 - acc: 0.9096 - f1_m: 0.9227 - precision_m: 0.9683 - recall_m: 0.8814 - val_loss: 4.0664 - val_acc: 0.5818 - val_f1_m: 0.5788 - val_precision_m: 0.6103 - val_recall_m: 0.5506\n",
            "Epoch 864/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2098 - acc: 0.9091 - f1_m: 0.9221 - precision_m: 0.9675 - recall_m: 0.8812 - val_loss: 4.0510 - val_acc: 0.5797 - val_f1_m: 0.5762 - val_precision_m: 0.6083 - val_recall_m: 0.5474\n",
            "Epoch 865/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2108 - acc: 0.9084 - f1_m: 0.9215 - precision_m: 0.9664 - recall_m: 0.8809 - val_loss: 4.0849 - val_acc: 0.5782 - val_f1_m: 0.5750 - val_precision_m: 0.6061 - val_recall_m: 0.5471\n",
            "Epoch 866/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2115 - acc: 0.9092 - f1_m: 0.9222 - precision_m: 0.9672 - recall_m: 0.8816 - val_loss: 4.0859 - val_acc: 0.5809 - val_f1_m: 0.5778 - val_precision_m: 0.6089 - val_recall_m: 0.5499\n",
            "Epoch 867/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2078 - acc: 0.9092 - f1_m: 0.9223 - precision_m: 0.9672 - recall_m: 0.8817 - val_loss: 4.1234 - val_acc: 0.5769 - val_f1_m: 0.5739 - val_precision_m: 0.6047 - val_recall_m: 0.5463\n",
            "Epoch 868/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2109 - acc: 0.9084 - f1_m: 0.9214 - precision_m: 0.9666 - recall_m: 0.8807 - val_loss: 4.0551 - val_acc: 0.5762 - val_f1_m: 0.5727 - val_precision_m: 0.6045 - val_recall_m: 0.5443\n",
            "Epoch 869/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2047 - acc: 0.9100 - f1_m: 0.9230 - precision_m: 0.9684 - recall_m: 0.8820 - val_loss: 4.1536 - val_acc: 0.5821 - val_f1_m: 0.5795 - val_precision_m: 0.6102 - val_recall_m: 0.5519\n",
            "Epoch 870/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2098 - acc: 0.9087 - f1_m: 0.9219 - precision_m: 0.9671 - recall_m: 0.8810 - val_loss: 4.1906 - val_acc: 0.5748 - val_f1_m: 0.5715 - val_precision_m: 0.6031 - val_recall_m: 0.5433\n",
            "Epoch 871/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2145 - acc: 0.9082 - f1_m: 0.9211 - precision_m: 0.9662 - recall_m: 0.8803 - val_loss: 4.1377 - val_acc: 0.5762 - val_f1_m: 0.5729 - val_precision_m: 0.6040 - val_recall_m: 0.5451\n",
            "Epoch 872/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2118 - acc: 0.9089 - f1_m: 0.9217 - precision_m: 0.9664 - recall_m: 0.8813 - val_loss: 4.1450 - val_acc: 0.5742 - val_f1_m: 0.5705 - val_precision_m: 0.6023 - val_recall_m: 0.5421\n",
            "Epoch 873/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2061 - acc: 0.9088 - f1_m: 0.9219 - precision_m: 0.9671 - recall_m: 0.8811 - val_loss: 4.1764 - val_acc: 0.5745 - val_f1_m: 0.5710 - val_precision_m: 0.6019 - val_recall_m: 0.5433\n",
            "Epoch 874/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2136 - acc: 0.9091 - f1_m: 0.9223 - precision_m: 0.9669 - recall_m: 0.8819 - val_loss: 4.1750 - val_acc: 0.5803 - val_f1_m: 0.5777 - val_precision_m: 0.6081 - val_recall_m: 0.5504\n",
            "Epoch 875/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.2181 - acc: 0.9080 - f1_m: 0.9209 - precision_m: 0.9658 - recall_m: 0.8802 - val_loss: 4.1508 - val_acc: 0.5667 - val_f1_m: 0.5626 - val_precision_m: 0.5941 - val_recall_m: 0.5344\n",
            "Epoch 876/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2154 - acc: 0.9080 - f1_m: 0.9209 - precision_m: 0.9663 - recall_m: 0.8799 - val_loss: 4.1429 - val_acc: 0.5780 - val_f1_m: 0.5745 - val_precision_m: 0.6060 - val_recall_m: 0.5464\n",
            "Epoch 877/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2209 - acc: 0.9077 - f1_m: 0.9207 - precision_m: 0.9657 - recall_m: 0.8800 - val_loss: 4.1305 - val_acc: 0.5733 - val_f1_m: 0.5699 - val_precision_m: 0.6010 - val_recall_m: 0.5420\n",
            "Epoch 878/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2147 - acc: 0.9083 - f1_m: 0.9214 - precision_m: 0.9668 - recall_m: 0.8805 - val_loss: 4.1652 - val_acc: 0.5818 - val_f1_m: 0.5784 - val_precision_m: 0.6106 - val_recall_m: 0.5497\n",
            "Epoch 879/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2102 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9672 - recall_m: 0.8810 - val_loss: 4.1626 - val_acc: 0.5828 - val_f1_m: 0.5794 - val_precision_m: 0.6108 - val_recall_m: 0.5513\n",
            "Epoch 880/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2067 - acc: 0.9095 - f1_m: 0.9228 - precision_m: 0.9679 - recall_m: 0.8819 - val_loss: 4.1647 - val_acc: 0.5803 - val_f1_m: 0.5774 - val_precision_m: 0.6083 - val_recall_m: 0.5497\n",
            "Epoch 881/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2124 - acc: 0.9086 - f1_m: 0.9216 - precision_m: 0.9673 - recall_m: 0.8803 - val_loss: 4.0983 - val_acc: 0.5761 - val_f1_m: 0.5722 - val_precision_m: 0.6036 - val_recall_m: 0.5441\n",
            "Epoch 882/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2127 - acc: 0.9087 - f1_m: 0.9217 - precision_m: 0.9668 - recall_m: 0.8810 - val_loss: 4.0816 - val_acc: 0.5773 - val_f1_m: 0.5739 - val_precision_m: 0.6063 - val_recall_m: 0.5449\n",
            "Epoch 883/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2105 - acc: 0.9097 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8817 - val_loss: 4.1724 - val_acc: 0.5757 - val_f1_m: 0.5725 - val_precision_m: 0.6042 - val_recall_m: 0.5442\n",
            "Epoch 884/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2131 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9671 - recall_m: 0.8806 - val_loss: 4.1845 - val_acc: 0.5716 - val_f1_m: 0.5680 - val_precision_m: 0.5992 - val_recall_m: 0.5401\n",
            "Epoch 885/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2145 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9670 - recall_m: 0.8804 - val_loss: 4.0856 - val_acc: 0.5746 - val_f1_m: 0.5714 - val_precision_m: 0.6031 - val_recall_m: 0.5431\n",
            "Epoch 886/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2113 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9676 - recall_m: 0.8814 - val_loss: 4.0732 - val_acc: 0.5760 - val_f1_m: 0.5725 - val_precision_m: 0.6041 - val_recall_m: 0.5443\n",
            "Epoch 887/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2130 - acc: 0.9091 - f1_m: 0.9223 - precision_m: 0.9677 - recall_m: 0.8813 - val_loss: 4.0958 - val_acc: 0.5713 - val_f1_m: 0.5674 - val_precision_m: 0.6000 - val_recall_m: 0.5384\n",
            "Epoch 888/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2115 - acc: 0.9076 - f1_m: 0.9206 - precision_m: 0.9661 - recall_m: 0.8796 - val_loss: 4.0725 - val_acc: 0.5825 - val_f1_m: 0.5792 - val_precision_m: 0.6110 - val_recall_m: 0.5508\n",
            "Epoch 889/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2074 - acc: 0.9093 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8817 - val_loss: 4.1000 - val_acc: 0.5777 - val_f1_m: 0.5748 - val_precision_m: 0.6064 - val_recall_m: 0.5465\n",
            "Epoch 890/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2080 - acc: 0.9091 - f1_m: 0.9222 - precision_m: 0.9675 - recall_m: 0.8813 - val_loss: 4.0939 - val_acc: 0.5771 - val_f1_m: 0.5735 - val_precision_m: 0.6048 - val_recall_m: 0.5454\n",
            "Epoch 891/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2104 - acc: 0.9087 - f1_m: 0.9217 - precision_m: 0.9670 - recall_m: 0.8808 - val_loss: 4.1020 - val_acc: 0.5762 - val_f1_m: 0.5727 - val_precision_m: 0.6047 - val_recall_m: 0.5442\n",
            "Epoch 892/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2121 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9675 - recall_m: 0.8809 - val_loss: 4.1755 - val_acc: 0.5702 - val_f1_m: 0.5668 - val_precision_m: 0.5977 - val_recall_m: 0.5392\n",
            "Epoch 893/1000\n",
            "99878/99878 [==============================] - 10s 97us/step - loss: 0.2140 - acc: 0.9086 - f1_m: 0.9216 - precision_m: 0.9665 - recall_m: 0.8809 - val_loss: 4.0025 - val_acc: 0.5753 - val_f1_m: 0.5713 - val_precision_m: 0.6038 - val_recall_m: 0.5423\n",
            "Epoch 894/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2092 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9674 - recall_m: 0.8816 - val_loss: 4.0437 - val_acc: 0.5727 - val_f1_m: 0.5695 - val_precision_m: 0.6013 - val_recall_m: 0.5410\n",
            "Epoch 895/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2097 - acc: 0.9084 - f1_m: 0.9213 - precision_m: 0.9663 - recall_m: 0.8806 - val_loss: 4.0569 - val_acc: 0.5732 - val_f1_m: 0.5700 - val_precision_m: 0.6020 - val_recall_m: 0.5414\n",
            "Epoch 896/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2097 - acc: 0.9077 - f1_m: 0.9208 - precision_m: 0.9664 - recall_m: 0.8796 - val_loss: 4.0896 - val_acc: 0.5741 - val_f1_m: 0.5697 - val_precision_m: 0.6018 - val_recall_m: 0.5410\n",
            "Epoch 897/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2121 - acc: 0.9085 - f1_m: 0.9217 - precision_m: 0.9671 - recall_m: 0.8806 - val_loss: 4.0897 - val_acc: 0.5748 - val_f1_m: 0.5711 - val_precision_m: 0.6036 - val_recall_m: 0.5421\n",
            "Epoch 898/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2088 - acc: 0.9095 - f1_m: 0.9226 - precision_m: 0.9678 - recall_m: 0.8817 - val_loss: 4.1473 - val_acc: 0.5795 - val_f1_m: 0.5763 - val_precision_m: 0.6078 - val_recall_m: 0.5481\n",
            "Epoch 899/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2098 - acc: 0.9092 - f1_m: 0.9222 - precision_m: 0.9669 - recall_m: 0.8818 - val_loss: 4.1306 - val_acc: 0.5772 - val_f1_m: 0.5741 - val_precision_m: 0.6058 - val_recall_m: 0.5459\n",
            "Epoch 900/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2149 - acc: 0.9088 - f1_m: 0.9216 - precision_m: 0.9673 - recall_m: 0.8804 - val_loss: 4.1292 - val_acc: 0.5800 - val_f1_m: 0.5775 - val_precision_m: 0.6086 - val_recall_m: 0.5495\n",
            "Epoch 901/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2216 - acc: 0.9094 - f1_m: 0.9222 - precision_m: 0.9672 - recall_m: 0.8816 - val_loss: 4.1600 - val_acc: 0.5734 - val_f1_m: 0.5699 - val_precision_m: 0.6012 - val_recall_m: 0.5419\n",
            "Epoch 902/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2122 - acc: 0.9091 - f1_m: 0.9221 - precision_m: 0.9671 - recall_m: 0.8814 - val_loss: 4.1363 - val_acc: 0.5849 - val_f1_m: 0.5820 - val_precision_m: 0.6132 - val_recall_m: 0.5540\n",
            "Epoch 903/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2156 - acc: 0.9087 - f1_m: 0.9215 - precision_m: 0.9668 - recall_m: 0.8806 - val_loss: 4.0888 - val_acc: 0.5732 - val_f1_m: 0.5695 - val_precision_m: 0.6015 - val_recall_m: 0.5408\n",
            "Epoch 904/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2114 - acc: 0.9085 - f1_m: 0.9214 - precision_m: 0.9666 - recall_m: 0.8806 - val_loss: 4.1046 - val_acc: 0.5730 - val_f1_m: 0.5689 - val_precision_m: 0.6016 - val_recall_m: 0.5398\n",
            "Epoch 905/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2084 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9684 - recall_m: 0.8809 - val_loss: 4.2082 - val_acc: 0.5759 - val_f1_m: 0.5736 - val_precision_m: 0.6034 - val_recall_m: 0.5468\n",
            "Epoch 906/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2111 - acc: 0.9092 - f1_m: 0.9221 - precision_m: 0.9674 - recall_m: 0.8813 - val_loss: 4.0831 - val_acc: 0.5761 - val_f1_m: 0.5728 - val_precision_m: 0.6050 - val_recall_m: 0.5441\n",
            "Epoch 907/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2097 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9677 - recall_m: 0.8815 - val_loss: 4.0997 - val_acc: 0.5744 - val_f1_m: 0.5711 - val_precision_m: 0.6033 - val_recall_m: 0.5423\n",
            "Epoch 908/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2153 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9663 - recall_m: 0.8808 - val_loss: 4.0417 - val_acc: 0.5697 - val_f1_m: 0.5653 - val_precision_m: 0.5981 - val_recall_m: 0.5362\n",
            "Epoch 909/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2097 - acc: 0.9098 - f1_m: 0.9227 - precision_m: 0.9678 - recall_m: 0.8820 - val_loss: 4.1665 - val_acc: 0.5786 - val_f1_m: 0.5753 - val_precision_m: 0.6067 - val_recall_m: 0.5473\n",
            "Epoch 910/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2127 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9671 - recall_m: 0.8811 - val_loss: 4.0538 - val_acc: 0.5771 - val_f1_m: 0.5735 - val_precision_m: 0.6058 - val_recall_m: 0.5447\n",
            "Epoch 911/1000\n",
            "99878/99878 [==============================] - 10s 101us/step - loss: 0.2082 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8817 - val_loss: 4.1580 - val_acc: 0.5783 - val_f1_m: 0.5750 - val_precision_m: 0.6068 - val_recall_m: 0.5466\n",
            "Epoch 912/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2110 - acc: 0.9094 - f1_m: 0.9225 - precision_m: 0.9674 - recall_m: 0.8818 - val_loss: 3.9696 - val_acc: 0.5792 - val_f1_m: 0.5760 - val_precision_m: 0.6086 - val_recall_m: 0.5470\n",
            "Epoch 913/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2080 - acc: 0.9094 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8816 - val_loss: 4.0784 - val_acc: 0.5742 - val_f1_m: 0.5702 - val_precision_m: 0.6022 - val_recall_m: 0.5416\n",
            "Epoch 914/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2097 - acc: 0.9084 - f1_m: 0.9214 - precision_m: 0.9663 - recall_m: 0.8807 - val_loss: 4.1382 - val_acc: 0.5727 - val_f1_m: 0.5701 - val_precision_m: 0.6011 - val_recall_m: 0.5423\n",
            "Epoch 915/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2049 - acc: 0.9096 - f1_m: 0.9226 - precision_m: 0.9674 - recall_m: 0.8820 - val_loss: 4.0364 - val_acc: 0.5736 - val_f1_m: 0.5699 - val_precision_m: 0.6020 - val_recall_m: 0.5412\n",
            "Epoch 916/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2082 - acc: 0.9091 - f1_m: 0.9224 - precision_m: 0.9672 - recall_m: 0.8818 - val_loss: 4.1195 - val_acc: 0.5698 - val_f1_m: 0.5667 - val_precision_m: 0.5987 - val_recall_m: 0.5381\n",
            "Epoch 917/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2190 - acc: 0.9084 - f1_m: 0.9215 - precision_m: 0.9661 - recall_m: 0.8811 - val_loss: 4.0456 - val_acc: 0.5712 - val_f1_m: 0.5681 - val_precision_m: 0.6002 - val_recall_m: 0.5395\n",
            "Epoch 918/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2107 - acc: 0.9082 - f1_m: 0.9212 - precision_m: 0.9663 - recall_m: 0.8805 - val_loss: 4.0283 - val_acc: 0.5736 - val_f1_m: 0.5698 - val_precision_m: 0.6026 - val_recall_m: 0.5406\n",
            "Epoch 919/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2130 - acc: 0.9102 - f1_m: 0.9231 - precision_m: 0.9679 - recall_m: 0.8826 - val_loss: 4.1628 - val_acc: 0.5716 - val_f1_m: 0.5679 - val_precision_m: 0.5996 - val_recall_m: 0.5395\n",
            "Epoch 920/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2101 - acc: 0.9092 - f1_m: 0.9223 - precision_m: 0.9673 - recall_m: 0.8816 - val_loss: 4.1097 - val_acc: 0.5773 - val_f1_m: 0.5729 - val_precision_m: 0.6048 - val_recall_m: 0.5443\n",
            "Epoch 921/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2111 - acc: 0.9087 - f1_m: 0.9218 - precision_m: 0.9670 - recall_m: 0.8810 - val_loss: 4.1430 - val_acc: 0.5743 - val_f1_m: 0.5710 - val_precision_m: 0.6021 - val_recall_m: 0.5431\n",
            "Epoch 922/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2143 - acc: 0.9092 - f1_m: 0.9219 - precision_m: 0.9668 - recall_m: 0.8813 - val_loss: 4.1841 - val_acc: 0.5726 - val_f1_m: 0.5695 - val_precision_m: 0.6001 - val_recall_m: 0.5421\n",
            "Epoch 923/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2095 - acc: 0.9095 - f1_m: 0.9225 - precision_m: 0.9677 - recall_m: 0.8816 - val_loss: 4.1110 - val_acc: 0.5748 - val_f1_m: 0.5705 - val_precision_m: 0.6024 - val_recall_m: 0.5419\n",
            "Epoch 924/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2099 - acc: 0.9100 - f1_m: 0.9231 - precision_m: 0.9683 - recall_m: 0.8822 - val_loss: 4.1413 - val_acc: 0.5786 - val_f1_m: 0.5754 - val_precision_m: 0.6069 - val_recall_m: 0.5472\n",
            "Epoch 925/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2096 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9677 - recall_m: 0.8815 - val_loss: 4.0952 - val_acc: 0.5781 - val_f1_m: 0.5741 - val_precision_m: 0.6065 - val_recall_m: 0.5452\n",
            "Epoch 926/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2087 - acc: 0.9092 - f1_m: 0.9222 - precision_m: 0.9677 - recall_m: 0.8811 - val_loss: 4.1067 - val_acc: 0.5794 - val_f1_m: 0.5760 - val_precision_m: 0.6077 - val_recall_m: 0.5477\n",
            "Epoch 927/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2091 - acc: 0.9095 - f1_m: 0.9225 - precision_m: 0.9679 - recall_m: 0.8814 - val_loss: 4.1111 - val_acc: 0.5721 - val_f1_m: 0.5667 - val_precision_m: 0.5991 - val_recall_m: 0.5379\n",
            "Epoch 928/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2139 - acc: 0.9087 - f1_m: 0.9218 - precision_m: 0.9671 - recall_m: 0.8808 - val_loss: 4.0220 - val_acc: 0.5685 - val_f1_m: 0.5643 - val_precision_m: 0.5964 - val_recall_m: 0.5357\n",
            "Epoch 929/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2060 - acc: 0.9101 - f1_m: 0.9231 - precision_m: 0.9681 - recall_m: 0.8824 - val_loss: 4.1269 - val_acc: 0.5767 - val_f1_m: 0.5736 - val_precision_m: 0.6055 - val_recall_m: 0.5452\n",
            "Epoch 930/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2169 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9665 - recall_m: 0.8808 - val_loss: 4.1424 - val_acc: 0.5704 - val_f1_m: 0.5670 - val_precision_m: 0.5984 - val_recall_m: 0.5390\n",
            "Epoch 931/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2095 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9675 - recall_m: 0.8815 - val_loss: 4.0490 - val_acc: 0.5761 - val_f1_m: 0.5728 - val_precision_m: 0.6045 - val_recall_m: 0.5444\n",
            "Epoch 932/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2101 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9676 - recall_m: 0.8814 - val_loss: 4.0492 - val_acc: 0.5711 - val_f1_m: 0.5675 - val_precision_m: 0.6002 - val_recall_m: 0.5384\n",
            "Epoch 933/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2122 - acc: 0.9086 - f1_m: 0.9217 - precision_m: 0.9667 - recall_m: 0.8809 - val_loss: 4.0958 - val_acc: 0.5746 - val_f1_m: 0.5712 - val_precision_m: 0.6025 - val_recall_m: 0.5431\n",
            "Epoch 934/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2122 - acc: 0.9087 - f1_m: 0.9217 - precision_m: 0.9669 - recall_m: 0.8809 - val_loss: 4.1357 - val_acc: 0.5756 - val_f1_m: 0.5713 - val_precision_m: 0.6030 - val_recall_m: 0.5429\n",
            "Epoch 935/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2140 - acc: 0.9089 - f1_m: 0.9217 - precision_m: 0.9668 - recall_m: 0.8810 - val_loss: 4.2660 - val_acc: 0.5734 - val_f1_m: 0.5709 - val_precision_m: 0.6011 - val_recall_m: 0.5439\n",
            "Epoch 936/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2216 - acc: 0.9087 - f1_m: 0.9214 - precision_m: 0.9660 - recall_m: 0.8811 - val_loss: 4.0529 - val_acc: 0.5705 - val_f1_m: 0.5660 - val_precision_m: 0.5980 - val_recall_m: 0.5374\n",
            "Epoch 937/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2147 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9673 - recall_m: 0.8816 - val_loss: 4.1192 - val_acc: 0.5778 - val_f1_m: 0.5749 - val_precision_m: 0.6061 - val_recall_m: 0.5471\n",
            "Epoch 938/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2152 - acc: 0.9089 - f1_m: 0.9220 - precision_m: 0.9670 - recall_m: 0.8813 - val_loss: 4.1184 - val_acc: 0.5703 - val_f1_m: 0.5657 - val_precision_m: 0.5975 - val_recall_m: 0.5372\n",
            "Epoch 939/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2085 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9674 - recall_m: 0.8811 - val_loss: 4.1216 - val_acc: 0.5702 - val_f1_m: 0.5661 - val_precision_m: 0.5981 - val_recall_m: 0.5375\n",
            "Epoch 940/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2129 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8818 - val_loss: 4.1422 - val_acc: 0.5724 - val_f1_m: 0.5690 - val_precision_m: 0.6004 - val_recall_m: 0.5409\n",
            "Epoch 941/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2125 - acc: 0.9084 - f1_m: 0.9215 - precision_m: 0.9669 - recall_m: 0.8805 - val_loss: 4.0894 - val_acc: 0.5751 - val_f1_m: 0.5715 - val_precision_m: 0.6029 - val_recall_m: 0.5435\n",
            "Epoch 942/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2103 - acc: 0.9097 - f1_m: 0.9226 - precision_m: 0.9676 - recall_m: 0.8819 - val_loss: 4.1571 - val_acc: 0.5648 - val_f1_m: 0.5600 - val_precision_m: 0.5926 - val_recall_m: 0.5311\n",
            "Epoch 943/1000\n",
            "99878/99878 [==============================] - 10s 100us/step - loss: 0.2110 - acc: 0.9093 - f1_m: 0.9224 - precision_m: 0.9676 - recall_m: 0.8815 - val_loss: 4.1586 - val_acc: 0.5713 - val_f1_m: 0.5676 - val_precision_m: 0.5991 - val_recall_m: 0.5395\n",
            "Epoch 944/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2111 - acc: 0.9095 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8816 - val_loss: 4.1784 - val_acc: 0.5682 - val_f1_m: 0.5635 - val_precision_m: 0.5958 - val_recall_m: 0.5348\n",
            "Epoch 945/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2130 - acc: 0.9083 - f1_m: 0.9214 - precision_m: 0.9666 - recall_m: 0.8806 - val_loss: 4.0315 - val_acc: 0.5765 - val_f1_m: 0.5728 - val_precision_m: 0.6051 - val_recall_m: 0.5441\n",
            "Epoch 946/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2126 - acc: 0.9095 - f1_m: 0.9224 - precision_m: 0.9677 - recall_m: 0.8815 - val_loss: 4.2296 - val_acc: 0.5724 - val_f1_m: 0.5690 - val_precision_m: 0.5998 - val_recall_m: 0.5414\n",
            "Epoch 947/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2114 - acc: 0.9096 - f1_m: 0.9226 - precision_m: 0.9680 - recall_m: 0.8817 - val_loss: 4.0960 - val_acc: 0.5760 - val_f1_m: 0.5716 - val_precision_m: 0.6036 - val_recall_m: 0.5431\n",
            "Epoch 948/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2115 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9669 - recall_m: 0.8805 - val_loss: 4.1537 - val_acc: 0.5718 - val_f1_m: 0.5679 - val_precision_m: 0.5992 - val_recall_m: 0.5400\n",
            "Epoch 949/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2117 - acc: 0.9088 - f1_m: 0.9218 - precision_m: 0.9671 - recall_m: 0.8809 - val_loss: 4.1256 - val_acc: 0.5808 - val_f1_m: 0.5773 - val_precision_m: 0.6086 - val_recall_m: 0.5492\n",
            "Epoch 950/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2148 - acc: 0.9090 - f1_m: 0.9220 - precision_m: 0.9669 - recall_m: 0.8815 - val_loss: 4.0888 - val_acc: 0.5740 - val_f1_m: 0.5693 - val_precision_m: 0.6015 - val_recall_m: 0.5406\n",
            "Epoch 951/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2151 - acc: 0.9082 - f1_m: 0.9212 - precision_m: 0.9665 - recall_m: 0.8803 - val_loss: 4.1562 - val_acc: 0.5725 - val_f1_m: 0.5683 - val_precision_m: 0.6000 - val_recall_m: 0.5400\n",
            "Epoch 952/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2128 - acc: 0.9087 - f1_m: 0.9219 - precision_m: 0.9674 - recall_m: 0.8808 - val_loss: 4.0498 - val_acc: 0.5771 - val_f1_m: 0.5727 - val_precision_m: 0.6057 - val_recall_m: 0.5434\n",
            "Epoch 953/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2125 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9680 - recall_m: 0.8812 - val_loss: 4.0863 - val_acc: 0.5756 - val_f1_m: 0.5717 - val_precision_m: 0.6043 - val_recall_m: 0.5426\n",
            "Epoch 954/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2094 - acc: 0.9098 - f1_m: 0.9228 - precision_m: 0.9679 - recall_m: 0.8820 - val_loss: 4.1705 - val_acc: 0.5797 - val_f1_m: 0.5768 - val_precision_m: 0.6080 - val_recall_m: 0.5489\n",
            "Epoch 955/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2123 - acc: 0.9085 - f1_m: 0.9215 - precision_m: 0.9666 - recall_m: 0.8807 - val_loss: 4.0505 - val_acc: 0.5712 - val_f1_m: 0.5676 - val_precision_m: 0.5999 - val_recall_m: 0.5388\n",
            "Epoch 956/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2175 - acc: 0.9084 - f1_m: 0.9213 - precision_m: 0.9664 - recall_m: 0.8806 - val_loss: 4.1673 - val_acc: 0.5644 - val_f1_m: 0.5601 - val_precision_m: 0.5931 - val_recall_m: 0.5308\n",
            "Epoch 957/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2110 - acc: 0.9085 - f1_m: 0.9217 - precision_m: 0.9665 - recall_m: 0.8811 - val_loss: 4.1887 - val_acc: 0.5763 - val_f1_m: 0.5724 - val_precision_m: 0.6037 - val_recall_m: 0.5444\n",
            "Epoch 958/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2116 - acc: 0.9093 - f1_m: 0.9222 - precision_m: 0.9676 - recall_m: 0.8813 - val_loss: 4.1624 - val_acc: 0.5772 - val_f1_m: 0.5740 - val_precision_m: 0.6064 - val_recall_m: 0.5451\n",
            "Epoch 959/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2146 - acc: 0.9088 - f1_m: 0.9217 - precision_m: 0.9671 - recall_m: 0.8807 - val_loss: 4.1706 - val_acc: 0.5725 - val_f1_m: 0.5696 - val_precision_m: 0.6018 - val_recall_m: 0.5410\n",
            "Epoch 960/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2124 - acc: 0.9096 - f1_m: 0.9225 - precision_m: 0.9676 - recall_m: 0.8819 - val_loss: 4.1049 - val_acc: 0.5746 - val_f1_m: 0.5715 - val_precision_m: 0.6031 - val_recall_m: 0.5431\n",
            "Epoch 961/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2166 - acc: 0.9079 - f1_m: 0.9208 - precision_m: 0.9655 - recall_m: 0.8803 - val_loss: 4.1055 - val_acc: 0.5775 - val_f1_m: 0.5744 - val_precision_m: 0.6063 - val_recall_m: 0.5459\n",
            "Epoch 962/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2155 - acc: 0.9082 - f1_m: 0.9211 - precision_m: 0.9665 - recall_m: 0.8802 - val_loss: 4.1104 - val_acc: 0.5731 - val_f1_m: 0.5694 - val_precision_m: 0.6008 - val_recall_m: 0.5412\n",
            "Epoch 963/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2153 - acc: 0.9093 - f1_m: 0.9224 - precision_m: 0.9671 - recall_m: 0.8819 - val_loss: 4.0460 - val_acc: 0.5764 - val_f1_m: 0.5719 - val_precision_m: 0.6048 - val_recall_m: 0.5426\n",
            "Epoch 964/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2104 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9677 - recall_m: 0.8814 - val_loss: 4.1121 - val_acc: 0.5762 - val_f1_m: 0.5724 - val_precision_m: 0.6050 - val_recall_m: 0.5433\n",
            "Epoch 965/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2108 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9671 - recall_m: 0.8811 - val_loss: 4.1849 - val_acc: 0.5661 - val_f1_m: 0.5619 - val_precision_m: 0.5941 - val_recall_m: 0.5332\n",
            "Epoch 966/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2091 - acc: 0.9093 - f1_m: 0.9222 - precision_m: 0.9672 - recall_m: 0.8815 - val_loss: 4.1790 - val_acc: 0.5677 - val_f1_m: 0.5634 - val_precision_m: 0.5951 - val_recall_m: 0.5352\n",
            "Epoch 967/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2229 - acc: 0.9084 - f1_m: 0.9215 - precision_m: 0.9660 - recall_m: 0.8812 - val_loss: 4.0631 - val_acc: 0.5674 - val_f1_m: 0.5627 - val_precision_m: 0.5965 - val_recall_m: 0.5327\n",
            "Epoch 968/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2268 - acc: 0.9094 - f1_m: 0.9220 - precision_m: 0.9664 - recall_m: 0.8818 - val_loss: 4.1677 - val_acc: 0.5740 - val_f1_m: 0.5707 - val_precision_m: 0.6025 - val_recall_m: 0.5423\n",
            "Epoch 969/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2182 - acc: 0.9095 - f1_m: 0.9223 - precision_m: 0.9669 - recall_m: 0.8820 - val_loss: 4.1987 - val_acc: 0.5742 - val_f1_m: 0.5705 - val_precision_m: 0.6018 - val_recall_m: 0.5426\n",
            "Epoch 970/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2158 - acc: 0.9091 - f1_m: 0.9221 - precision_m: 0.9667 - recall_m: 0.8818 - val_loss: 4.2675 - val_acc: 0.5752 - val_f1_m: 0.5712 - val_precision_m: 0.6025 - val_recall_m: 0.5433\n",
            "Epoch 971/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2227 - acc: 0.9084 - f1_m: 0.9211 - precision_m: 0.9659 - recall_m: 0.8807 - val_loss: 4.0600 - val_acc: 0.5769 - val_f1_m: 0.5727 - val_precision_m: 0.6057 - val_recall_m: 0.5433\n",
            "Epoch 972/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2151 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8817 - val_loss: 4.1698 - val_acc: 0.5731 - val_f1_m: 0.5690 - val_precision_m: 0.6015 - val_recall_m: 0.5400\n",
            "Epoch 973/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2152 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9674 - recall_m: 0.8816 - val_loss: 4.1765 - val_acc: 0.5763 - val_f1_m: 0.5726 - val_precision_m: 0.6049 - val_recall_m: 0.5438\n",
            "Epoch 974/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2197 - acc: 0.9101 - f1_m: 0.9229 - precision_m: 0.9674 - recall_m: 0.8826 - val_loss: 4.1746 - val_acc: 0.5728 - val_f1_m: 0.5681 - val_precision_m: 0.6004 - val_recall_m: 0.5393\n",
            "Epoch 975/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2206 - acc: 0.9092 - f1_m: 0.9219 - precision_m: 0.9667 - recall_m: 0.8814 - val_loss: 4.1733 - val_acc: 0.5705 - val_f1_m: 0.5666 - val_precision_m: 0.5985 - val_recall_m: 0.5381\n",
            "Epoch 976/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2216 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9667 - recall_m: 0.8814 - val_loss: 4.1298 - val_acc: 0.5729 - val_f1_m: 0.5685 - val_precision_m: 0.6009 - val_recall_m: 0.5396\n",
            "Epoch 977/1000\n",
            "99878/99878 [==============================] - 10s 99us/step - loss: 0.2288 - acc: 0.9072 - f1_m: 0.9199 - precision_m: 0.9650 - recall_m: 0.8791 - val_loss: 4.1247 - val_acc: 0.5728 - val_f1_m: 0.5690 - val_precision_m: 0.6008 - val_recall_m: 0.5406\n",
            "Epoch 978/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2111 - acc: 0.9086 - f1_m: 0.9219 - precision_m: 0.9670 - recall_m: 0.8811 - val_loss: 4.1694 - val_acc: 0.5779 - val_f1_m: 0.5746 - val_precision_m: 0.6064 - val_recall_m: 0.5463\n",
            "Epoch 979/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2081 - acc: 0.9096 - f1_m: 0.9227 - precision_m: 0.9677 - recall_m: 0.8819 - val_loss: 4.2315 - val_acc: 0.5743 - val_f1_m: 0.5705 - val_precision_m: 0.6025 - val_recall_m: 0.5419\n",
            "Epoch 980/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2052 - acc: 0.9102 - f1_m: 0.9232 - precision_m: 0.9685 - recall_m: 0.8823 - val_loss: 4.1168 - val_acc: 0.5787 - val_f1_m: 0.5748 - val_precision_m: 0.6071 - val_recall_m: 0.5460\n",
            "Epoch 981/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2081 - acc: 0.9094 - f1_m: 0.9226 - precision_m: 0.9677 - recall_m: 0.8818 - val_loss: 4.1963 - val_acc: 0.5741 - val_f1_m: 0.5701 - val_precision_m: 0.6029 - val_recall_m: 0.5410\n",
            "Epoch 982/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2097 - acc: 0.9087 - f1_m: 0.9218 - precision_m: 0.9671 - recall_m: 0.8809 - val_loss: 4.2337 - val_acc: 0.5761 - val_f1_m: 0.5723 - val_precision_m: 0.6039 - val_recall_m: 0.5441\n",
            "Epoch 983/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2092 - acc: 0.9093 - f1_m: 0.9223 - precision_m: 0.9677 - recall_m: 0.8813 - val_loss: 4.1214 - val_acc: 0.5774 - val_f1_m: 0.5741 - val_precision_m: 0.6065 - val_recall_m: 0.5452\n",
            "Epoch 984/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2125 - acc: 0.9087 - f1_m: 0.9216 - precision_m: 0.9670 - recall_m: 0.8806 - val_loss: 4.1145 - val_acc: 0.5718 - val_f1_m: 0.5680 - val_precision_m: 0.6010 - val_recall_m: 0.5386\n",
            "Epoch 985/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2060 - acc: 0.9094 - f1_m: 0.9224 - precision_m: 0.9674 - recall_m: 0.8817 - val_loss: 4.0945 - val_acc: 0.5800 - val_f1_m: 0.5766 - val_precision_m: 0.6088 - val_recall_m: 0.5478\n",
            "Epoch 986/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2056 - acc: 0.9097 - f1_m: 0.9228 - precision_m: 0.9680 - recall_m: 0.8819 - val_loss: 4.1218 - val_acc: 0.5781 - val_f1_m: 0.5746 - val_precision_m: 0.6069 - val_recall_m: 0.5457\n",
            "Epoch 987/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2073 - acc: 0.9089 - f1_m: 0.9221 - precision_m: 0.9674 - recall_m: 0.8812 - val_loss: 4.0787 - val_acc: 0.5803 - val_f1_m: 0.5766 - val_precision_m: 0.6091 - val_recall_m: 0.5476\n",
            "Epoch 988/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2116 - acc: 0.9081 - f1_m: 0.9213 - precision_m: 0.9665 - recall_m: 0.8804 - val_loss: 4.1106 - val_acc: 0.5741 - val_f1_m: 0.5691 - val_precision_m: 0.6013 - val_recall_m: 0.5404\n",
            "Epoch 989/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2128 - acc: 0.9094 - f1_m: 0.9223 - precision_m: 0.9674 - recall_m: 0.8815 - val_loss: 4.0463 - val_acc: 0.5743 - val_f1_m: 0.5697 - val_precision_m: 0.6022 - val_recall_m: 0.5406\n",
            "Epoch 990/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2251 - acc: 0.9087 - f1_m: 0.9214 - precision_m: 0.9661 - recall_m: 0.8809 - val_loss: 4.2479 - val_acc: 0.5804 - val_f1_m: 0.5766 - val_precision_m: 0.6082 - val_recall_m: 0.5484\n",
            "Epoch 991/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2228 - acc: 0.9096 - f1_m: 0.9224 - precision_m: 0.9675 - recall_m: 0.8816 - val_loss: 4.2539 - val_acc: 0.5728 - val_f1_m: 0.5681 - val_precision_m: 0.5997 - val_recall_m: 0.5399\n",
            "Epoch 992/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2209 - acc: 0.9090 - f1_m: 0.9219 - precision_m: 0.9669 - recall_m: 0.8813 - val_loss: 4.1666 - val_acc: 0.5711 - val_f1_m: 0.5659 - val_precision_m: 0.5992 - val_recall_m: 0.5363\n",
            "Epoch 993/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2203 - acc: 0.9086 - f1_m: 0.9215 - precision_m: 0.9664 - recall_m: 0.8809 - val_loss: 4.1650 - val_acc: 0.5738 - val_f1_m: 0.5700 - val_precision_m: 0.6020 - val_recall_m: 0.5415\n",
            "Epoch 994/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2146 - acc: 0.9089 - f1_m: 0.9219 - precision_m: 0.9670 - recall_m: 0.8811 - val_loss: 4.2169 - val_acc: 0.5725 - val_f1_m: 0.5672 - val_precision_m: 0.6001 - val_recall_m: 0.5380\n",
            "Epoch 995/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2150 - acc: 0.9095 - f1_m: 0.9223 - precision_m: 0.9671 - recall_m: 0.8817 - val_loss: 4.1542 - val_acc: 0.5720 - val_f1_m: 0.5665 - val_precision_m: 0.5991 - val_recall_m: 0.5376\n",
            "Epoch 996/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2160 - acc: 0.9102 - f1_m: 0.9230 - precision_m: 0.9680 - recall_m: 0.8824 - val_loss: 4.0281 - val_acc: 0.5760 - val_f1_m: 0.5714 - val_precision_m: 0.6046 - val_recall_m: 0.5419\n",
            "Epoch 997/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2250 - acc: 0.9086 - f1_m: 0.9214 - precision_m: 0.9663 - recall_m: 0.8808 - val_loss: 4.0348 - val_acc: 0.5709 - val_f1_m: 0.5655 - val_precision_m: 0.5993 - val_recall_m: 0.5355\n",
            "Epoch 998/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2275 - acc: 0.9092 - f1_m: 0.9220 - precision_m: 0.9662 - recall_m: 0.8819 - val_loss: 4.0989 - val_acc: 0.5747 - val_f1_m: 0.5697 - val_precision_m: 0.6024 - val_recall_m: 0.5405\n",
            "Epoch 999/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2163 - acc: 0.9088 - f1_m: 0.9217 - precision_m: 0.9667 - recall_m: 0.8811 - val_loss: 4.1730 - val_acc: 0.5792 - val_f1_m: 0.5752 - val_precision_m: 0.6074 - val_recall_m: 0.5465\n",
            "Epoch 1000/1000\n",
            "99878/99878 [==============================] - 10s 98us/step - loss: 0.2136 - acc: 0.9094 - f1_m: 0.9225 - precision_m: 0.9678 - recall_m: 0.8815 - val_loss: 4.1555 - val_acc: 0.5760 - val_f1_m: 0.5720 - val_precision_m: 0.6038 - val_recall_m: 0.5436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc09c8d9198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uZmtqnCckBs",
        "colab_type": "code",
        "outputId": "4705b0e3-08bb-447a-abbb-b7091c7dcdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# test_tfidf\n",
        "test_tfidf_mat = test_tfidf.toarray()\n",
        "\n",
        "# reshape from [samples, sequence] into [samples, sequence, features]\n",
        "fea_vec_dim = test_tfidf_mat.shape[1]\n",
        "n_class = 5\n",
        "print(fea_vec_dim, n_class)\n",
        "\n",
        "X_test = test_tfidf_mat.reshape((test_tfidf_mat.shape[0], test_tfidf_mat.shape[1], 1))\n",
        "X_test.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3443 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1706, 3443, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Q_LZ-Cy-QQ",
        "colab_type": "code",
        "outputId": "7990160a-8ad3-46c1-d255-524c928bf86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# evaluate the model\n",
        "import numpy as np\n",
        "def print_metrics(accuracy, f1_score, precision, recall):  \n",
        "    print('SIMPLE CNN MODEL PERFORMANCE')\n",
        "    print('Accuracy:', np.round(accuracy, 4))\n",
        "    print('Precision:', np.round(precision, 4))\n",
        "    print('Recall:', np.round(recall, 4))\n",
        "    print('F1 Score:', np.round(f1_score, 4))\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print_metrics(accuracy, f1_score, precision, recall)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SIMPLE CNN MODEL PERFORMANCE\n",
            "Accuracy: 0.5709\n",
            "Precision: 0.5994\n",
            "Recall: 0.5368\n",
            "F1 Score: 0.5658\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4K2CJdZ-PS_",
        "colab_type": "text"
      },
      "source": [
        "# **Wrod Embedding:** \n",
        " \n",
        "*   Even if we already use an Embedding Layer to avoid sparsity and reduce dimensionality, a pre-trained word embedding can be very interesting since it already contains information from thousands of lines of various text.\n",
        "*   We can see it as a type of transfer learning, which can be very useful since we don’t have much data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWDnR_ENO185",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH37-2cgO2vh",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoshYTfdO3QJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s135jqgtO4K-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytmXBAgFO4n2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2igxAGCJO5kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x):\n",
        "    tk = treebank_tokenizer.tokenize(x)\n",
        "    tk = [s.lower() for s in tk]\n",
        "    return tk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYze4dFNO-CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sentences.apply(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv5s9-frPEZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(sentences, B):\n",
        "    \n",
        "    lens = sentences.apply(lambda x:len(x))\n",
        "    ebds = []\n",
        "    tsteps = max(lens)\n",
        "    \n",
        "    X = np.zeros((len(sentences) , tsteps , B))\n",
        "    \n",
        "    for i in range(len(sentences)):\n",
        "        \n",
        "        words = sentences[i]\n",
        "        k = tsteps -1\n",
        "        \n",
        "        for word in words[::-1] :\n",
        "            try:\n",
        "                X[i , k] = model[word]\n",
        "                k = k - 1\n",
        "            except:\n",
        "                lens[i] = lens[i] - 1\n",
        "                \n",
        "    ntsteps = max(lens)\n",
        "    \n",
        "    return X[: , (tsteps - ntsteps): ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S68A55-WP5XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = sentences.reset_index(drop = True)\n",
        "X = vectorize(sentences , 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suO_UF5PQYZB",
        "colab_type": "code",
        "outputId": "0136877f-8d90-4eee-a1c1-79fb70c64193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = vectorize(sentences , 100)\n",
        "Y = sentiment\n",
        "\n",
        "print(\"X's shape :\" , X.shape)\n",
        "print(\"Y's shape :\" , Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X's shape : (8529, 0, 100)\n",
            "Y's shape : (8529,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}