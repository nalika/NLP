{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalika/NLP/blob/master/Lab_1_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmX0w13AeuBF",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction to Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKrSwiWekqKw",
        "colab_type": "text"
      },
      "source": [
        "**Environment setup**\n",
        "> First, make sure you have an environment using GPU:\n",
        "1.   Click runtime at the top\n",
        "2.   Change runtime type\n",
        "3.   Hardware accelerator\n",
        "4.   GPU then save\n",
        "\n",
        "> Then, click connect in the top right and wait until you see a checkmark\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbMeTpwTmIKQ",
        "colab_type": "text"
      },
      "source": [
        "**Drive mounting and data reading**\n",
        "\n",
        "> On the left hand side, you should see table of contents (if you don't see this, click on the arrow to expand the menu):\n",
        "1.   Click on files\n",
        "2.   Mount drive\n",
        "3.   Then, run the code using the play button to the left of the cell and follow the instructions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBu4RBLo1roW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhcuv4LB1527",
        "colab_type": "text"
      },
      "source": [
        "> 4.   Click refresh in the files tab\n",
        "5.   Open the drive folder then my drive, and you will see the contents of your Google Drive\n",
        "6. To reference files from here in your code, right click on the file and use copy path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQWbgE2V2oNz",
        "colab_type": "text"
      },
      "source": [
        "# **Linear Regression with Sklearn libraries, Numpy, and Python coding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpSYdk1_2x4D",
        "colab_type": "text"
      },
      "source": [
        "> First, we need to import the libraries we'll be using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYWEgm0R27qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the linear model package from sklearn\n",
        "from sklearn import linear_model\n",
        "\n",
        "# Get the train/test split package from sklearn for preparing our dataset to\n",
        "# train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get the mean_squared_error and r2_score packages from sklearn to measure our\n",
        "# model's performance\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Import the numpy library to work with and manipulate the data\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K57rQlSVDiU4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Next, we will read the following dataset: \n",
        "* https://github.com/ageron/handson-ml/blob/master/datasets/housing/housing.csv\n",
        "*   Try searching on Google for \"github ageron housing.csv\" to download this file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha6N2_2JEQBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reads a comma-separated value (CSV) file\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Lakehead University/2020-2021' +\n",
        "                      '/Work/Natural Language Processing/Labs/housing.csv')\n",
        "                      # The path to your .csv file\n",
        "\n",
        "# View the first five rows of the dataset\n",
        "print(\"Here are the first five rows of the dataset:\")\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z51Hvp4K0WB",
        "colab_type": "text"
      },
      "source": [
        "> Next, we will split the dataset into the input (X) and output (Y) datasets\n",
        "*   The Y dataset is what we want to predict\n",
        "*   The X dataset is what we will use to predict Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NclAORVLMO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will predict the \"MEDV\" column\n",
        "Y = dataset['MEDV']\n",
        "\n",
        "# The remainder of the columns will be used to predict Y\n",
        "# Select from the \"CRIM\" column to the \"LSTAT\" column\n",
        "X = dataset.loc[:,'CRIM':'LSTAT']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aaZBqJDM83-",
        "colab_type": "text"
      },
      "source": [
        "> Once the dataset has been read, we need to split it into training and testing portions\n",
        "*   The linear model will learn from the training set\n",
        "*   The performance of the trained model will be evaluated by predicting the outputs of the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-zjQosaNSwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits the dataset so 80% is used for training and 20% for testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "# View the number of entries in the splits\n",
        "print(\"There are \" + str(x_train.size) + \" training entries and \" + \n",
        "      str(x_test.size) + \" testing entries!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXIesJAyOVOm",
        "colab_type": "text"
      },
      "source": [
        "> Now that the dataset has been split, we need to train the linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62kQvUp6OeTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilbMCmi-OocN",
        "colab_type": "text"
      },
      "source": [
        "> Next, we will evaluate the performance of the model\n",
        "*   The R^2 score is a measure of variance in the model. The closer to 1 it is, the better it's performing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff7xI_c8OuZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predictions of the model on the testing dataset\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Output the performance of the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"The model's mean squared error is: \" + str(mse))\n",
        "\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"The model's R^2 score is: \" + str(r2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7nKihyuRfak",
        "colab_type": "text"
      },
      "source": [
        "# **Linear Regression with Pytorch on Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzDtWIKORoZk",
        "colab_type": "text"
      },
      "source": [
        "> First, let's import the needed libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybn9yEMXRsXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the pytorch library\n",
        "import torch\n",
        "\n",
        "# Import the Linear package from pytorch for our model\n",
        "from torch.nn import Linear\n",
        "\n",
        "# Import the SGD (stochastic gradient descent) package from pytorch for\n",
        "# our optimizer\n",
        "from torch.optim import SGD\n",
        "\n",
        "# Import the L1Loss (mean absolute error loss) package from pytorch for\n",
        "# our performance measure\n",
        "from torch.nn import L1Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K29MK55WS0BD",
        "colab_type": "text"
      },
      "source": [
        "> Next, we need to convert the dataset we defined previously to numpy arrays to use with pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMo2wsRRSzJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the training data\n",
        "x_train = x_train.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "# Convert the testing data\n",
        "x_test = x_test.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF8ZYJKYTkpc",
        "colab_type": "text"
      },
      "source": [
        "> Next, we will define our model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMpSHnYcUXhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to tell it how many inputs (X columns) and \n",
        "# outputs (Y columns) there are\n",
        "model = Linear(X.shape[1], 1)\n",
        "\n",
        "# Set the model to use the GPU for processing\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OTLEiNvb6CM",
        "colab_type": "text"
      },
      "source": [
        "> The basic definition of linear regression is *Y = Xβ + ε* where *β* and *ε* are parameters that the model is trying to optimize to better predict the *Y* value\n",
        "*   For sklearn, the common optimization technique used is called *stochastic gradient descent*\n",
        "*   With pytorch, we need to define an optimization technique for our model as well as the learning rate for it\n",
        "\n",
        "> The learning rate affects how the parameters are updated when optimizing. A high learning rate will change the parameters more significantly than a low learning rate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDnWRI4eaGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a learning rate\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Define the optimizer with our model's parameters\n",
        "optimizer = SGD(model.parameters(), learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P66AipbJfguo",
        "colab_type": "text"
      },
      "source": [
        "> Now we need to use the optimizer to fine-tune the parameters based on the model's output\n",
        "*   For sklearn's linear_model, the default technique it uses is to run until it has minimized the total sum of squared errors between the predicted outputs and expected outputs\n",
        "*   With pytorch, we decide how long to train the model for using our selected optimizer. We will measure how it's doing with the \"L1Loss\" package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUxKT72dhXBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of epochs to train for\n",
        "epochs = 100\n",
        "\n",
        "# Define the performance measure\n",
        "performance = L1Loss()\n",
        "\n",
        "# Convert the training set into torch variables for our model using the GPU\n",
        "# as floats. The reshape is to remove a warning pytorch outputs otherwise.\n",
        "inputs = torch.from_numpy(x_train).cuda().float()\n",
        "outputs = torch.from_numpy(y_train.reshape(y_train.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(epochs):\n",
        "  # Clear the current errors so they don't cummulate\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Get the model's predictions for the training dataset\n",
        "  predictions = model(inputs)\n",
        "\n",
        "  # See how well the model performed\n",
        "  loss = performance(predictions, outputs)\n",
        "\n",
        "  # Compute the gradients for our optimizer\n",
        "  loss.backward()\n",
        "\n",
        "  # Use the optimizer to update the model's parameters based on the gradients\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"Epoch \" + str(epoch + 1) + \": \" + str(loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ounuoXn95P",
        "colab_type": "text"
      },
      "source": [
        "> Lastly, we will test the model to see how it performs on the testing dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU2bqyMloGI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the testing set into torch variables for our model using the GPU\n",
        "# as floats\n",
        "inputs = torch.from_numpy(x_test).cuda().float()\n",
        "outputs = torch.from_numpy(y_test.reshape(y_test.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Get the predictions of the model on the testing dataset\n",
        "predictions = model(inputs)\n",
        "\n",
        "# Output the performance of the model\n",
        "loss = performance(predictions, outputs)\n",
        "print(\"The model's L1 loss is: \" + str(loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}